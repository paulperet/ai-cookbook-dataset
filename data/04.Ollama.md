# Guide: Using the Phi Model Family with Ollama

Ollama is a powerful tool that simplifies running open-source large language models (LLMs) and small language models (SLMs) locally. It provides a straightforward way to deploy models like the Phi family and build local API endpoints for applications such as AI assistants. This guide walks you through installation, API usage, and integration with popular programming languages.

## Prerequisites

Before you begin, ensure you have:
- A system running Windows, macOS, or Linux.
- Basic familiarity with using a terminal or command line.

## Step 1: Install Ollama

Download and install Ollama from the [official website](https://ollama.com/download). The installation process is straightforward for all supported operating systems.

> **Note:** If you are using a GitHub Codespace, Ollama is likely pre-installed.

## Step 2: Run a Phi Model

Once installed, you can immediately start using models from the command line. For example, to run the `phi4` model:

```bash
ollama run phi4
```

The first time you execute this command, the specified model will be downloaded. After the download completes, you can interact with the model directly in your terminal.

## Step 3: Start the Ollama Server

To enable API access, start the Ollama server:

```bash
ollama serve
```

This command starts a local server, typically on `http://127.0.0.1:11434`.

> **Note:** If you encounter the error `"Error: listen tcp 127.0.0.1:11434: bind: address already in use"`, it usually means the server is already running. You can safely ignore it. If you need to restart the service:
> - **macOS:** `brew services restart ollama`
> - **Linux:** `sudo systemctl stop ollama` (then restart with `ollama serve`)

## Step 4: Use the Ollama HTTP API

Ollama provides two primary API endpoints: `generate` and `chat`. You can interact with them using tools like `curl` or Postman.

### Chat API Example

The `/api/chat` endpoint is used for conversational interactions. Here's an example `curl` request:

```bash
curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "You are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble sort algorithm"
    }
  ],
  "stream": false
}'
```

This request sends a system prompt and a user query to the `phi3` model, requesting a non-streamed (`"stream": false`) response containing a bubble sort algorithm.

## Step 5: Integrate Ollama with Programming Languages

Ollama's OpenAI-compatible API makes it easy to integrate with various programming ecosystems.

### Python Integration

Use the official `openai` Python package to connect to your local Ollama server.

First, ensure the package is installed:

```bash
pip install openai
```

Then, use the following code to call the model:

```python
import openai

# Configure the client to point to your local Ollama server
client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",  # API key is not required for local Ollama
)

# Create a chat completion request
response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a haiku about a hungry cat"},
    ],
)

# Print the model's response
print("Response:")
print(response.choices[0].message.content)
```

### JavaScript Integration

You can use Ollama within JavaScript environments, often with helper libraries. Below is a conceptual example using a hypothetical scripting syntax:

```javascript
// Example: Summarize a file with Phi-4
script({
    model: "ollama:phi4",
    title: "Summarize with Phi-4",
    system: ["system"],
})

// Define a file and request a summary
const file = def("FILE", env.files)
$`Summarize ${file} in a single paragraph.`
```

### C# Integration with Semantic Kernel

For .NET applications, you can use the Microsoft Semantic Kernel SDK.

1. Create a new Console Application and add the required NuGet package:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

2. Replace the code in your `Program.cs` file:

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

var builder = Kernel.CreateBuilder();

// Add the chat completion service using the local Ollama endpoint
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "not required");

var kernel = builder.Build();

// Invoke a simple prompt
string prompt = "Write a joke about kittens";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

3. Run the application:

```bash
dotnet run
```

## Additional Resources

- **Ollama Model Library:** Browse all available models at [ollama.com/library](https://ollama.com/library).
- **Pre-download a Model:** Use `ollama pull phi4` to download a model without immediately running it.
- **Ollama API Documentation:** For detailed API specifications, visit the [official Ollama API docs](https://github.com/ollama/ollama/blob/main/docs/api.md).

You now have a functional local deployment of Phi models via Ollama and can integrate it into your applications using Python, JavaScript, C#, or direct HTTP calls.