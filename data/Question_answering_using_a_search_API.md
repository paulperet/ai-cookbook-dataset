# AI-Powered Search & Re-ranking: A Hybrid Approach for Question Answering

## Introduction

Searching for relevant information can feel like finding a needle in a haystack. This guide explores a hybrid AI technique that augments existing search systems, combining the strengths of two common retrieval methods:

1.  **Mimicking Human Browsing:** An LLM triggers searches, evaluates results, and iteratively refines queries.
2.  **Retrieval with Embeddings:** Content and queries are embedded, and results are retrieved based on semantic similarity.

While promising, each has drawbacks: iterative browsing can be slow, and embedding-based retrieval requires maintaining a vector database. This tutorial presents a middle-ground approach that can be layered on top of any existing search API (like Slack, ElasticSearch, or, in our example, the News API). The workflow has three core steps:

1.  **Search:** An LLM generates multiple search queries from a user's question, which are executed in parallel.
2.  **Re-rank:** Search results are semantically re-ranked against a hypothetical ideal answer generated by the LLM.
3.  **Answer:** The top re-ranked results are used by the LLM to generate a final, cited answer.

Let's implement this pipeline.

## Prerequisites & Setup

You will need two API keys:
1.  An **OpenAI API key**.
2.  A **News API key** (get one [here](https://newsapi.org/)).

Set these as environment variables (`OPENAI_API_KEY` and `NEWS_API_KEY`) before proceeding.

First, install the required libraries and set up our helper functions.

```bash
pip install openai requests tqdm
```

```python
# Import dependencies
from datetime import date, timedelta
import json
from numpy import dot
from openai import OpenAI
import os
import requests
from tqdm.notebook import tqdm  # Use `from tqdm import tqdm` if not in a notebook

# Initialize the OpenAI client
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# Load environment variables
news_api_key = os.getenv("NEWS_API_KEY")

# Define the model to use
GPT_MODEL = "gpt-3.5-turbo"

# Helper: Get structured JSON output from GPT
def json_gpt(input: str):
    completion = client.chat.completions.create(
        model=GPT_MODEL,
        messages=[
            {"role": "system", "content": "Output only valid JSON"},
            {"role": "user", "content": input},
        ],
        temperature=0.5,
    )
    text = completion.choices[0].message.content
    parsed = json.loads(text)
    return parsed

# Helper: Generate embeddings for a list of text strings
def embeddings(input: list[str]) -> list[list[float]]:
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=input
    )
    return [data.embedding for data in response.data]
```

## Step 1: Search - Generating and Executing Queries

The process begins with a user's question. To cast a wide net, we'll use the LLM to generate a diverse set of search queries based on this question.

### 1.1 Define the User Question

```python
USER_QUESTION = "Who won the NBA championship? And who was the MVP? Tell me a bit about the last game."
```

### 1.2 Generate Multiple Search Queries

We prompt the model to think of various related queries to maximize the chance of finding relevant articles.

```python
QUERIES_INPUT = f"""
You have access to a search API that returns recent news articles.
Generate an array of search queries that are relevant to this question.
Use a variation of related keywords for the queries, trying to be as general as possible.
Include as many queries as you can think of, including and excluding terms.
For example, include queries like ['keyword_1 keyword_2', 'keyword_1', 'keyword_2'].
Be creative. The more queries you include, the more likely you are to find relevant results.

User question: {USER_QUESTION}

Format: {{"queries": ["query_1", "query_2", "query_3"]}}
"""

queries = json_gpt(QUERIES_INPUT)["queries"]
# Include the original question as a query for good measure
queries.append(USER_QUESTION)

print(f"Generated {len(queries)} queries.")
print(queries)
```

### 1.3 Execute Searches in Parallel

Now, we define a function to call the News API and execute all generated queries.

```python
def search_news(
    query: str,
    news_api_key: str = news_api_key,
    num_articles: int = 50,
    from_datetime: str = "2023-06-01",  # NBA Finals occurred in June 2023
    to_datetime: str = "2023-06-30",
) -> dict:
    """Fetch news articles for a given query from the News API."""
    response = requests.get(
        "https://newsapi.org/v2/everything",
        params={
            "q": query,
            "apiKey": news_api_key,
            "pageSize": num_articles,
            "sortBy": "relevancy",
            "from": from_datetime,
            "to": to_datetime,
        },
    )
    return response.json()

# Collect articles from all queries
articles = []
for query in tqdm(queries):
    result = search_news(query)
    if result["status"] == "ok":
        articles.extend(result["articles"])
    else:
        raise Exception(f"API Error: {result['message']}")

# Remove duplicate articles based on URL
unique_articles = {}
for article in articles:
    unique_articles[article["url"]] = article
articles = list(unique_articles.values())

print(f"Total unique articles retrieved: {len(articles)}")
```

At this point, we have a large set of articles, many of which may not be relevant. Next, we'll use semantic re-ranking to filter and prioritize the best results.

## Step 2: Re-rank - Filtering with Semantic Similarity

Inspired by the **Hypothetical Document Embeddings (HyDE)** technique, we will generate a "hypothetical" ideal answer. We then compare our search results to this ideal answer, rather than to the original question, to find content that *looks like* a good answer.

### 2.1 Generate a Hypothetical Answer

```python
HA_INPUT = f"""
Generate a hypothetical answer to the user's question. This answer will be used to rank search results.
Pretend you have all the information you need to answer, but don't use any actual facts. Instead, use placeholders
like NAME did something, or NAME said something at PLACE.

User question: {USER_QUESTION}

Format: {{"hypotheticalAnswer": "hypothetical answer text"}}
"""

hypothetical_answer = json_gpt(HA_INPUT)["hypotheticalAnswer"]
print("Hypothetical Answer Generated:")
print(hypothetical_answer)
```

### 2.2 Create Embeddings and Calculate Similarity

We generate embeddings for both the hypothetical answer and a concise representation of each article (title + description + snippet). We then calculate the cosine similarity between them.

```python
# Generate embedding for the hypothetical answer
hypothetical_answer_embedding = embeddings([hypothetical_answer])[0]

# Prepare text snippets from articles for embedding
article_texts = [
    f"{article['title']} {article['description']} {article['content'][0:100]}"
    for article in articles
]

# Generate embeddings for all articles
article_embeddings = embeddings(article_texts)

# Calculate cosine similarity (dot product, as embeddings are normalized)
cosine_similarities = [
    dot(hypothetical_answer_embedding, article_embedding)
    for article_embedding in article_embeddings
]
```

### 2.3 Sort and Filter Articles

We pair each article with its similarity score, sort them in descending order, and select the top results.

```python
# Pair articles with their scores
scored_articles = list(zip(articles, cosine_similarities))

# Sort by score, highest first
sorted_articles = sorted(scored_articles, key=lambda x: x[1], reverse=True)

# Inspect the top 5 re-ranked articles
print("Top 5 re-ranked articles:\n")
for article, score in sorted_articles[:5]:
    print(f"Title: {article['title']}")
    print(f"Description: {article['description']}")
    print(f"Content Snippet: {article['content'][0:100]}...")
    print(f"Relevance Score: {score:.4f}\n")
```

The re-ranked list should now contain the most semantically relevant articles at the top, ready to be used for answer generation.

## Step 3: Answer - Synthesizing the Final Response

Finally, we provide the top re-ranked results to the LLM and ask it to synthesize a comprehensive answer, citing its sources.

### 3.1 Format the Top Results

```python
# Format the top 5 articles for the prompt
formatted_top_results = [
    {
        "title": article["title"],
        "description": article["description"],
        "url": article["url"],
    }
    for article, _score in sorted_articles[:5]
]
```

### 3.2 Generate the Final Answer

We construct a prompt that includes the user's question and the formatted search results, instructing the model to reference the provided URLs.

```python
ANSWER_INPUT = f"""
Generate an answer to the user's question based on the given search results.
TOP_RESULTS: {formatted_top_results}
USER_QUESTION: {USER_QUESTION}

Include as much information as possible in the answer. Reference the relevant search result URLs as markdown links.
"""

completion = client.chat.completions.create(
    model=GPT_MODEL,
    messages=[{"role": "user", "content": ANSWER_INPUT}],
    temperature=0.5,
)

final_answer = completion.choices[0].message.content
print(final_answer)
```

### Example Output

> The Denver Nuggets won their first-ever NBA championship by defeating the Miami Heat 94-89 in Game 5 of the NBA Finals held on Tuesday at the Ball Arena in Denver, according to this [Business Standard article](https://www.business-standard.com/sports/other-sports-news/nba-finals-denver-nuggets-beat-miami-hea-lift-thier-first-ever-nba-title-123061300285_1.html). Nikola Jokic, the Nuggets' center, was named the NBA Finals MVP. In a rock-fight of a Game 5, the Nuggets reached the NBA mountaintop, securing their franchise's first NBA championship and setting Nikola Jokic's legacy as an all-timer in stone, according to this [Yahoo Sports article](https://sports.yahoo.com/nba-finals-nikola-jokic-denver-nuggets-survive-miami-heat-to-secure-franchises-first-nba-championship-030321214.html). For more information and photos of the Nuggets' celebration, check out this [Al Jazeera article](https://www.aljazeera.com/gallery/2023/6/15/photos-denver-nuggets-celebrate-their-first-nba-title) and this [CNN article](https://www.cnn.com/2023/06/12/sport/denver-nuggets-nba-championship-spt-intl?cid=external-feeds_iluminar_yahoo).

## Conclusion

You have successfully built a hybrid search-and-answer pipeline. This approach enhances a standard search API with AI to:
1.  **Broaden the search** via LLM-generated queries.
2.  **Intelligently filter results** using semantic re-ranking against a hypothetical ideal answer.
3.  **Synthesize a concise, cited answer** from the best sources.

This method is versatile and can be adapted to any search backend, providing a significant boost in result relevance without the overhead of maintaining a full vector database.