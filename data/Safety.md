# Gemini API Safety Settings Guide

This guide demonstrates how to use the adjustable safety settings in the Gemini API. You will learn how to identify when a prompt is blocked by safety filters, understand the reason for the block, and adjust the filters to allow content through when appropriate.

## Prerequisites

First, install the required Python SDK.

```bash
pip install -q -U "google-genai>=1.0.0"
```

## 1. Import Libraries and Set Up the Client

After installation, import the necessary modules and configure the client with your API key. Ensure your `GOOGLE_API_KEY` is stored securely. This example uses Google Colab secrets.

```python
from google import genai
from google.colab import userdata

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)
```

## 2. Send a Prompt and Check for Safety Blocks

You will now send a prompt that may trigger the safety filters. The `generate_content` method returns a response object containing safety feedback.

Define your model and a potentially unsafe prompt.

```python
MODEL_ID = "gemini-3-flash-preview"

unsafe_prompt = """
  I support Martians Soccer Club and I think Jupiterians Football Club
  sucks! Write a ironic phrase about them.
"""

response = client.models.generate_content(
    model=MODEL_ID,
    contents=unsafe_prompt
)
```

### 2.1 Inspect the Finish Reason

The `finish_reason` for each candidate in the response indicates whether the generation was successful or blocked.

*   `FinishReason.STOP`: The request completed successfully.
*   `FinishReason.SAFETY`: The request was blocked by a safety filter.

```python
print(response.candidates[0].finish_reason)
```

**Output:**
```
FinishReason.STOP
```

In this case, the model generated a response, meaning it was not blocked.

### 2.2 Examine Safety Ratings

If a request is blocked (`FinishReason.SAFETY`), you can inspect the `safety_ratings` list to see which specific harm category triggered the filter.

```python
from pprint import pprint
pprint(response.candidates[0].safety_ratings, indent=2)
```

**Output:**
```
None
```

Since our request was not blocked, `safety_ratings` is `None`.

### 2.3 Access the Generated Text

Finally, retrieve the text generated by the model.

```python
try:
    print(response.text)
except:
    print("No information generated by the model.")
```

**Output:**
```
Oh, Jupiterians? They're so good at losing, it's almost an art form. You have to admire their dedication to consistently disappointing their fans!
```

## 3. Customize Safety Settings

For some use cases, you may need to adjust the strictness of the safety filters. You can do this by passing a `safety_settings` configuration to the `generate_content` request.

In the following example, we set all safety filters to block only content with a `LOW` probability or higher of being harmful. This makes the filters less strict.

**Important:** Google's commitment to Responsible AI and its [AI Principles](https://ai.google/responsibility/principles/) means that for some highly unsafe prompts, Gemini will still refuse to generate content, even with filters disabled.

```python
from google.genai import types

response = client.models.generate_content(
    model=MODEL_ID,
    contents=unsafe_prompt,
    config=types.GenerateContentConfig(
        safety_settings=[
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            )
        ]
    )
)
```

### 3.1 Verify the Request was Processed

Check the `finish_reason` again. If your adjusted settings were sufficient, it should now be `FinishReason.STOP`.

```python
print(response.candidates[0].finish_reason)
```

**Output:**
```
FinishReason.SAFETY
```

In this specific run, the request was still blocked by safety filters.

### 3.2 Check the Safety Ratings After Customization

When you customize settings, inspecting the `safety_ratings` becomes crucial to see which specific filter was triggered.

```python
from pprint import pprint
pprint(response.candidates[0].safety_ratings, indent=2)
```

**Output:**
```
[ SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None),
  SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None),
  SafetyRating(blocked=True, category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>, probability=<HarmProbability.LOW: 'LOW'>, probability_score=None, severity=None, severity_score=None),
  SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None)]
```

The output shows that the `HARM_CATEGORY_HARASSMENT` filter was triggered (`blocked=True`) with a `LOW` probability rating, which explains why the request was blocked despite our custom settings.

## 4. Next Steps and Resources

*   **Learn More:** For a deeper understanding, read the official documentation on [safety guidance](https://ai.google.dev/docs/safety_guidance) and [safety settings](https://ai.google.dev/docs/safety_setting_gemini).
*   **API Reference:** The four configurable safety categories are:
    *   `HARM_CATEGORY_HARASSMENT`
    *   `HARM_CATEGORY_HATE_SPEECH`
    *   `HARM_CATEGORY_SEXUALLY_EXPLICIT`
    *   `HARM_CATEGORY_DANGEROUS_CONTENT`

    You can refer to them by their full name or common aliases (e.g., `"DANGEROUS"`). Safety settings can be configured globally in the `GenerativeModel` constructor or per request in `generate_content`.