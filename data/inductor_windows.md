# Guide: Using `torch.compile` on Windows for CPU and XPU

## Introduction

TorchInductor is PyTorch's new compiler backend. It takes the FX Graphs generated by TorchDynamo and compiles them into optimized C++ or Triton kernels. This guide walks you through the steps required to enable and use TorchInductor via `torch.compile` on a Windows system, targeting both CPU and Intel XPU (GPU) devices.

## Prerequisites

Before you begin, ensure you have the following:

1.  A Windows operating system.
2.  Administrative privileges to install software.
3.  A command-line environment (cmd.exe or PowerShell).

## Step 1: Install a C++ Compiler

TorchInductor requires a C++ compiler for code generation and optimization. Microsoft Visual C++ (MSVC) is the primary supported compiler on Windows.

1.  Download and install [Microsoft Visual Studio](https://visualstudio.microsoft.com/downloads/).
2.  During the installation, navigate to the **Workloads** tab.
3.  Select **Desktop & Mobile** and check the box for **Desktop Development with C++**.
4.  Complete the installation.

**Note:** For potentially better CPU performance, TorchInductor also supports the LLVM Compiler and the Intel oneAPI DPC++ Compiler. These alternatives still require the MSVC runtime libraries to be installed. Instructions for using these compilers are provided in the [Alternative Compilers](#alternative-compilers-for-cpu-performance) section later in this guide.

## Step 2: Set Up Your Development Environment

With the compiler installed, you need to configure your shell environment.

1.  Open a new command prompt (`cmd.exe`).
2.  Activate the MSVC build environment by running its configuration script. The path may vary depending on your Visual Studio version and installation location.
    ```bash
    "C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\Build\vcvars64.bat"
    ```
3.  Create and activate a Python virtual environment (recommended).
    ```bash
    python -m venv pytorch_inductor_env
    pytorch_inductor_env\Scripts\activate
    ```

## Step 3: Install PyTorch

Install the appropriate version of PyTorch for your target device.

*   **For CPU:** Install PyTorch 2.5 or later.
*   **For Intel XPU (GPU):** Install PyTorch 2.7 or later. Follow the official [Getting Started on Intel GPU](https://pytorch.org/docs/main/notes/get_start_xpu.html) guide for detailed installation instructions.

You can find the correct `pip` or `conda` command for your system on the [PyTorch Get Started page](https://pytorch.org/get-started/locally/).

## Step 4: Compile and Run Your First Model

Now you're ready to use `torch.compile`. Let's create a simple function, compile it, and execute it.

1.  Create a new Python script (e.g., `test_compile.py`).
2.  Add the following code. Set `device="cpu"` or `device="xpu"` based on your target.

```python
import torch

# Set your target device: "cpu" or "xpu"
device = "cpu"

def foo(x, y):
    a = torch.sin(x)
    b = torch.cos(x)
    return a + b

# Compile the function using TorchInductor
opt_foo1 = torch.compile(foo)

# Create some dummy data and run the compiled function
x = torch.randn(10, 10).to(device)
y = torch.randn(10, 10).to(device)
result = opt_foo1(x, y)

print(result)
```

3.  Run the script from your activated environment:
    ```bash
    python test_compile.py
    ```

You should see a tensor printed to the console, confirming that the function was successfully compiled and executed.

```
tensor([[-3.9074e-02,  1.3994e+00,  1.3894e+00,  3.2630e-01,  8.3060e-01,
          1.1833e+00,  1.4016e+00,  7.1905e-01,  9.0637e-01, -1.3648e+00],
        [ 1.3728e+00,  7.2863e-01,  8.6888e-01, -6.5442e-01,  5.6790e-01,
          5.2025e-01, -1.2647e+00,  1.2684e+00, -1.2483e+00, -7.2845e-01],
        ... # Output truncated for brevity
        [ 5.7244e-04,  1.2799e+00,  1.3595e+00,  1.0907e+00,  3.7191e-01,
          1.4062e+00,  1.3672e+00,  6.8502e-02,  8.5216e-01,  8.6046e-01]])
```

The first run will be slightly slower as it performs the compilation. Subsequent runs will use the cached compiled kernel for faster execution.

## Alternative Compilers for CPU Performance

To potentially achieve better performance on CPU, you can configure TorchInductor to use a different C++ compiler. **You must have MSVC installed as described in Step 1**, as these compilers depend on its runtime libraries.

### Using the Intel oneAPI DPC++ Compiler

1.  Download and install the [Intel oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler-download.html) for Windows.
2.  Before running your Python script, set the `CXX` environment variable in your command prompt to point to the Intel compiler wrapper:
    ```bash
    set CXX=icx-cl
    ```

### Using the LLVM/Clang Compiler

1.  Download and install the [LLVM Compiler](https://github.com/llvm/llvm-project/releases). Choose the pre-built binaries for Windows (64-bit).
2.  Before running your Python script, set the `CXX` environment variable:
    ```bash
    set CXX=clang-cl
    ```

After setting the `CXX` variable, activate your virtual environment and run your Python script as before. TorchInductor will use the specified compiler for code generation.

## Conclusion

In this guide, you learned how to set up `torch.compile` with TorchInductor on Windows. You installed the necessary MSVC compiler, configured your environment, installed PyTorch, and ran a compiled function. You also explored how to switch to alternative compilers like the Intel or LLVM compilers to potentially enhance CPU performance. You can now apply `torch.compile` to your own PyTorch models to leverage just-in-time compilation and optimization on Windows.