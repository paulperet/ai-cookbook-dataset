# NLP from Scratch: Building Character-Level RNNs

Welcome to this three-part tutorial series where you will build and train character-level Recurrent Neural Networks (RNNs) from scratch using PyTorch. You will learn essential Natural Language Processing (NLP) techniques by creating models that classify and generate text.

## What You Will Learn
*   How to construct and train Recurrent Neural Networks from the ground up.
*   Essential data handling and preprocessing techniques for NLP tasks.
*   How to train an RNN to identify the language of origin for a given word.

## Prerequisites
Before you begin, ensure you have a working knowledge of Python and basic machine learning concepts. We recommend reviewing the following PyTorch resources:
*   [PyTorch Learn the Basics series](https://pytorch.org/tutorials/beginner/basics/intro.html)
*   [How to install PyTorch](https://pytorch.org/get-started/locally/)

## Tutorial Series Overview

### Part 1: Classifying Names with a Character-Level RNN
**Goal:** Learn how to build and train an RNN to classify names into their language of origin.
*   **Link:** [NLP From Scratch - Part 1: Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)
*   **Key Skills:** Data preparation, RNN architecture, training loops, and evaluation.

### Part 2: Generating Names with a Character-Level RNN
**Goal:** Expand the RNN model from Part 1 to generate new names that resemble those from a target language.
*   **Link:** [NLP From Scratch - Part 2: Generating Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)
*   **Key Skills:** Sampling from network outputs, text generation, and creative applications of RNNs.

### Part 3: Translation with a Sequence to Sequence Network and Attention
**Goal:** Build a more advanced sequence-to-sequence model with an attention mechanism to translate text from French to English.
*   **Link:** [NLP From Scratch - Part 3: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
*   **Key Skills:** Encoder-decoder architecture, attention mechanisms, and handling variable-length sequences.

Each tutorial builds upon the concepts of the previous one, guiding you from foundational RNNs to more complex architectures used in modern NLP. Click the links above to get started with Part 1.