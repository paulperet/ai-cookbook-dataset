# AI Model Deployment Technologies: A Quick Reference Guide

This guide provides a concise overview of key technologies for deploying and running machine learning models, particularly large language models (LLMs), across different hardware platforms.

## Prerequisites & Setup

This is a conceptual guide. No specific installation is required to read it, but practical use of these technologies will require their respective SDKs or runtimes.

## Core Technologies

### 1. DirectML
**Purpose:** A low-level, vendor-agnostic API for hardware-accelerated machine learning on Windows.
**Key Features:**
* Built on DirectX 12 to leverage GPU acceleration.
* Works across GPU vendors (AMD, Intel, NVIDIA) without code changes.
* Primarily used for model training and inference.
* Part of the Windows AI Platform, supported on Windows 10 & 11.
* Integrates with ONNX Runtime and WinML.

**Use Case:** Deploying models on any Windows device with a compatible GPU.

### 2. CUDA
**Purpose:** A parallel computing platform and API for general-purpose processing on NVIDIA GPUs (GPGPU).
**Key Features:**
* Proprietary technology developed by NVIDIA.
* Enables massive parallelism for compute-intensive tasks.
* Widely used in machine learning, scientific computing, and video processing.
* Requires NVIDIA GPUs and the CUDA Toolkit.

**Use Case:** High-performance model training and inference on NVIDIA hardware.

### 3. ONNX (Open Neural Network Exchange)
**Purpose:** An open format for representing machine learning models, enabling framework interoperability.
**Key Features:**
* Defines an extensible computation graph model with standard operators and data types.
* Allows models to be moved between different ML frameworks (e.g., PyTorch to TensorFlow).
* ONNX Runtime executes these models efficiently across various hardware.

**Example Deployment for Phi-3 Mini:**
ONNX Runtime can run the Phi-3 Mini model with the following optimized configurations:
* **int4 DML:** Quantized to 4-bit integers via AWQ for DirectML.
* **fp16 CUDA:** 16-bit floating point for NVIDIA GPUs.
* **int4 CUDA:** Quantized to 4-bit integers via RTN for NVIDIA GPUs.
* **int4 CPU/Mobile:** Quantized to 4-bit integers via RTN for CPU and mobile devices.

**Use Case:** Creating portable, optimized models that run on servers, desktops (Windows/Linux/macOS), and mobile CPUs.

### 4. Llama.cpp
**Purpose:** A C++ library for efficient inference of LLMs (like Llama).
**Key Features:**
* Built alongside the `ggml` tensor library for performance.
* Aims for faster inference and lower memory usage than Python implementations.
* Supports hardware optimization and quantization.
* Provides a simple API and examples.

**Use Case:** Running LLMs efficiently, especially when resource-constrained. Compatible with models like Phi-3.

### 5. GGUF (Generic Graph Update Format)
**Purpose:** A format for representing and updating machine learning models, ideal for smaller language models (SLMs).
**Key Features:**
* Enables effective CPU execution with 4-8 bit quantization.
* Beneficial for rapid prototyping and edge device deployment.
* Useful in batch jobs like CI/CD pipelines.

**Use Case:** Deploying quantized SLMs on CPUs for edge computing or prototyping.

## Summary Table

| Technology | Primary Use | Key Advantage | Hardware Target |
|------------|-------------|---------------|-----------------|
| **DirectML** | ML Acceleration | Vendor-agnostic GPU access | Windows GPUs (AMD, Intel, NVIDIA) |
| **CUDA** | Parallel Computing | High-performance on NVIDIA GPUs | NVIDIA GPUs |
| **ONNX** | Model Interoperability | Framework-agnostic model format | Cross-platform (CPU/GPU) |
| **Llama.cpp** | LLM Inference | Efficient, low-memory inference | CPU/GPU (via optimizations) |
| **GGUF** | Model Format | Efficient quantization for SLMs | CPUs (edge devices) |

## Next Steps

To implement a workflow:
1. **Choose your model** (e.g., Phi-3 Mini).
2. **Select a deployment target** (Windows GPU, NVIDIA Server, Mobile CPU).
3. **Apply the appropriate technology** (e.g., ONNX + DirectML for Windows GPU).
4. **Optimize with quantization** (e.g., int4 via GGUF for CPU deployment).

These technologies provide the foundation for building scalable, efficient AI applications across diverse hardware environments.