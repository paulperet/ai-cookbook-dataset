# Getting Started with Phi-3 Locally: A Step-by-Step Guide

This guide walks you through setting up your local environment to run Microsoft's Phi-3 model using Ollama. You'll learn how to install the necessary tools, pull the model, and interact with it via both the command line and Python.

## Prerequisites & Setup

Before you begin, ensure you have the following tools installed on your system:

1.  **Ollama**: The platform for running large language models locally. Download it from [ollama.com](https://ollama.com/).
2.  **Python 3.10 or later**: Required for running the Python examples. Download it from [python.org](https://www.python.org/downloads/).
3.  **OpenAI Python SDK**: This library provides a convenient interface for interacting with local models via Ollama. Install it using pip:

    ```bash
    pip install openai
    ```

## Step 1: Download and Run the Phi-3 Model

First, you'll use Ollama to pull the `phi3:mini` model to your machine and start an interactive chat session.

1.  Open your terminal or command prompt.
2.  Run the following command. This will download the model (this may take a few minutes depending on your internet connection).

    ```bash
    ollama run phi3:mini
    ```

3.  Once the download is complete and you see a `>>>` prompt, the model is ready. You can now send it a message directly. For example:

    ```bash
    >>> Write a haiku about hungry hippos
    ```

4.  After a few seconds, the model will stream its response back to you.

**Congratulations!** You've successfully run a large language model locally. To exit the interactive session, type `/bye`.

## Step 2: Interact with the Model from Python

While the command line is useful for quick tests, you'll typically want to integrate the model into your applications. Let's use Python to have a structured conversation.

1.  Create a new Python file (e.g., `chat.py`).
2.  Add the following code. This script uses the OpenAI client library, configured to point to your local Ollama server, to send a series of messages to the model.

    ```python
    from openai import OpenAI

    # Point the client to your local Ollama server
    client = OpenAI(
        base_url='http://localhost:11434/v1',
        api_key='ollama', # Required by the library, but not used by Ollama
    )

    MODEL_NAME = 'phi3:mini' # Change this if you are using a different model

    # Define the conversation. The 'system' message sets the assistant's behavior.
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is the capital of France?"},
            {"role": "assistant", "content": "The capital of France is Paris."},
            {"role": "user", "content": "And what is a popular landmark there?"}
        ]
    )

    # Print the model's final response
    print(response.choices[0].message.content)
    ```

3.  Save the file and run it from your terminal:

    ```bash
    python chat.py
    ```

4.  You should see the model's answer printed in your terminal. This demonstrates a multi-turn conversation where the model remembers the previous context (that we were talking about Paris).

## Next Steps: Explore Advanced Techniques

Now that you have the basics working, you can explore more advanced patterns. The provided `ollama.ipynb` Jupyter notebook contains examples of common techniques used with language models, such as:

*   Adjusting model parameters (temperature, top_p).
*   Using different prompt formats.
*   Implementing few-shot learning.

To continue learning:
1.  Open the notebook `ollama.ipynb`.
2.  Run each cell sequentially.
3.  If you are using a model other than `phi3:mini`, remember to update the `MODEL_NAME` variable in the first cell.

You have successfully set up a local AI development environment. You can now build applications using the powerful and efficient Phi-3 model directly on your own machine.