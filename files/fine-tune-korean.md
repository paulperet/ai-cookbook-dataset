ì´ ë…¸íŠ¸ë¶ì€ OpenAIì˜  **gpt-oss (openâ€‘weight)** ëª¨ë¸ì„ **í•œêµ­ ë‰´ìŠ¤ ë¬¸ì²´ + ìµœì‹  ëŒ€í™”ì²´**ë¡œ ì„¸ë°€ íŠœë‹í•˜ëŠ” ë°©ë²•ì„
í•œêµ­ì–´/ì˜ì–´ **ì´ì¤‘ ì–¸ì–´**ë¡œ ì œê³µí•©ë‹ˆë‹¤.  
This notebook shows how to fineâ€‘tune OpenAI's **gpt-oss (openâ€‘weight)** models for **Korean news style + modern chat tone**, in **Korean & English**.

---

### MXFP4 workflow clarifications Â· MXFP4 ì›Œí¬í”Œë¡œ ì •ë¦¬

**EN:**  
- Training or fine-tuning **directly in MXFP4 is not supported** by public frameworks today.  
- Recommended path: train in **BF16** (or **QLoRA 4â€‘bit nf4**) â†’ **merge LoRA** â†’ **postâ€‘training quantize to MXFP4** â†’ `save_pretrained()` for deployment.  
- If you need an MXFP4 artifact, you must **reâ€‘quantize from BF16** after merging adapters. (Export utilities are evolving; if your toolchain already supports MXFP4 serialization, thatâ€™s ideal.)

**KR:**  
- í˜„ìž¬ ê³µê°œ í”„ë ˆìž„ì›Œí¬ì—ì„œëŠ” **MXFP4ë¡œ ì§ì ‘ í•™ìŠµ/íŒŒì¸íŠœë‹**ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  
- ê¶Œìž¥ ê²½ë¡œ: **BF16**(ë˜ëŠ” **QLoRA 4â€‘bit nf4**)ë¡œ í•™ìŠµ â†’ **LoRA ë³‘í•©** â†’ **ì‚¬í›„(MXFP4) ì–‘ìží™”** â†’ ë°°í¬ìš©ìœ¼ë¡œ `save_pretrained()` ì €ìž¥.  
- MXFP4 ì•„í‹°íŒ©íŠ¸ê°€ í•„ìš”í•˜ë©´, ì–´ëŒ‘í„° ë³‘í•© í›„ **BF16 â†’ MXFP4 ìž¬ì–‘ìží™”**ê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì§ë ¬í™” ìœ í‹¸ì€ ì§„í™” ì¤‘ì´ë©°, íˆ´ì²´ì¸ì—ì„œ MXFP4 ì €ìž¥ì„ ì§€ì›í•˜ë©´ ê°€ìž¥ ì¢‹ìŠµë‹ˆë‹¤.)

---

### LoRA targets (MoE) Â· LoRA íƒ€ê¹ƒ(MoE í¬í•¨)

**EN:**  
- Minimal config (fast, low VRAM): target attention only, e.g. `["q_proj","v_proj"]`.  
- MoEâ€‘aware config (better domain adaptation, more VRAM/time): include **expert projection layers** in addition to attention.  

```python
from peft import LoraConfig

TARGET_MODULES = ["q_proj", "v_proj"]  # baseline
MOE_TARGET_PARAMETERS = [
    # example expert layers; adjust indices to your model depth
    "mlp.experts.gate_up_proj",
    "mlp.experts.down_proj",
]

lora_cfg = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05,
    target_modules="all-linear",              # cover all linear layers
    target_parameters=MOE_TARGET_PARAMETERS,  # add expert projections
    bias="none", task_type="CAUSAL_LM",
)
```

- Start with attentionâ€‘only; if KR domain fit is insufficient, enable MoE targets and reâ€‘eval.

**KR:**  
- ìµœì†Œ êµ¬ì„±(ë¹ ë¥´ê³  VRAM ì ˆì•½): `["q_proj","v_proj"]` ë“± **ì–´í…ì…˜ë§Œ** ì ìš©.  
- **MoE ì¸ì§€ êµ¬ì„±**(ë„ë©”ì¸ ì í•©ì„±â†‘, ìžì› ì†Œëª¨â†‘): ì–´í…ì…˜ì— **ì „ë¬¸ê°€(Expert) íˆ¬ì˜ ë ˆì´ì–´**ë¥¼ ì¶”ê°€ë¡œ í¬í•¨.  
- ë¨¼ì € ì–´í…ì…˜ë§Œìœ¼ë¡œ ì‹œë„í•œ ë’¤, í•œêµ­ì–´ ë„ë©”ì¸ ì í•©ì„±ì´ ë¶€ì¡±í•˜ë©´ MoE íƒ€ê¹ƒì„ ì¼œê³  ìž¬í‰ê°€í•˜ì„¸ìš”.

## Contents Â· ëª©ì°¨
0) Goals & Scope Â· ëª©í‘œ & ë²”ìœ„  
1) Environment check Â· í™˜ê²½ ì ê²€  
2) ì„¤ì •ê°’ Â· Config  
3) íŒ¨í‚¤ì§€ ì„¤ì¹˜ Â· Install Deps  
4) ë°ì´í„° ì†Œì‹±(í•œêµ­í˜•) Â· KRâ€‘Context Data Sourcing  
5) ìƒ˜í”Œ ë°ì´í„° ìƒì„± Â· Create Sample Data  
6) ì „ì²˜ë¦¬(PIPA) & ìŠ¤íƒ€ì¼ ë¼ë²¨ Â· PII Scrubbing & Style Tags  
7) ë°ì´í„° ë¡œë”©/í¬ë§·íŒ… Â· Load & Format  
8) ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ Â· Load Model & Tokenizer  
9) Fineâ€‘Tuning (LoRA/QLoRA) Â· ì„¸ë°€ íŠœë‹  
   9a) Data curation & splits  
   9b) Hyperparameters (r/alpha/dropout)  
   9c) Merge adapters (BF16)  
   9d) Save merged BF16 (`save_pretrained`)  
   9e) Export & Quantize (BF16 â†’ MXFP4) Â· ë‚´ë³´ë‚´ê¸° & ì–‘ìží™”  
10) í‰ê°€(ë‰´ìŠ¤/ëŒ€í™”) Â· Evaluation (News/Chat)  
11) Inference Prompt Templates Â· ì¶”ë¡  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿  
12) ìµœì‹ ì„± ìœ ì§€ Â· Freshness Strategy  
13) ì•ˆì „/ì»´í”Œë¼ì´ì–¸ìŠ¤ Â· Safety & Compliance  
14) ë¬¸ì œí•´ê²° & ë‹¤ìŒ ë‹¨ê³„ Â· Troubleshooting & Next Steps


### âš™ï¸ Training vs Quantization â€” Whatâ€™s supported
- **Do:** Train with BF16/FP16 or QLoRA; export merged weights.
- **Then:** Quantize to **MXFP4** for inference using provided conversion scripts/utilities.
- **Donâ€™t:** Attempt to run an endâ€‘toâ€‘end â€œtrain in MXFP4â€ pipeline â€” not supported today.

> **PII & Compliance Reminder:** For KR data, follow your enterprise policy (mask RRN/phone/account IDs, remove emails) **before** training & logging. Keep train/val/test splits stratified by source and style tags.

### ðŸ§ª MoE adapters (optional)
You can target MoE layers with adapters, but treat this as **advanced/experimental**. Start with attention projections first and validate KR benchmarks before expanding scope.

> **Note:** Keep `transformers`, `peft`, `accelerate`, and `trl` at versions known to support BF16/4â€‘bit LoRA.  
If you pin `safetensors`, remember that **native MXFP4 serialization is not yet standardized**; loaders may upcast internally.

### ðŸ”Ž Support Matrix â€” At a glance
- **Fineâ€‘tuning precision:** BF16/FP16 âœ… Â· QLoRA 4â€‘bit âœ… Â· **MXFP4 FT âŒ**
- **Quantization target:** MXFP4 âœ… (postâ€‘training)
- **API FT (hosted) for OSS models:** âŒ
- **Openâ€‘source FT (Transformers/TRL/PEFT):** âœ…
- **LoRA targets:** `q_proj`, `k_proj`, `v_proj`, `o_proj` âœ…; MoE expert adapters **experimental** âš ï¸

---

## 0) Goals & Scope Â· ëª©í‘œ & ë²”ìœ„
- **KR**: í•œêµ­ì–´ ì¼ë°˜ ë‰´ìŠ¤ + ì¼ìƒ/ìƒë‹´ ëŒ€í™”ì²´ì— ìµœì í™”. `style=news_headline|news_lead|news_body|kakao_casual|kakao_formal` ì œì–´.
- **EN**: Optimize for Korean news writing and modern chat tone; control output via style tags above.
- **Stack**: `transformers`, `trl(SFTTrainer)`, `peft(LoRA/QLoRA)`, `datasets`.
- **Hardware**: Single/few GPUs (BF16 preferred). CPU/Mac for lightweight tests.

## 1) Environment check Â· í™˜ê²½ ì ê²€


```python
import os, sys, platform
print("Python:", sys.version)
print("OS/Platform:", platform.platform())
print("CUDA_VISIBLE_DEVICES:", os.environ.get("CUDA_VISIBLE_DEVICES", ""))

try:
    import torch
    print("Torch:", torch.__version__, "CUDA:", torch.cuda.is_available())
    if torch.cuda.is_available():
        print("GPU:", torch.cuda.get_device_name(0))
except Exception as e:
    print("Torch not installed or GPU not detected:", e)
```

    Python: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
    OS/Platform: Linux-6.8.0-60-generic-x86_64-with-glibc2.35
    CUDA_VISIBLE_DEVICES: 
    Torch: 2.7.1+cu126 CUDA: True
    GPU: NVIDIA H100 80GB HBM3


## 2) ì„¤ì •ê°’ Â· Config


```python
from pathlib import Path
import os

# === Model & Training Params ===
BASE_URL = "http://localhost:8000/v1"     # vLLM OpenAI-compatible endpoint
API_KEY  = "dummy-key"                     # vLLM ignores; SDK requires a value
MODEL    = "openai/gpt-oss-120b"           # must match the model vLLM loaded
OUTPUT_DIR = "ft-oss-kr-news-chat-bilingual"

# Data mix (news : chat)
MIX_NEWS = 0.6
MIX_CHAT = 0.4

# LoRA
LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
TARGET_MODULES = ["q_proj", "v_proj"]  # adjust per model

# Training
EPOCHS = 1
PER_DEVICE_BS = 2
GRAD_ACCUM = 8
LEARNING_RATE = 2e-4
BF16 = True
LOG_STEPS = 20
SAVE_STEPS = 200
SAVE_TOTAL_LIMIT = 2

print("Config ready.")
```

    Config ready.


## 3) íŒ¨í‚¤ì§€ ì„¤ì¹˜ Â· Install Deps


```python
# %pip install --upgrade pip
# %pip install transformers accelerate datasets peft trl bitsandbytes sentencepiece
# (optional) serving/runtimes
# %pip install vllm
# %pip install llama-cpp-python

import importlib, pip

for dep in ["transformers","accelerate","datasets","peft","trl",
            "bitsandbytes","sentencepiece","vllm","llama_cpp"]:
    try:
        print(f"{dep}: {importlib.import_module(dep).__version__}")
    except Exception:
        print(f"{dep}: not installed")

print(f"pip: {pip.__version__}")

print("Install cells are commented. Un-comment in your environment.")
```

    transformers: 4.55.3
    accelerate: 1.10.0
    datasets: 4.0.0
    peft: not installed
    trl: 0.21.0
    bitsandbytes: not installed
    sentencepiece: 0.2.1
    vllm: 0.10.1
    llama_cpp: 0.3.16
    pip: 25.2
    Install cells are commented. Un-comment in your environment.


## 4) ë°ì´í„° ì†Œì‹±(í•œêµ­í˜•) Â· KRâ€‘Context Data Sourcing

**KR**  
- ê³µê°œ ë²¤ì¹˜ë§ˆí¬(ì£¼ì œ ë¶„ë¥˜/ìš”ì•½/QA) + **í—ˆìš©ëœ ë‰´ìŠ¤ APIì˜ ë©”íƒ€ë°ì´í„°(ì œëª©/ìš”ì•½/ì„¹ì…˜)** ì¤‘ì‹¬ìœ¼ë¡œ ìŠ¤íƒ€ì¼ ë³´ì •.
- ê¸°ì‚¬ **ì›ë¬¸ ëŒ€ëŸ‰ ìž¬í•™ìŠµì€ ì €ìž‘ê¶Œ/ì•½ê´€ ì´ìŠˆ** â†’ ë©”íƒ€ë°ì´í„°Â·ê³µê°œ ì½”í¼ìŠ¤ ìœ„ì£¼.
- ëŒ€í™”ì²´ëŠ” í•©ë²• ê³µê°œ ì½”í¼ìŠ¤(ë°˜ë§/ì¡´ëŒ“ë§/ì´ëª¨í‹°ì½˜/ì¶•ì•½ì–´ ë¼ë²¨ í¬í•¨) ìš°ì„ .
- PIPA: ì£¼ë¯¼ë²ˆí˜¸/ì—°ë½ì²˜/ì´ë©”ì¼/ê³„ì¢Œ ë“± ê°œì¸ì •ë³´ëŠ” **í›ˆë ¨ ì „/ë¡œê·¸ ì „** ìŠ¤í¬ëŸ¬ë¹™.

**EN**  
- Prefer public KR benchmarks (topic classification / summarization / QA) and **allowed news API metadata** for style calibration.
- Avoid mass training on news full texts due to license/ToS constraints; use metadata + open corpora.
- For chat, use lawful open corpora with tone/emoji/informalâ€‘formal annotations.
- Scrub PII (phone, RRNs, emails, accounts) before training/logging.

## 5) ìƒ˜í”Œ ë°ì´í„° ìƒì„± Â· Create Sample Data


```python
import json, pathlib
pathlib.Path("data").mkdir(exist_ok=True)

news_samples = [
  {"style":"news_lead","topic":"ê²½ì œ","title":"ë°˜ë„ì²´ ìˆ˜ì¶œ í˜¸ì¡°â€¦ 7ì›” ìˆ˜ì¶œì•¡ 20% ì¦ê°€","summary":"ìˆ˜ì¶œ ê°œì„ ì„¸ê°€ ì´ì–´ì§€ë©° ê²½ê¸° íšŒë³µ ê¸°ëŒ€ê°€ ì»¤ì¡Œë‹¤."},
  {"style":"news_headline","topic":"ì •ì¹˜","title":"êµ­íšŒ, ë°ì´í„° ì‚°ì—… ìœ¡ì„±ë²• ë³¸íšŒì˜ í†µê³¼","summary":"ë°ì´í„° í™œìš© ì´‰ì§„ê³¼ ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ê°•í™”í•˜ëŠ” ë‚´ìš©."},
  {
    "style": "news_lead",
    "topic": "ê²½ì œ",
    "title": "ì¹´ì¹´ì˜¤íŽ˜ì´ ë³´ì•ˆ ì ê²€â€¦ ê³ ê°ë¬¸ì˜: help+vip@corp.co.kr",
    "summary": "ê³ ê°ì„¼í„° 010-1234-5678ë¡œ ë¬¸ì˜ í­ì£¼. ê³„ì¢Œ 110-123-456789 ê´€ë ¨ ê²°ì œ ì˜¤ë¥˜ ë…¼ëž€."
  },
  {
    "style": "news_headline",
    "topic": "ì‚¬íšŒ",
    "title": "ê°œì¸ì •ë³´ ìœ ì¶œ ì˜í˜¹â€¦ ì£¼ë¯¼ë²ˆí˜¸ 901010-1234567 ìœ í†µ ì£¼ìž¥",
    "summary": "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ëž€ë¡œ 123ì—ì„œ ìžë£Œ í™•ë³´â€¦ ë‹´ë‹¹ìž john.doe+news@example.com"
  }
]

chat_samples = [
  {"style":"kakao_casual","dialog":["ì£¼ë§ì— ë¹„ ì˜¨ëŒ€?","ì‘ ì¼ìš”ì¼ì— ê½¤ ì˜¨ë‹¤ë”ë¼ â˜”","í— ìš°ì‚° ì±™ê²¨ì•¼ê² ë‹¤"]},
  {"style":"kakao_formal","dialog":["ì•ˆë…•í•˜ì„¸ìš”. ë°°ì†¡ ì¼ì • í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.","ë‚´ì¼ ì¤‘ ë„ì°© ì˜ˆì •ìž…ë‹ˆë‹¤.","ì•ˆë‚´ ê°ì‚¬í•©ë‹ˆë‹¤."]},
  {
    "style": "kakao_formal",
    "dialog": [
      "ë°°ì†¡ í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ì£¼ë¬¸ë²ˆí˜¸ ORD-2025-0001 ìž…ë‹ˆë‹¤.",
      "ì—°ë½ì²˜ëŠ” 010-2222-3333 ìž…ë‹ˆë‹¤. (ìœ ë‹ˆì½”ë“œ í•˜ì´í”ˆ)",
      "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ëŠ” ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    ]
  }
]

with open("data/news.jsonl","w",encoding="utf-8") as f:
    for ex in news_samples: f.write(json.dumps(ex, ensure_ascii=False)+"\n")
with open("data/chat.jsonl","w",encoding="utf-8") as f:
    for ex in chat_samples: f.write(json.dumps(ex, ensure_ascii=False)+"\n")

print("Created: data/news.jsonl, data/chat.jsonl")
```

    Created: data/news.jsonl, data/chat.jsonl


## 6) ì „ì²˜ë¦¬(PIPA) & ìŠ¤íƒ€ì¼ ë¼ë²¨ Â· PII Scrubbing & Style Tags


```python
# Step 6 â€” PII scrubbing + style tags (no Harmony here)
import json, re, unicodedata
from pathlib import Path

# --- Normalization helpers ---
HYPHENS = dict.fromkeys(map(ord, "â€-â€’â€“â€”â€•ï¹˜ï¹£ï¼"), ord("-"))  # map unicode hyphens â†’ ASCII
def normalize(s: str) -> str:
    if not isinstance(s, str): return s
    s = unicodedata.normalize("NFKC", s)
    s = s.translate(HYPHENS)
    return s

# --- PII patterns (illustrative; tune for production) ---
RE_EMAIL = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
# KR mobile numbers with spaces/hyphens: 010-1234-5678, 010 1234 5678, etc.
RE_PHONE = re.compile(r"\b01[016789][-\s]?\d{3,4}[-\s]?\d{4}\b")
# Korean RRN (ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸) basic pattern
RE_RRN = re.compile(r"\b\d{6}-\d{7}\b")
# Bank-ish account numbers: strictly digits in groups (avoid codes with letters)
RE_ACCOUNT = re.compile(r"\b\d{2,3}-\d{2,4}-\d{3,6}\b")
# Very simple postal address cue (city names) â€“ conservative, just redact the token (optional)
RE_CITY = re.compile(r"(ì„œìš¸íŠ¹ë³„ì‹œ|ë¶€ì‚°ê´‘ì—­ì‹œ|ëŒ€êµ¬ê´‘ì—­ì‹œ|ì¸ì²œê´‘ì—­ì‹œ|ê´‘ì£¼ê´‘ì—­ì‹œ|ëŒ€ì „ê´‘ì—­ì‹œ|ìš¸ì‚°ê´‘ì—­ì‹œ|ì„¸ì¢…íŠ¹ë³„ìžì¹˜ì‹œ|ê²½ê¸°ë„|ê°•ì›ë„|ì¶©ì²­ë¶ë„|ì¶©ì²­ë‚¨ë„|ì „ë¼ë¶ë„|ì „ë¼ë‚¨ë„|ê²½ìƒë¶ë„|ê²½ìƒë‚¨ë„|ì œì£¼íŠ¹ë³„ìžì¹˜ë„)")

# Allowlist: things that look like PII but arenâ€™t (e.g., bill/order codes w/ letters)
def looks_like_code(s: str) -> bool:
    return bool(re.search(r"[A-Za-z]", s))  # if letters present, treat as code, not account/phone

# Order of application matters (longest/most specific first sometimes helps)
SCRUBBERS = [
    ("[RRN]", RE_RRN),
    ("[EMAIL]", RE_EMAIL),
    ("[PHONE]", RE_PHONE),
    ("[ACCOUNT]", RE_ACCOUNT),
    ("[CITY]", RE_CITY),  # optional; comment out if you don't want to redact city tokens
]

def scrub_text(text: str) -> tuple[str, dict]:
    """Return (scrubbed_text, hits_dict). Avoid false positives with basic allowlisting."""
    if not isinstance(text, str) or not text:
        return text, {}
    orig = text
    text = normalize(text)
    hits = {}

    # Guard account-like and phone-like strings that contain letters (likely codes)
   