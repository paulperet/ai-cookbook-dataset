```python
import torch 
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline 
```

[2024-08-20 03:12:24.098444: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory, 2024-08-20 03:12:24.098474: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.]

```python
torch.random.manual_seed(0) 
model = AutoModelForCausalLM.from_pretrained( 
    "../phi-3-instruct",  
    device_map="cuda",  
    torch_dtype="auto",  
    trust_remote_code=True,  
) 
```

[Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]]

```python
tokenizer = AutoTokenizer.from_pretrained("../phi-3-instruct") 
```

You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.

```python
# messages = [ 
#     {"role": "system", "content": "你是我的人工智能助手，协助我用中文解答问题"}, 
#     {"role": "user", "content": "你能否介绍一下如何提升学习效率吗?"}, 
# ] 

# <|system|>
# You are a helpful assistant.<|end|>
# <|user|>
# Question?<|end|>
# <|assistant|>

messages = "<|system|>\n 你是我的人工智能助手，协助我用中文解答问题.\n<|end|><|user|>\n 你知道长沙吗？? \n<|end|><|assistant|>"
```

```python
pipe = pipeline( 
    "text-generation", 
    model=model, 
    tokenizer=tokenizer, 
) 
```

```python
generation_args = { 
    "max_new_tokens": 1024, 
    "return_full_text": False, 
    "temperature": 0.3, 
    "do_sample": False, 
} 
```

```python
output = pipe(messages, **generation_args) 
```

```python
output[0]['generated_text']
```

    ' 是的，我知道长沙。长沙是中国南部的一座大城市，位于湖南省中部，是中国的国际性大都市。它拥有丰富的历史和文化，曾是明朝的都城。长沙以其繁华的商业、独特的自然风光和丰富的历史遗迹而闻名，如橘子洲、岳麓山和长沙博物院。此外，长沙也是中国重要的科技和教育中心，包括中国工程院和中国科学院长沙分院。'