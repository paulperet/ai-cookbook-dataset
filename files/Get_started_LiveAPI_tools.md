# Gemini Live - Multimodal live API tool use with websockets

This notebook provides examples of how to use tools with the Multimodal Live API with the Gemini models. The API provides Google Search, Code Execution and Function Calling.

This tutorial assumes you are familiar with the Live API, as described in the [Live API starter tutorial](./Get_started_LiveAPI.ipynb).

Note: This version of the tutorial uses websockets directly. The [SDK version of this tutorial](../../quickstarts/Get_started_LiveAPI_tools.ipynb) is a bit simpler because the SDK handles some of the details for you.

## Set up

To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../../quickstarts/Authentication.ipynb) for an example.


```
from google.colab import userdata
GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
```

Install the `websockets` package:


```
%pip install -q websockets
```

import the necessary modules


```
import asyncio
import base64
import contextlib
import os
import json
import wave

from IPython import display

from websockets.asyncio.client import connect
```


```
uri = f"wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent?key={GOOGLE_API_KEY}"

model = "models/gemini-2.0-flash-live-001"
```

Define a context manager to convert streamed PCM data into a wave file that can be played directly using an IPython audio widget.


```
@contextlib.contextmanager
def wave_file(filename, channels=1, rate=24000, sample_width=2):
    with wave.open(filename, "wb") as wf:
        wf.setnchannels(channels)
        wf.setsampwidth(sample_width)
        wf.setframerate(rate)
        yield wf
```

And define a custom logger so you can toggle extra information, like the in-flight requests and responses.


```
import logging

logger = logging.getLogger("Live")
logger.setLevel("INFO")
```

These helpers handle the websocket connection and prompt transmission (`run` and `send`), server handshake (`setup`) and process server responses (`handle_server_content`, `handle_tool_call`).


```
async def setup(ws, modality, tools):
  setup = {
      "setup": {
          "model": model,
          "tools": tools,
          "generation_config": {
              "response_modalities": [modality]
              }}}
  await ws.send(json.dumps(setup))
  setup_response = json.loads(await ws.recv())
  logger.debug(setup_response)

async def send(ws, prompt):
  msg = {
    "client_content": {
      "turns": [{"role": "user", "parts": [{"text": prompt}]}],
      "turn_complete": True,
    }
  }
  print(">>> ", msg)
  await ws.send(json.dumps(msg))


def handle_server_content(wf, server_content):
  audio = False
  model_turn = server_content.pop("modelTurn", None)
  if model_turn:
    text = model_turn["parts"][0].pop("text", None)
    if text is not None:
      print(text)

    inline_data = model_turn['parts'][0].pop('inlineData', None)
    if inline_data is not None:
      print('.', end='')
      b64data = inline_data['data']
      pcm_data = base64.b64decode(b64data)
      wf.writeframes(pcm_data)
      audio = True

  turn_complete = server_content.pop('turnComplete', None)
  return turn_complete, audio


async def handle_tool_call(ws, tool_call):
  print("    ", tool_call)
  for fc in tool_call['functionCalls']:

    msg = {
      'tool_response': {
          'function_responses': [{
              'id': fc['id'],
              'name': fc['name'],
              'response':{'result': {'string_value': 'ok'}}
          }]
        }
    }
    print('>>> ', msg)
    await ws.send(json.dumps(msg))



async def run(prompt, modality='TEXT', tools=None):
  if tools is None:
    tools=[]

  async with (
      connect(uri, additional_headers={"Content-Type": "application/json"}) as ws,
  ):
    await setup(ws, modality, tools)
    await send(ws, prompt)

    audio = False
    filename = 'audio.wav'
    with wave_file(filename) as wf:
      async for raw_response in ws:
        response = json.loads(raw_response.decode())
        logger.debug(str(response)[:150])

        server_content = response.pop("serverContent", None)
        if server_content is not None:
          turn_complete, a = handle_server_content(wf, server_content)
          audio = audio or a
          if turn_complete:
            print()
            print('Turn complete')
            break

        tool_call = response.pop('toolCall', None)
        if tool_call is not None:
          await handle_tool_call(ws, tool_call)

  if audio:
    display.display(display.Audio(filename, autoplay=True))
```

Run a test prompt to ensure everything is set up.


```
await run(prompt='Hello?')
```

    >>>  {'client_content': {'turns': [{'role': 'user', 'parts': [{'text': 'Hello?'}]}], 'turn_complete': True}}
    Hello
    ! How can I help you today?
    
    
    Turn complete


## Simple function call

Define some stub functions to use in a function calling example.


```
turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}
turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}
```

Send the function declarations as part of the `tools` (in the generation config).


```
prompt = "Turn on the lights"

tools = [
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools)
```

    >>>  {'client_content': {'turns': [{'role': 'user', 'parts': [{'text': 'Turn on the lights'}]}], 'turn_complete': True}}
         {'functionCalls': [{'name': 'turn_on_the_lights', 'args': {}, 'id': 'function-call-97628297814691086'}]}
    >>>  {'tool_response': {'function_responses': [{'id': 'function-call-97628297814691086', 'name': 'turn_on_the_lights', 'response': {'result': {'string_value': 'ok'}}}]}}
    OK,
     I've turned on the lights.
    
    
    Turn complete


Try the same thing again, but using audio-out this time.


```
prompt = "Turn on the lights"

tools = [
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools, modality = "AUDIO")
```

    >>>  {'client_content': {'turns': [{'role': 'user', 'parts': [{'text': 'Turn on the lights'}]}], 'turn_complete': True}}
         {'functionCalls': [{'name': 'turn_on_the_lights', 'args': {}, 'id': 'function-call-2750470989785539634'}]}
    >>>  {'tool_response': {'function_responses': [{'id': 'function-call-2750470989785539634', 'name': 'turn_on_the_lights', 'response': {'result': {'string_value': 'ok'}}}]}}
    [., ..., .]
    Turn complete


## Code execution

The API can generate and execute code during the conversation too.


```
prompt="What is the largest prime palindrome under 100000"

tools = [
    {'code_execution': {}}
]

await run(prompt, tools=tools, modality='AUDIO')
```

    >>>  {'client_content': {'turns': [{'role': 'user', 'parts': [{'text': 'What is the largest prime palindrome under 100000'}]}], 'turn_complete': True}}
    [., ..., .]
    Turn complete


## Google search

A `google_search` tool is also available for use during live conversations.


```
prompt="Can you use google search tell me about the largest earthquake in california the week of Dec 5 2024?"

tools = [
   {'google_search': {}}
]

await run(prompt, tools=tools, modality='TEXT')
```

    >>>  {'client_content': {'turns': [{'role': 'user', 'parts': [{'text': 'Can you use google search tell me about the largest earthquake in california the week of Dec 5 2024?'}]}], 'turn_complete': True}}
    Based
     on the search results, the largest earthquake in California during the week of December 5, 
    2024, was a magnitude 7.0 earthquake that struck offshore
     of Cape Mendocino on December 5, 2024, at 10:44 a.m. local time.
    
    Here are
     some details about the earthquake:
    
    *   **Magnitude:** 7.0
    *   **Date:** December 5, 2024
    *
       **Location:** Offshore of Cape Mendocino, about 60 miles (100 kilometers) west of Ferndale, Northern California
    *   **Tectonic Setting:** Occurred in the Mendocino Triple Junction, where the
     Pacific, Juan de Fuca, and North American tectonic plates meet.
    *   **Tsunami Warning:** A tsunami warning was issued for parts of coastal Oregon and California but was later lifted.
    *   **Aftershocks:** Several
     aftershocks were recorded following the main earthquake.
    *   **Impact:** The earthquake was felt throughout Northern California and parts of Oregon. While it was a significant earthquake, it caused little immediate damage due to its remote location.
    
    
    Turn complete


Try the same again, with audio.


```
prompt="Can you use google search tell me about the largest earthquake in california the week of Dec 5 2024?"

tools = [
   {'google_search': {}}
]

await run(prompt, tools=tools, modality='AUDIO')
```

    >>>  {'client_content': {'turns': [{'role': 'user', 'parts': [{'text': 'Can you use google search tell me about the largest earthquake in california the week of Dec 5 2024?'}]}], 'turn_complete': True}}
    [., ..., .]
    Turn complete


## Compositional Function Calling

Compositional function calling allows you to ask the model to use your provided functions in generated code. In this example, you can test this by asking for a `sleep` before calling the provided tool.


```
prompt = """
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
  """

tools = [
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

import time
start = time.time()
await run(prompt, tools=tools, modality="AUDIO")
end = time.time()
print(f'Elapsed: {end-start}s')
```

    >>>  {'client_content': {'turns': [{'role': 'user', 'parts': [{'text': '\n  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n  '}]}], 'turn_complete': True}}
         {'functionCalls': [{'name': 'turn_on_the_lights', 'args': {}, 'id': 'function-call-1800692609416461513'}]}
    >>>  {'tool_response': {'function_responses': [{'id': 'function-call-1800692609416461513', 'name': 'turn_on_the_lights', 'response': {'result': {'string_value': 'ok'}}}]}}
         {'functionCalls': [{'name': 'turn_off_the_lights', 'args': {}, 'id': 'function-call-4553691635797568566'}]}
    >>>  {'tool_response': {'function_responses': [{'id': 'function-call-4553691635797568566', 'name': 'turn_off_the_lights', 'response': {'result': {'string_value': 'ok'}}}]}}
    [., ..., .]
    Turn complete


    Elapsed: 18.519413948059082s


## Multi-tool

The model can be asked to use multiple tools in a single conversational turn. In this example, a single prompt is used to perform 3 tasks using all 3 provided tools.


```
prompt = """
  Hey, I need you to do three things for me.

  1. Turn on the lights
  2. Then compute the largest prime plaindrome under 100000.
  3. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?

  Thanks!
  """

tools = [
    {'google_search': {}},
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools, modality="AUDIO")
```

    >>>  {'client_content': {'turns': [{'role': 'user', 'parts': [{'text': '\n  Hey, I need you to do three things for me.\n\n  1. Turn on the lights\n  2. Then compute the largest prime plaindrome under 100000.\n  3. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?\n\n  Thanks!\n  '}]}], 'turn_complete': True}}
         {'functionCalls': [{'name': 'turn_on_the_lights', 'args': {}, 'id': 'function-call-1123065684868959924'}]}
    >>>  {'tool_response': {'function_responses': [{'id': 'function-call-1123065684868959924', 'name': 'turn_on_the_lights', 'response': {'result': {'string_value': 'ok'}}}]}}
    [., ..., .]
    Turn complete


## Next steps

This tutorial just shows basic usage of the Live API, using the Python GenAI SDK.

- If you aren't looking for code, and just want to try multimedia streaming use [Live API in Google AI Studio](https://aistudio.google.com/app/live).
- If you want to see how to setup streaming interruptible audio and video using the Live API and the SDK see the [Audio and Video input Tutorial](../../quickstarts/Get_started_LiveAPI.py).
- There is a [Streaming audio in Colab example](../../quickstarts/websockets/LiveAPI_streaming_in_colab.ipynb), but this is more of a **demo**, it's **not optimized for readability**.
- Other nice Gemini examples can also be found in the [Cookbook](https://github.com/google-gemini/cookbook/blob/main/gemini-2/).