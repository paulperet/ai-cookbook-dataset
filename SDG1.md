# Synthetic Data generation (Part 1)

Synthetic data generation using large language models (LLMs) offers a powerful solution to a commonly faced problem: the availability of high-quality, diverse, and privacy-compliant data. This could be used in a number of scenarios such as training a data science  machine learning model (SVMs, decision trees, KNN's), finetuning a different GPT model on the data, as a solution to the coldstart problem, helping build compelling demos/apps with realistic data, scenario testing etc.

There are a number of key drivers which may see you wanting to leverage synthetic data. 
1. Human data may have privacy restrictions and/or identifiable data within it which we do not want to be used. 
2. Synthetic data can be much more structured and therefore easier to manipulate than real data. 
3. In domains where data is sparse or data of certain categories is sparse we may want to augment the data. 
4. When dealing with imbalanced datasets or datasets which lack diversity, we may want to create data to improve the richness of our datasets.

Unlike traditional data augmentation or manual data creation methods, using LLMs allows for the generation of rich, nuanced, and contextually relevant datasets that can significantly enhance it's usefulness to enterprises and developers.

We split this tutorial into 2 parts. In this cookbook, we will have the following agenda:
1. CSV with a structured prompt
2. CSV with a Python program
3. Multitable CSV with a python program
4. Simply creating textual data
5. Dealing with imbalanced or non-diverse textual data
while in part 2, we will look at prompting strategies for getting better textual data.

The last two in particular are useful for creating synthetic data to finetune another GPT model. For example using higher quality data produced by `gpt-4o` to finetune the cheaper and quicker `gpt-3.5-turbo` for improved performance while reducing costs.

### Getting setup

```python
%pip install openai
%pip install pandas
%pip install scikit-learn
%pip install matplotlib
```

```python
from openai import OpenAI
import os
import re
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import json
import matplotlib

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "<your OpenAI API key if not set as env var>"))
```

### 1. CSV with a structure prompt
Here we create data in the simplest way. You can quickly generate data by addressing 3 key points: telling it the format of the data (CSV), the schema, and useful information regarding how columns relate (the LLM will be able to deduce this from the column names but a helping hand will improve performance).

```python
datagen_model = "gpt-4o-mini"
question = """
Create a CSV file with 10 rows of housing data.
Each row should include the following fields:
 - id (incrementing integer starting at 1)
 - house size (m^2)
 - house price
 - location
 - number of bedrooms

Make sure that the numbers make sense (i.e. more rooms is usually bigger size, more expensive locations increase price. more size is usually higher price etc. make sure all the numbers make sense). Also only respond with the CSV.
"""

response = client.chat.completions.create(
  model=datagen_model,
  messages=[
    {"role": "system", "content": "You are a helpful assistant designed to generate synthetic data."},
    {"role": "user", "content": question}
  ]
)
res = response.choices[0].message.content
print(res)
```

    ```csv
    id,house_size_m2,house_price,location,number_of_bedrooms
    1,50,150000,Suburban,2
    2,75,250000,City Center,3
    3,100,350000,Suburban,4
    4,120,450000,Suburban,4
    5,80,300000,City Center,3
    6,90,400000,City Center,3
    7,150,600000,Premium Area,5
    8,200,750000,Premium Area,5
    9,55,180000,Suburban,2
    10,300,950000,Premium Area,6
    ```

### 2. CSV with a Python program
The issue with generating data directly is we are limited in the amount of data we can generate because of the context. Instead what we can do is ask the LLM to generate a python program to generate the synthetic data. This allows us to scale to much more data while also providing us a view into how the data was generated by inspecting the python program.

This would then let us edit the python program as we desire while giving us a good basis to start from.

```python
question = """
Create a Python program to generate 100 rows of housing data.
I want you to at the end of it output a pandas dataframe with 100 rows of data.
Each row should include the following fields:
 - id (incrementing integer starting at 1)
 - house size (m^2)
 - house price
 - location
 - number of bedrooms

Make sure that the numbers make sense (i.e. more rooms is usually bigger size, more expensive locations increase price. more size is usually higher price etc. make sure all the numbers make sense).
"""

response = client.chat.completions.create(
  model=datagen_model,
  messages=[
    {"role": "system", "content": "You are a helpful assistant designed to generate synthetic data."},
    {"role": "user", "content": question}
  ]
)
res = response.choices[0].message.content
print(res)
```

    Certainly! Below is a Python program that generates synthetic housing data according to your specifications. We will create a pandas DataFrame with the defined fields and characteristics.
    
    ```python
    import pandas as pd
    import random
    
    def generate_housing_data(num_rows):
        data = []
        
        locations = [
            ('City Center', 10000, 150),  # (location name, base price per m², base size)
            ('Suburban Area', 8000, 100),
            ('Country Side', 5000, 80),
            ('Coastal Region', 12000, 110),
            ('Urban Neighborhood', 9000, 130)
        ]
        
        for i in range(1, num_rows + 1):
            # Randomly pick a location
            location, base_price_per_m2, base_size = random.choice(locations)
            
            # Generate number of bedrooms (1 to 5)
            number_of_bedrooms = random.randint(1, 5)
            
            # Calculate house size based on the number of bedrooms
            house_size = base_size + (10 * number_of_bedrooms) + random.randint(-5, 15)  # Adding some noise
            
            # Calculate house price based on house size and location
            house_price = base_price_per_m2 * house_size + random.randint(-5000, 10000)  # Adding some noise
    
            # Append the generated data to the list
            data.append({
                'id': i,
                'house_size_m2': house_size,
                'house_price': house_price,
                'location': location,
                'number_of_bedrooms': number_of_bedrooms
            })
    
        # Create a pandas DataFrame
        df = pd.DataFrame(data)
        return df
    
    # Generate 100 rows of housing data
    housing_data_df = generate_housing_data(100)
    
    # Show the result
    print(housing_data_df)
    ```
    
    ### Explanation:
    - The `generate_housing_data` function creates synthetic housing data for a specified number of rows (`num_rows`).
    - We define different locations with corresponding base prices per square meter and average house sizes.
    - For each house, we randomly select a location, number of bedrooms, and calculate house size and price to ensure a sensible correlation between the values.
    - Finally, we create a pandas DataFrame from the generated data and return it.
    
    You can run this program in your Python environment, and it will output a DataFrame containing 100 rows of synthetic housing data.

We need to make sure to parse the output of this appropriately as often there may be surrounding text to the python code. We can also explicitly ask it to state all assumptions it made about the data it's generating, however in this circumstance it told us that automatically.

### 3. Multitable CSV with a python program
For more complex relationships however we need to make sure to specify a few more characteristics. 

To create multiple different datasets which relate to each other (for example housing, location, house type), as before we would need to specify the format, schema and useful information. However, the useful information required to get good performance is higher now. It's case-specific but a good amount of things to describe would be how the datasets relate to each other, addressing the size of the datasets in relation to one another, making sure foreign and primary keys are made appropriately and ideally using previously generated datasets to populate new ones so the actual data values match where necessary.

```python
question = """
Create a Python program to generate 3 different pandas dataframes.

1. Housing data
I want 100 rows. Each row should include the following fields:
 - id (incrementing integer starting at 1)
 - house size (m^2)
 - house price
 - location
 - number of bedrooms
 - house type
 + any relevant foreign keys

2. Location
Each row should include the following fields:
 - id (incrementing integer starting at 1)
 - country
 - city
 - population
 - area (m^2)
 + any relevant foreign keys

 3. House types
 - id (incrementing integer starting at 1)
 - house type
 - average house type price
 - number of houses
 + any relevant foreign keys

Make sure that the numbers make sense (i.e. more rooms is usually bigger size, more expensive locations increase price. more size is usually higher price etc. make sure all the numbers make sense).
Make sure that the dataframe generally follow common sense checks, e.g. the size of the dataframes make sense in comparison with one another.
Make sure the foreign keys match up and you can use previously generated dataframes when creating each consecutive dataframes.
You can use the previously generated dataframe to generate the next dataframe.
"""

response = client.chat.completions.create(
  model=datagen_model,
  messages=[
    {"role": "system", "content": "You are a helpful assistant designed to generate synthetic data."},
    {"role": "user", "content": question}
  ]
)
res = response.choices[0].message.content
print(res)
```

    Certainly! Below is a Python program that generates the three specified pandas DataFrames for housing data, location data, and house types. Each DataFrame will include the necessary fields, and the foreign keys will ensure proper relationships among them.
    
    ```python
    import pandas as pd
    import numpy as np
    
    # Set random seed for reproducibility
    np.random.seed(0)
    
    # Function to generate location DataFrame
    def generate_location_data(num_locations):
        locations = {
            "id": range(1, num_locations + 1),
            "country": np.random.choice(['USA', 'Canada', 'UK'], num_locations),
            "city": np.random.choice(['New York', 'Toronto', 'London', 'Vancouver', 'Manchester'], num_locations),
            "population": np.random.randint(50000, 1000000, num_locations),
            "area": np.random.randint(10000, 500000, num_locations)
        }
        return pd.DataFrame(locations)
    
    # Function to generate house types DataFrame
    def generate_house_type_data(num_house_types):
        house_types = {
            "id": range(1, num_house_types + 1),
            "house_type": np.random.choice(['Detached', 'Semi-Detached', 'Terraced', 'Flat'], num_house_types),
            "average_house_type_price": np.random.randint(100000, 1000000, num_house_types),
            "number_of_houses": np.random.randint(10, 1000, num_house_types)
        }
        return pd.DataFrame(house_types)
    
    # Function to generate housing data DataFrame
    def generate_housing_data(num_houses, location_df, house_type_df):
        house_sizes = np.random.randint(50, 300, num_houses)  # size in m^2
        location_ids = np.random.choice(location_df['id'], num_houses)
        house_type_ids = np.random.choice(house_type_df['id'], num_houses)
        
        # Generate prices based on size, location, and house type
        house_prices = (house_sizes * np.random.randint(2000, 5000, num_houses) // 10) + \
                       (location_ids * 1000) + \
                       (house_type_df.loc[house_type_ids - 1, 'average_house_type_price'].values // 4)
        
        housing_data = {
            "id": range(1, num_houses + 1),
            "house_size": house_sizes,
            "house_price": house_prices,
            "location_id": location_ids,
            "bedrooms": np.random.randint(1, 6, num_houses),
            "house_type_id": house_type_ids
        }
        
        return pd.DataFrame(housing_data)
    
    # Generate DataFrames
    num_locations = 10
    num_house_types = 4
    num_houses = 100
    
    location_df = generate_location_data(num_locations)
    house_type_df = generate_house_type_data(num_house_types)
    housing_df = generate_housing_data(num_houses, location_df, house_type_df)
    
    # Display the generated DataFrames
    print("Location DataFrame:")
    print(location_df.head(), "\n")
    
    print("House Types DataFrame:")
    print(house_type_df.head(), "\n")
    
    print("Housing DataFrame:")
    print(housing_df.head(), "\n")
    
    # Printing the DataFrame shapes
    print(f"Shapes: \nLocation: {location_df.shape}, House Types: {house_type_df.shape}, Housing: {housing_df.shape}")
    ```
    
    ### Explanation of the Code:
    1. **Location DataFrame:** 
       - Generates random locations with attributes such as country, city, population, and area.
      
    2. **House Types DataFrame:** 
       - Generates different types of houses along with average prices and quantity available.
      
    3. **Housing DataFrame:** 
       - Generates housing data with increments on price based on house size, location, and house type, while also ensuring foreign keys (IDs) for location and house type.
    
    ### Output:
    The three DataFrames generated will logically relate to one another with consistent data types and primary–foreign key relationships, resulting in a coherent representation of the housing dataset. The output displays heads of each DataFrame and their shapes for verification.

### 4. Simply creating textual data
Here we take a first look at creating textual data. This can be used to finetune another GPT model for example. In this case we imagine ourselves a retailer trying to streamline the process of creating descriptions for items they are selling. We again need to specify the format of the data, in particular in this case we want one which is easy to parse as an output.

The example we consider below is one in which we want to create input output training pairs for GPT model to finetune on. We will have the products' name and the category it belongs to as input and the output will be a description. 

Specifying the structure of the output explicitly and giving commands to not deviate from this help enforce the output structure. You can run this in a loop and append the data to generate more synthetic data. Again, as before we will need to parse the data well so that our code further downstream does not break.

```python
output_string = ""
for i in range(3):
  question = f"""
  I am creating input output training pairs to fine tune my gpt model. The usecase is a retailer generating a description for a product from a product catalogue. I want the input to be product name and category (to which the product belongs to) and output to be description.
  The format should be of the form:
  1.
  Input: product_name, category
  Output: description
  2.
  Input: product_name, category
  Output: description

  Do not add any extra characters around that formatting as it will make the output parsing break.
  Create as many training pairs as possible.
  """

  response = client.chat.completions.create(
    model=datagen_model,
    messages=[
      {"role": "system", "content": "You are a helpful assistant designed to generate synthetic data."},
      {"role": "user", "content": question}
    ]
  )
  res = response.choices[0].message.content
  output_string += res + "\n" + "\n"
print(output_string[:1000]) #displaying truncated response
```

    1.
    Input: Wireless Bluetooth Headphones, Electronics
    Output: Immerse yourself in high-quality sound with these Wireless Bluetooth Headphones, featuring active noise cancellation and a comfortable over-ear design for extended listening sessions.
    
    2.
    Input: Organic Green Tea, Beverages
    Output: Enjoy a refreshing cup of Organic Green Tea, sourced from the finest leaves, packed with antioxidants, and perfect for a healthy, invigorating boost anytime.
    
    3.
    Input: Stainless Steel Kitchen Knife, Kitchenware
    Output: Cut with precision and ease using this Stainless Steel Kitchen Knife, designed with an ergonomic handle and a sharp blade for all your culinary tasks.
    
    4.
    Input: Hiking Backpack, Outdoor Gear
    Output: Explore the great outdoors with this durable Hiking Backpack, featuring multiple compartments for optimal organization and a breathable design for ultimate comfort on long treks.
    
    5.
    Input: Air Fryer, Kitchen Appliances
    Output: Cook your favorite meals with less oil using this Air Fryer

Note: the above output is truncated. And now we can parse it as below to get a list of products, categories and their descriptions. For example, let's take a look at the products it's generated.

```python
#regex to parse data
pattern = re.compile(r'Input:\s*(.+?),\s*(.+?)\nOutput:\s*(.+?)(?=\n\n|\Z)', re.DOTALL)
matches = pattern.findall(output_string)
products = []
categories = []
descriptions = []

for match in matches:
    product, category, description = match
    products.append(product.strip())
    categories.append(category.strip())
    descriptions.append(description.strip())
products
```

    ['Wireless Bluetooth Headphones',
     'Organic Green Tea',
     'Stainless Steel Kitchen Knife',
     'Hiking Backpack',
     'Air Fry