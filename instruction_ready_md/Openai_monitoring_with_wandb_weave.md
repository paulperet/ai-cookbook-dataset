# OpenAI API Monitoring with Weights & Biases Weave

Use the W&B Weave OpenAI integration to automatically monitor your OpenAI API calls, track usage metrics, and create interactive dashboards for your team. This guide walks you through setting up monitoring and exploring your LLM usage with templated Weave Boards.

## Prerequisites

You'll need:
- An [OpenAI API key](https://platform.openai.com/account/api-keys)
- A [Weights & Biases account](https://wandb.ai/site)

## Step 1: Install Dependencies and Authenticate

First, install the required packages and authenticate with both W&B and OpenAI.

```bash
pip install -qqq weave openai tiktoken wandb
```

```python
import wandb
import weave
import os

# Login to Weights & Biases
wandb.login()

# Set W&B base URL
WANDB_BASE_URL = "https://api.wandb.ai"
os.environ["WANDB_BASE_URL"] = WANDB_BASE_URL

# Authenticate with OpenAI
from getpass import getpass

if os.getenv("OPENAI_API_KEY") is None:
    os.environ["OPENAI_API_KEY"] = getpass(
        "Paste your OpenAI key from: https://platform.openai.com/account/api-keys\n"
    )
assert os.getenv("OPENAI_API_KEY", "").startswith(
    "sk-"
), "This doesn't look like a valid OpenAI API key"
print("OpenAI API key configured")
```

## Step 2: Configure Your W&B Project

Set up your W&B entity (username or team name) and project details. You can find valid entity options in the left sidebar of your [W&B Home Page](https://wandb.ai/home).

```python
# Set to your wandb username or team name
WB_ENTITY = ""

# Top-level directory for this work
WB_PROJECT = "weave"

# Record table which stores the logs of OpenAI API calls
STREAM_NAME = "openai_logs"
```

## Step 3: Initialize Monitoring

Start monitoring OpenAI API usage by calling `init_monitor()`. This creates a stream that records and stores all your OpenAI API calls.

```python
from weave.monitoring import openai, init_monitor

# Initialize the monitor with your stream path
m = init_monitor(f"{WB_ENTITY}/{WB_PROJECT}/{STREAM_NAME}")

# Specify a model for simplicity
OPENAI_MODEL = "gpt-3.5-turbo"

# Generate some sample logs to populate the stream
r = openai.ChatCompletion.create(
    model=OPENAI_MODEL, messages=[{"role": "user", "content": "hello world!"}]
)
r = openai.ChatCompletion.create(
    model=OPENAI_MODEL, messages=[{"role": "user", "content": "what is 2+2?"}]
)
```

After running this cell, you'll see a link to view your project in the Weave UI.

## Step 4: Explore Your Monitoring Dashboard

1. Click the link provided to preview your data stream
2. In the right sidebar, click "OpenAI Monitor Board" to create a Weave Board for this data stream
3. The board provides automatic tracking of LLM usage metrics including cost, latency, and throughput

To save and share your work:
- Rename the board by clicking the autogenerated name at the top
- Click "Publish" in the top right to share with others
- Find previously saved boards at weave.wandb.ai

## Practical Examples

Now let's explore different ways to monitor OpenAI API calls in practice.

### Example 1: Basic Prompt and Completion Logging

Monitor a simple ChatCompletion request and extract the response text.

```python
response = openai.ChatCompletion.create(
    model=OPENAI_MODEL,
    messages=[
        {
            "role": "user",
            "content": f"What is the meaning of life, the universe, and everything?",
        },
    ],
)
print(response["choices"][0]["message"]["content"])
```

### Example 2: Track Parameters as Attributes

Factor out important parameters and track them as attributes on the logged record for better organization and filtering.

```python
system_prompt = "you always write in bullet points"
prompt_template = "solve the following equation step by step: {equation}"
params = {"equation": "4 * (3 - 1)"}

openai.ChatCompletion.create(
    model=OPENAI_MODEL,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt_template.format(**params)},
    ],
    # Add additional attributes to the logged record
    monitor_attributes={
        "system_prompt": system_prompt,
        "prompt_template": prompt_template,
        "params": params,
    },
)
```

### Example 3: Monitor Streaming Responses

Log streaming responses as a single record. Note: token counts aren't tracked in streaming mode.

```python
from weave.monitoring.openai import message_from_stream

r = openai.ChatCompletion.create(
    model=OPENAI_MODEL,
    messages=[
        {
            "role": "system",
            "content": "You are a robot and only speak in robot, like beep bloop bop.",
        },
        {"role": "user", "content": "Tell me a 50-word story."},
    ],
    stream=True,
)

for s in message_from_stream(r):
    print(s, end="")
```

### Example 4: Structure Prompt Engineering Experiments

Compare different prompt variations and track them systematically for analysis in your Weave Board.

```python
def explain_math(system_prompt, prompt_template, params):
    openai.ChatCompletion.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt_template.format(**params)},
        ],
        monitor_attributes={
            "system_prompt": system_prompt,
            "prompt_template": prompt_template,
            "params": params,
        },
    )


# Define your experiment variables
system_prompts = [
    "you're extremely flowery and poetic",
    "you're very direct and precise",
    "balance brevity with insight",
]
prompt_template = "explain the solution of the following to a {audience}: {equation}"
equations = ["x^2 + 4x + 9 = 0", "15 * (2 - 6) / 4"]
audience = ["new student", "math genius"]

# Run the experiments
for system_prompt in system_prompts:
    for equation in equations:
        for person in audience:
            params = {"equation": equation, "audience": person}
            explain_math(system_prompt, prompt_template, params)
```

## Next Steps

With your monitoring set up, you can now:

1. **Explore your data** in the Weave Board by slicing, aggregating, and filtering
2. **Customize panels** to focus on patterns relevant to your use case
3. **Share insights** with your team through the interactive dashboard
4. **Iterate visually** as you refine your prompts and parameters

The Weave Board template provides a starting point, but you can customize it extensively based on your specific monitoring needs. Try grouping by different parameters in the board to uncover insights about your LLM usage patterns.