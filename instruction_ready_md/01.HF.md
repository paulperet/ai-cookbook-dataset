# Using the Phi Family of Models on Hugging Face

## Introduction

[Hugging Face](https://huggingface.co/) is a leading AI community and platform, hosting a vast collection of open-source models and datasets. Major organizations, including Microsoft, Meta, and Google, publish their models here. Microsoft's Phi family of Small Language Models (SLMs) is fully available on Hugging Face, offering various model sizes and formats (like GGUF and ONNX) to suit different deployment needs.

This guide will show you how to download and run a basic inference with a Phi model.

## Prerequisites

Ensure you have Python installed on your system. You will install the necessary libraries in the following steps.

## Step 1: Locate the Phi Models

All Microsoft Phi models are hosted under the `microsoft` organization on Hugging Face. You can browse the collections via these links:

*   **Phi-1 / Phi-1.5:** [Model Collection](https://huggingface.co/collections/microsoft/phi-1-6626e29134744e94e222d572)
*   **Phi-3 / Phi-3.5:** [Model Collection](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)
*   **Phi-4:** [Model Collection](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
*   **Specialized Phi-4 Models:**
    *   `microsoft/Phi-4-reasoning`
    *   `microsoft/Phi-4-reasoning-plus`
    *   `microsoft/Phi-4-mini-reasoning`

## Step 2: Download a Model

You have two primary methods for downloading models: using the Hugging Face CLI or Git.

### Option A: Download Using the Hugging Face CLI

This method is straightforward and handles large files well.

1.  Install the Hugging Face Hub CLI tools.
    ```bash
    pip install -U "huggingface_hub[cli]"
    ```

2.  Log in to Hugging Face using an access token from your [Settings page](https://huggingface.co/settings/tokens).
    ```bash
    huggingface-cli login --token $HF_TOKEN --add-to-git-credential
    ```

3.  Download the model. The following command downloads `phi-4` to the Hugging Face cache directory.
    ```bash
    huggingface-cli download microsoft/phi-4
    ```
    To download it to a specific directory, use the `--local-dir` flag:
    ```bash
    huggingface-cli download microsoft/phi-4 --local-dir /your/custom/path
    ```

### Option B: Download Using Git

This method is useful if you are familiar with Git and Git LFS (Large File Storage).

1.  Ensure Git LFS is installed and initialized on your system.
    ```bash
    git lfs install
    ```

2.  Clone the model repository directly.
    ```bash
    git clone https://huggingface.co/microsoft/phi-4
    ```

## Step 3: Run a Basic Inference with Phi-4

Now, let's run the model to generate a response. We'll use the `transformers` library.

1.  Install the `transformers` library.
    ```bash
    pip install transformers -U
    ```

2.  Create a new Python script (e.g., `inference.py`) and add the following code. This script initializes a text-generation pipeline with the `phi-4` model and asks a financial question.

    ```python
    import transformers

    # Initialize the text-generation pipeline
    # The model will be automatically downloaded on first run if not present locally.
    pipeline = transformers.pipeline(
        "text-generation",
        model="microsoft/phi-4",
        model_kwargs={"torch_dtype": "auto"},
        device_map="auto",
    )

    # Define the user's prompt in a chat format
    messages = [
        {
            "role": "user",
            "content": "I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"
        },
    ]

    # Generate a response
    outputs = pipeline(messages, max_new_tokens=2048)

    # Print the model's final response
    print(outputs[0]["generated_text"][-1])
    ```

3.  Run the script from your terminal:
    ```bash
    python inference.py
    ```

The model will process the prompt and generate a detailed, step-by-step explanation of the compound interest calculation, estimating how many years it would take for $20,000 to grow to $1,000,000 at a 4% annual interest rate compounded semi-annually.

## Next Steps

You have successfully downloaded a Phi model and performed inference. To build more advanced applications, you can explore:
*   Using different Phi model variants for specific tasks (e.g., `Phi-4-reasoning`).
*   Quantizing the model for efficient deployment using formats like GGUF.
*   Integrating the model into a Retrieval-Augmented Generation (RAG) pipeline or an AI agent framework.