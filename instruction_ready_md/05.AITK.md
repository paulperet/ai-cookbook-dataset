# Guide: Getting Started with the Phi Family in AI Toolkit for VS Code

This guide walks you through installing and using the AI Toolkit for VS Code to work with models like Phi-4. You'll learn how to download models, run them locally, and integrate them into your applications via a local REST API.

## Prerequisites

Before you begin, ensure you have the following:

1.  **Visual Studio Code:** Installed on your system. [Download VS Code](https://code.visualstudio.com/docs/setup/windows).
2.  **System Requirements:**
    *   **Platform:** Windows, Linux, or macOS.
    *   **For Fine-Tuning:** An NVIDIA GPU is required.
    *   **For Windows Fine-Tuning:** You must also install the Windows Subsystem for Linux (WSL) with Ubuntu 18.04 or greater. [Install WSL](https://learn.microsoft.com/windows/wsl/install).

## Step 1: Install the AI Toolkit Extension

The AI Toolkit is a VS Code extension available in the marketplace.

1.  Open Visual Studio Code.
2.  In the Activity Bar (the vertical icon menu), select the **Extensions** icon (or press `Ctrl+Shift+X`).
3.  In the Extensions search bar, type **"AI Toolkit"**.
4.  Select the extension named **"AI Toolkit for Visual Studio Code"** from Microsoft.
5.  Click the **Install** button.

Once installed, the AI Toolkit icon will appear in your Activity Bar. The first time you use it, you will be prompted to sign in with your GitHub account to access the model catalog.

## Step 2: Explore the Toolkit Interface

Click the AI Toolkit icon in the Activity Bar to open its primary sidebar. It is organized into several key sections:

*   **Models:** Browse and manage your AI models.
*   **Resources:** Access additional tools and documentation.
*   **Playground:** Interactively test models.
*   **Fine-tuning:** Configure and run model fine-tuning jobs.
*   **Evaluation:** Assess model performance.

To start, select **Model Catalog** from the **Resources** section or the main view.

## Step 3: Download a Model

The Model Catalog lets you browse models from sources like GitHub and Azure AI Foundry.

1.  In the **Model Catalog**, find a model you wish to use (e.g., a Phi model).
2.  **Important:** Check the model card for its size, target platform (Windows/Linux), and accelerator type (CPU/GPU).
    *   For optimal performance on Windows with a GPU, select a model version specifically optimized for Windows (using the DirectML accelerator).
    *   Model names typically follow the format: `{model_name}-{accelerator}-{quantization}-{format}`.
3.  Select your desired model and click **Download** to save it locally.

> **Tip:** To check for a GPU on Windows, open **Task Manager** (`Ctrl+Shift+Esc`), go to the **Performance** tab, and look for entries like "GPU 0".

## Step 4: Test the Model in the Playground

After the model finishes downloading, you can test it immediately.

1.  On the model's card in the catalog, click **Load in Playground**.
2.  The toolkit will install any necessary prerequisites and dependencies, then open a new VS Code workspace with the model loaded.
3.  Use the Playground interface to send prompts and see the model's responses in real-time.

## Step 5: Integrate the Model into Your Application

The AI Toolkit runs a local REST API server on **port 5272** that is compatible with the OpenAI API format. This allows you to use the model in your own applications.

### Method 1: Using the REST API Directly

You can send HTTP requests to `http://127.0.0.1:5272/v1/chat/completions`.

1.  Create a request body file (e.g., `body.json`):
    ```json
    {
        "model": "Phi-4",
        "messages": [
            {
                "role": "user",
                "content": "what is the golden ratio?"
            }
        ],
        "temperature": 0.7,
        "max_tokens": 100,
        "stream": true
    }
    ```
2.  Send the request using `curl`:
    ```bash
    curl -X POST http://127.0.0.1:5272/v1/chat/completions \
      -H 'Content-Type: application/json' \
      -d @body.json
    ```

### Method 2: Using the OpenAI Python Client Library

You can use the official `openai` Python package, pointing it to your local server.

1.  Install the library and make a request:
    ```python
    # pip install openai
    from openai import OpenAI

    # Point the client to the local AI Toolkit server
    client = OpenAI(
        base_url="http://127.0.0.1:5272/v1/",
        api_key="x"  # API key is required by the library but not used locally
    )

    response = client.chat.completions.create(
        model="Phi-4",
        messages=[
            {"role": "user", "content": "what is the golden ratio?"}
        ]
    )

    print(response.choices[0].message.content)
    ```

### Method 3: Using the Azure OpenAI .NET Client Library

For .NET applications, you can use the `Azure.AI.OpenAI` client by overriding the request endpoint.

1.  Add the NuGet package to your project:
    ```bash
    dotnet add package Azure.AI.OpenAI --version 1.0.0-beta.17
    ```
2.  Create a policy file to redirect requests to your local server (`OverridePolicy.cs`):
    ```csharp
    using Azure.Core.Pipeline;
    using Azure.Core;

    internal partial class OverrideRequestUriPolicy(Uri overrideUri) : HttpPipelineSynchronousPolicy
    {
        private readonly Uri _overrideUri = overrideUri;
        public override void OnSendingRequest(HttpMessage message)
        {
            message.Request.Uri.Reset(_overrideUri);
        }
    }
    ```
3.  Use the client in your main application code (`Program.cs`):
    ```csharp
    using Azure.AI.OpenAI;

    // Define the local AI Toolkit endpoint
    Uri localhostUri = new("http://localhost:5272/v1/chat/completions");

    // Configure the client to use the local endpoint
    OpenAIClientOptions clientOptions = new();
    clientOptions.AddPolicy(
        new OverrideRequestUriPolicy(localhostUri),
        Azure.Core.HttpPipelinePosition.BeforeTransport);

    // Initialize the client (API key is not used locally)
    OpenAIClient client = new(openAIApiKey: "unused", clientOptions);

    // Create a chat request
    ChatCompletionsOptions options = new()
    {
        DeploymentName = "Phi-4", // Specify your loaded model
        Messages =
        {
            new ChatRequestSystemMessage("You are a helpful assistant."),
            new ChatRequestUserMessage("What is the golden ratio?"),
        }
    };

    // Get a streaming response
    var streamingResponse = await client.GetChatCompletionsStreamingAsync(options);
    await foreach (var chatChunk in streamingResponse)
    {
        Console.Write(chatChunk.ContentUpdate);
    }
    ```

## Next Steps: Fine-Tuning and Evaluation

*   **Fine-Tuning:** Use the **Fine-tuning** section in the AI Toolkit sidebar to adapt pre-trained models to your specific data, using either local compute or remote Azure resources.
*   **Evaluation:** Use the **Evaluation** section to systematically assess and compare model performance.
*   **Support:** For common issues and troubleshooting, visit the [AI Toolkit Q&A page on GitHub](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/QA.md).

You are now ready to explore, test, and integrate powerful AI models like the Phi family directly within your VS Code development environment.