# Classify Text with Embeddings Using the Gemini API

This guide demonstrates how to use embeddings generated by the Gemini API to train a model that classifies newsgroup posts by topic. You will work with a subset of the 20 Newsgroups dataset, generate embeddings for the text, and build a simple neural network classifier.

**Note:** This tutorial requires paid tier rate limits to run properly. Please review the [pricing details](https://ai.google.dev/pricing) for more information.

## Prerequisites & Setup

First, install the required library and set up your API key.

```bash
pip install -U -q google-genai
```

### Obtain and Configure Your API Key

1.  Get an API key from [Google AI Studio](https://aistudio.google.com/app/apikey).
2.  In Google Colab, add the key to the secrets manager (under the "ðŸ”‘" icon in the left panel). Name it `GEMINI_API_KEY`.
3.  Use the following code to load the key and initialize the client.

```python
from google import genai
from google.genai import types
from google.colab import userdata

# Securely fetch your API key
GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')
client = genai.Client(api_key=GEMINI_API_KEY)
```

### Select an Embedding Model

While any embedding model will work, it's crucial to choose one and stick with it for production, as outputs from different models are not directly compatible.

```python
# List available embedding models
for m in client.models.list():
  if 'embedContent' in m.supported_actions:
    print(m.name)
```

For this tutorial, we'll use `gemini-embedding-001`.

```python
MODEL_ID = "gemini-embedding-001"
```

## Step 1: Prepare the Dataset

You'll use the 20 Newsgroups dataset, which contains posts across 20 topics. You'll preprocess the text and organize it into Pandas DataFrames.

```python
from sklearn.datasets import fetch_20newsgroups
import re
import pandas as pd

# Load the dataset
newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')

# View the target class names
print(newsgroups_train.target_names)
```

### Preprocess the Text Data

The raw text contains headers, emails, and other noise. The following function cleans the text and organizes it into a structured DataFrame.

```python
def preprocess_newsgroup_data(newsgroup_dataset):
  # Remove emails, names, and specific headers
  newsgroup_dataset.data = [re.sub(r'[\w\.-]+@[\w\.-]+', '', d) for d in newsgroup_dataset.data]
  newsgroup_dataset.data = [re.sub(r"\([^()]*\)", "", d) for d in newsgroup_dataset.data]
  newsgroup_dataset.data = [d.replace("From: ", "") for d in newsgroup_dataset.data]
  newsgroup_dataset.data = [d.replace("\nSubject: ", "") for d in newsgroup_dataset.data]

  # Truncate long entries to 5,000 characters
  newsgroup_dataset.data = [d[0:5000] if len(d) > 5000 else d for d in newsgroup_dataset.data]

  # Create DataFrame
  df_processed = pd.DataFrame(newsgroup_dataset.data, columns=['Text'])
  df_processed['Label'] = newsgroup_dataset.target
  df_processed['Class Name'] = ''
  
  # Map numeric labels to class names
  for idx, row in df_processed.iterrows():
    df_processed.at[idx, 'Class Name'] = newsgroup_dataset.target_names[row['Label']]

  return df_processed

# Apply preprocessing
df_train = preprocess_newsgroup_data(newsgroups_train)
df_test = preprocess_newsgroup_data(newsgroups_test)

print(df_train.head())
```

### Sample the Data

To make the tutorial run faster, you'll sample 100 posts per class from the training set and 25 from the test set, focusing only on the science-related categories (`sci.*`).

```python
def sample_data(df, num_samples, classes_to_keep):
  # Sample evenly from each class
  df = df.groupby('Label', as_index=False).apply(lambda x: x.sample(num_samples)).reset_index(drop=True)
  # Filter for specific classes
  df = df[df['Class Name'].str.contains(classes_to_keep)]
  # Re-encode the labels after filtering
  df['Class Name'] = df['Class Name'].astype('category')
  df['Encoded Label'] = df['Class Name'].cat.codes
  return df

TRAIN_NUM_SAMPLES = 100
TEST_NUM_SAMPLES = 25
CLASSES_TO_KEEP = 'sci'

df_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)
df_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)

print(df_train.value_counts('Class Name'))
print(df_test.value_counts('Class Name'))
```

## Step 2: Generate Embeddings

Embeddings transform text into numerical vectors that capture semantic meaning. The Gemini API supports different `task_type` parameters. For this classification task, you will use `task_type="CLASSIFICATION"`.

### Create the Embedding Function

You'll define a function that takes a batch of texts and returns their embeddings.

```python
import numpy as np
from tqdm.auto import tqdm
from google.genai import types

tqdm.pandas()

def make_embed_text_fn(model):
    def embed_fn(texts: list[str]) -> list[list[float]]:
        # Set the task_type to CLASSIFICATION
        result = client.models.embed_content(
            model=model,
            contents=[texts],
            config=types.EmbedContentConfig(task_type="CLASSIFICATION")
        ).embeddings
        return np.array([embedding.values for embedding in result])
    return embed_fn

def create_embeddings(df):
    embed_fn = make_embed_text_fn(MODEL_ID)
    batch_size = 25  # Maximum batch size for the API
    all_embeddings = []

    # Process texts in batches
    for i in tqdm(range(0, len(df), batch_size)):
        batch = df["Text"].iloc[i : i + batch_size].tolist()
        embeddings = embed_fn(batch)
        all_embeddings.extend(embeddings)

    df["Embeddings"] = all_embeddings
    return df
```

### Apply Embeddings to the DataFrames

Now, generate embeddings for both the training and test datasets.

```python
df_train = create_embeddings(df_train)
df_test = create_embeddings(df_test)

print(df_train.head())
print(df_test.head())
```

## Step 3: Build a Classification Model

With the embeddings as input features, you can now build a simple neural network classifier using Keras.

```python
import keras
from keras import layers

def build_classification_model(input_size: int, num_classes: int) -> keras.Model:
  # Define the model architecture
  inputs = x = keras.Input(shape=(input_size,))
  x = layers.Dense(256, activation='relu')(x)
  x = layers.Dropout(0.5)(x)
  outputs = layers.Dense(num_classes, activation='softmax')(x)
  
  model = keras.Model(inputs=inputs, outputs=outputs)
  model.compile(
      optimizer='adam',
      loss='sparse_categorical_crossentropy',
      metrics=['accuracy']
  )
  return model

# Prepare the data for training
X_train = np.stack(df_train['Embeddings'].values)
y_train = df_train['Encoded Label'].values

X_test = np.stack(df_test['Embeddings'].values)
y_test = df_test['Encoded Label'].values

# Build and train the model
input_size = X_train.shape[1]
num_classes = len(df_train['Class Name'].unique())

model = build_classification_model(input_size, num_classes)
history = model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)

# Evaluate on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_accuracy:.4f}")
```

## Summary

You have successfully:
1.  Loaded and preprocessed the 20 Newsgroups dataset.
2.  Generated semantic embeddings using the Gemini API with the `CLASSIFICATION` task type.
3.  Built and trained a neural network classifier on those embeddings.

This workflow demonstrates how to leverage powerful pre-trained embedding models as a feature extraction step to build effective text classifiers with relatively simple downstream models.