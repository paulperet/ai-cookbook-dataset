# Enhancing Whisper Transcriptions: A Guide to Pre- & Post-Processing

This guide walks you through practical techniques to improve the quality of transcriptions generated by OpenAI's Whisper model. You'll learn how to pre-process audio by trimming silence and segmenting long files, and then post-process the raw transcript to add punctuation, correct financial terminology, and handle character encoding. While this tutorial uses a sample earnings call, the methods can be adapted to any audio file.

## Prerequisites & Setup

First, ensure you have the necessary Python libraries installed. You'll need `openai`, `pydub`, and standard utilities.

```bash
pip install openai pydub
```

Now, import the required modules and set up your OpenAI client.

```python
from openai import OpenAI
import os
import urllib
from pathlib import Path
from pydub import AudioSegment
import ssl

# Initialize the OpenAI client
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "<your OpenAI API key if not set as env var>"))
```

### Download the Example Audio File

We'll use a sample earnings call audio file for demonstration. The following code downloads and saves it locally.

```python
# Define remote and local file paths
earnings_call_remote_filepath = "https://cdn.openai.com/API/examples/data/EarningsCall.wav"
earnings_call_filepath = "data/EarningsCall.wav"

# Download the file
ssl._create_default_https_context = ssl._create_unverified_context
urllib.request.urlretrieve(earnings_call_remote_filepath, earnings_call_filepath)
```

## Step 1: Pre-processing Audio with Silence Trimming

Long silences at the start of an audio file can sometimes degrade Whisper's transcription accuracy. Let's create a function to detect and trim this leading silence.

### 1.1 Define a Silence Detection Function

This function scans the audio from the beginning and returns the timestamp (in milliseconds) where meaningful sound begins.

```python
def milliseconds_until_sound(sound, silence_threshold_in_decibels=-20.0, chunk_size=10):
    """
    Detects leading silence in an audio segment.
    Returns the milliseconds until the first sound above the threshold.
    """
    trim_ms = 0

    assert chunk_size > 0  # Prevent infinite loop
    while sound[trim_ms:trim_ms+chunk_size].dBFS < silence_threshold_in_decibels and trim_ms < len(sound):
        trim_ms += chunk_size

    return trim_ms
```

### 1.2 Create the Trimming Function

Next, define a function that loads an audio file, trims the leading silence, and exports the cleaned version.

```python
def trim_start(filepath):
    """
    Trims leading silence from an audio file and saves the trimmed version.
    Returns the trimmed AudioSegment and the new file path.
    """
    path = Path(filepath)
    directory = path.parent
    filename = path.name

    audio = AudioSegment.from_file(filepath, format="wav")
    start_trim = milliseconds_until_sound(audio)
    trimmed = audio[start_trim:]

    new_filename = directory / f"trimmed_{filename}"
    trimmed.export(new_filename, format="wav")
    return trimmed, new_filename
```

### 1.3 Apply Trimming to the Sample File

Now, run the trimming function on your downloaded audio file.

```python
trimmed_audio, trimmed_filename = trim_start(earnings_call_filepath)
print(f"Trimmed audio saved to: {trimmed_filename}")
```

## Step 2: Segmenting Long Audio Files

For very long audio, processing it in smaller segments can improve reliability and manageability. Let's split the trimmed audio into one-minute segments.

```python
# Load the trimmed audio file
trimmed_audio = AudioSegment.from_wav(trimmed_filename)

# Define segment length (1 minute in milliseconds)
one_minute = 1 * 60 * 1000
start_time = 0
segment_index = 0

# Create an output directory for the segments
output_dir_trimmed = "trimmed_earnings_directory"
if not os.path.isdir(output_dir_trimmed):
    os.makedirs(output_dir_trimmed)

# Segment the audio
while start_time < len(trimmed_audio):
    segment = trimmed_audio[start_time:start_time + one_minute]
    segment.export(os.path.join(output_dir_trimmed, f"trimmed_{segment_index:02d}.wav"), format="wav")
    start_time += one_minute
    segment_index += 1

print(f"Created {segment_index} segment(s) in '{output_dir_trimmed}'.")
```

## Step 3: Transcribing the Audio Segments

With the audio prepared, you can now transcribe each segment using the Whisper API.

### 3.1 Define a Transcription Helper

Create a function that sends an audio file to the Whisper model and returns the transcribed text.

```python
def transcribe_audio(file, output_dir):
    """Transcribes a single audio file using Whisper-1."""
    audio_path = os.path.join(output_dir, file)
    with open(audio_path, 'rb') as audio_data:
        transcription = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_data
        )
        return transcription.text
```

### 3.2 Transcribe All Segments

Get a sorted list of the segmented audio files and transcribe them in order.

```python
# List and sort the segment files numerically
audio_files = sorted(
    (f for f in os.listdir(output_dir_trimmed) if f.endswith(".wav")),
    key=lambda f: int(''.join(filter(str.isdigit, f)))
)

# Transcribe each segment
transcriptions = [transcribe_audio(file, output_dir_trimmed) for file in audio_files]

# Combine the segment transcriptions into one full text
full_transcript = ' '.join(transcriptions)
print("Raw transcript from Whisper:")
print(full_transcript)
```

## Step 4: Post-processing the Transcript

The raw transcript may lack proper formatting and contain minor errors. Let's apply a series of cleaning and enhancement steps.

### 4.1 Remove Non-ASCII Characters (Optional)

If your transcription is in English and you encounter stray Unicode characters, you can strip them. **Skip this step if transcribing non-Latin scripts (e.g., Greek, Chinese).**

```python
def remove_non_ascii(text):
    """Removes any non-ASCII characters from the text."""
    return ''.join(i for i in text if ord(i) < 128)

ascii_transcript = remove_non_ascii(full_transcript)
print("Transcript after removing non-ASCII characters:")
print(ascii_transcript)
```

### 4.2 Add Punctuation and Formatting

Whisper includes some punctuation, but you can use a language model to refine it further. This function calls the Chat Completions API to add consistent punctuation, capitalization, and symbols.

```python
def punctuation_assistant(ascii_transcript):
    """Uses GPT-3.5 Turbo to add punctuation and formatting to a transcript."""
    system_prompt = """You are a helpful assistant that adds punctuation to text.
    Preserve the original words and only insert necessary punctuation such as periods,
    commas, capitalization, symbols like dollar signs or percentage signs, and formatting.
    Use only the context provided. If there is no context provided say, 'No context provided'\n"""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": ascii_transcript}
        ]
    )
    return response.choices[0].message.content

punctuated_transcript = punctuation_assistant(ascii_transcript)
print("Transcript after punctuation enhancement:")
print(punctuated_transcript)
```

### 4.3 Correct Financial Product Terminology

For domain-specific content like earnings calls, you can refine product names and acronyms. This function uses GPT-4 to map spoken numbers to formal product names (e.g., "five two nine" becomes "529 (Education Savings Plan)").

```python
def product_assistant(punctuated_transcript):
    """Uses GPT-4 to correct and expand financial product terms and acronyms."""
    system_prompt = """You are an intelligent assistant specializing in financial products;
    your task is to process transcripts of earnings calls, ensuring that all references to
    financial products and common financial terms are in the correct format. For each
    financial product or common term that is typically abbreviated as an acronym, the full term
    should be spelled out followed by the acronym in parentheses. For example, '401k' should be
    transformed to '401(k) retirement savings plan', 'HSA' should be transformed to 'Health Savings Account (HSA)'
    , 'ROA' should be transformed to 'Return on Assets (ROA)', 'VaR' should be transformed to 'Value at Risk (VaR)'
    , and 'PB' should be transformed to 'Price to Book (PB) ratio'. Similarly, transform spoken numbers representing
    financial products into their numeric representations, followed by the full name of the product in parentheses.
    For instance, 'five two nine' to '529 (Education Savings Plan)' and 'four zero one k' to '401(k) (Retirement Savings Plan)'.
    However, be aware that some acronyms can have different meanings based on the context (e.g., 'LTV' can stand for
    'Loan to Value' or 'Lifetime Value'). You will need to discern from the context which term is being referred to
    and apply the appropriate transformation. In cases where numerical figures or metrics are spelled out but do not
    represent specific financial products (like 'twenty three percent'), these should be left as is. Your role is to
    analyze and adjust financial product terminology in the text. Once you've done that, produce the adjusted
    transcript and a list of the words you've changed"""

    response = client.chat.completions.create(
        model="gpt-4",
        temperature=0,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": punctuated_transcript}
        ]
    )
    return response.choices[0].message.content

final_transcript = product_assistant(punctuated_transcript)
print("Final, polished transcript:")
print(final_transcript)
```

## Summary

You've successfully built a pipeline to enhance Whisper transcriptions:
1.  **Pre-processed** the audio by trimming leading silence and segmenting it.
2.  **Transcribed** the audio using the Whisper API.
3.  **Post-processed** the text by removing unwanted characters, adding punctuation, and correcting domain-specific terminology.

This workflow can be adapted to your specific use case by adjusting parameters like silence thresholds, segment lengths, and the post-processing prompts. Experiment with different settings to achieve the best results for your audio data.