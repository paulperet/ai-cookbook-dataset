# Compiled Autograd: Capturing a Larger Backward Graph for `torch.compile`

**Author:** [Simon Fan](https://github.com/xmfan)

## What You Will Learn
*   How compiled autograd interacts with `torch.compile`.
*   How to use the compiled autograd API.
*   How to inspect logs using `TORCH_LOGS`.

## Prerequisites
*   PyTorch 2.4 or later.
*   Familiarity with `torch.compile`. Complete the [Introduction to torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) tutorial.
*   Understanding of TorchDynamo and AOTAutograd from the [Get Started with PyTorch 2.x](https://pytorch.org/get-started/pytorch-2.0/) guide.

## Overview

Compiled Autograd is a `torch.compile` extension introduced in PyTorch 2.4 designed to capture a larger, more complete backward graph.

Standard `torch.compile` captures the backward graph only **partially** through its AOTAutograd component. This approach has two key limitations:
1.  **Graph breaks in the forward pass** force corresponding graph breaks in the backward pass.
2.  **[Backward hooks](https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution)** are not captured within the graph.

Compiled Autograd addresses these by integrating directly with PyTorch's autograd engine at runtime, enabling the capture of the *full* backward graph. This can lead to performance improvements for models affected by the limitations above.

However, Compiled Autograd introduces its own trade-offs:
*   **Runtime Overhead:** It adds a cache lookup step at the beginning of each backward pass.
*   **Recompilation Risk:** Capturing a larger graph makes it more susceptible to recompilations and graph breaks within Dynamo.

> **Note:** Compiled Autograd is under active development and may not be compatible with all PyTorch features. For the latest compatibility status, refer to the [Compiled Autograd Landing Page](https://docs.google.com/document/d/11VucFBEewzqgkABIjebZIzMvrXr3BtcY1aGKpX61pJY).

## Setup

We'll use a simple neural network model for our examples. This model takes a 10-dimensional input, processes it through a single linear layer, and outputs another 10-dimensional vector.

```python
import torch

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.linear(x)
```

## Basic Usage

To enable Compiled Autograd, you must set the configuration flag `torch._dynamo.config.compiled_autograd` to `True` *before* calling `torch.compile`.

1.  **Create your model and data:**
    ```python
    model = Model()
    x = torch.randn(10)  # Create a random 10-dimensional input
    ```

2.  **Enable Compiled Autograd and compile your training function:**
    ```python
    torch._dynamo.config.compiled_autograd = True

    @torch.compile
    def train(model, x):
        loss = model(x).sum()
        loss.backward()

    # Execute the compiled function
    train(model, x)
    ```

### What Happens Under the Hood?

When `train(model, x)` is executed:
1.  The Python interpreter calls TorchDynamo because the function is decorated with `@torch.compile`.
2.  Dynamo intercepts the Python bytecode, simulates execution, and records the forward operations into a graph.
3.  The `AOTDispatcher` component disables hooks and calls the autograd engine to compute gradients for `model.linear.weight` and `model.linear.bias`. It records these operations into a graph and rewrites the forward/backward implementation using `torch.autograd.Function`.
4.  The Inductor compiler generates an optimized version of the AOTDispatcher's forward and backward graphs.
5.  Dynamo instructs the Python interpreter to execute this optimized function for the forward pass (`loss = model(x).sum()`).
6.  The interpreter executes `loss.backward()`, which calls into the autograd engine. Because `compiled_autograd` is enabled, the call is routed to the **Compiled Autograd engine**.
7.  Compiled Autograd computes the gradients and, crucially, records *all* operations (including any backward hooks) into a new, comprehensive graph. This graph includes the backward subgraph previously generated by AOTDispatcher.
8.  Compiled Autograd then generates a new function representing the fully-traced `loss.backward()` and executes it using `torch.compile` in inference mode.
9.  These steps apply recursively to any subgraphs within the Compiled Autograd graph, but subsequent AOTDispatcher passes won't need to partition the graph further.

## Inspecting Compiled Autograd Logs

You can inspect the graphs captured by Compiled Autograd using the `TORCH_LOGS` environment variable.

*   **Basic Graph Output:** To print the compiled autograd graph, run your script with:
    ```bash
    TORCH_LOGS="compiled_autograd" python your_script.py
    ```
*   **Verbose Output:** For more detailed output including tensor metadata and recompilation reasons (with a performance cost), use:
    ```bash
    TORCH_LOGS="compiled_autograd_verbose" python your_script.py
    ```

When you run the basic usage example with logging enabled, the compiled autograd graph will be printed to `stderr`. You'll notice some node names prefixed with `aot0_` (e.g., `aot0_view_2`). These correspond to nodes from the AOTAutograd backward graph (with ID=0) that have been incorporated into the larger Compiled Autograd graph.

> **Note:** The logged graph is the **unoptimized** Python code representation of the C++ autograd execution generated by Compiled Autograd. It is *not* the final, optimized graph produced by the compiler backend (like Inductor).

## Using Different Compiler Configs for Forward and Backward

You can apply different `torch.compile` configurations to the forward and backward passes. For instance, you can enforce a `fullgraph` compilation for the backward pass even if the forward pass contains graph breaks.

**Method 1: Explicit Compilation**
```python
def train(model, x):
    # Compile the forward pass
    model = torch.compile(model)
    loss = model(x).sum()

    # Enable Compiled Autograd and compile the backward pass with fullgraph
    torch._dynamo.config.compiled_autograd = True
    torch.compile(lambda: loss.backward(), fullgraph=True)()
```

**Method 2: Context Manager**
The `compiled_autograd.enable` context manager applies your chosen compiler config to all autograd calls within its scope.
```python
def train(model, x):
    # Compile the forward pass
    model = torch.compile(model)
    loss = model(x).sum()

    # Apply a specific compile config to the backward pass via context manager
    with torch._dynamo.compiled_autograd.enable(torch.compile(fullgraph=True)):
        loss.backward()
```

## How Compiled Autograd Addresses AOTAutograd Limitations

### 1. Eliminating Backward Graph Breaks from Forward Breaks
With standard `torch.compile`, graph breaks in the forward pass force corresponding breaks in the backward graph. Compiled Autograd can trace a single, continuous backward graph even when the forward pass is split.

```python
@torch.compile(backend="aot_eager")
def fn(x):
    # 1st graph
    temp = x + 10
    torch._dynamo.graph_break()
    # 2nd graph
    temp = temp + 10
    torch._dynamo.graph_break()
    # 3rd graph
    return temp.sum()

x = torch.randn(10, 10, requires_grad=True)
loss = fn(x)

# 1. Standard torch.compile (AOTAutograd)
torch._dynamo.utils.counters.clear()
loss.backward(retain_graph=True)
print("Graphs without Compiled Autograd:", torch._dynamo.utils.counters["stats"]["unique_graphs"])
# Output: Graphs without Compiled Autograd: 3
# (Three backward graphs due to two forward graph breaks)

torch._dynamo.utils.counters.clear()

# 2. torch.compile with Compiled Autograd
with torch._dynamo.compiled_autograd.enable(torch.compile(backend="aot_eager")):
    loss.backward()
print("Graphs with Compiled Autograd:", torch._dynamo.utils.counters["stats"]["unique_graphs"])
# Output: Graphs with Compiled Autograd: 1
# (A single, full backward graph captured)
```

> **Note:** It is still possible for Dynamo to introduce graph breaks when tracing complex backward hooks captured by Compiled Autograd.

### 2. Capturing Backward Hooks
Backward hooks, which are not captured by AOTAutograd, can be integrated into the graph by Compiled Autograd.

```python
@torch.compile(backend="aot_eager")
def fn(x):
    return x.sum()

x = torch.randn(10, 10, requires_grad=True)
# Register a backward hook
x.register_hook(lambda grad: grad + 10)

loss = fn(x)

with torch._dynamo.compiled_autograd.enable(torch.compile(backend="aot_eager")):
    loss.backward()
```
When you inspect the logs for this code, you should see a `call_hook` node in the captured graph, which Dynamo will later inline. This demonstrates that the hook logic is now part of the compiled graph.

## Common Recompilation Reasons in Compiled Autograd

Compiled Autograd's cache is sensitive to changes in the autograd graph structure.

### 1. Changes in Autograd Node Structure
If the autograd history tracked by the loss tensor changes between iterations, it will trigger a recompilation.

```python
torch._dynamo.config.compiled_autograd = True
x = torch.randn(10, requires_grad=True)

for op in [torch.add, torch.sub, torch.mul, torch.div]:
    loss = op(x, x).sum()
    torch.compile(lambda: loss.backward(), backend="eager")()
    # Each iteration uses a different operator, creating a new autograd node.
```
You will see recompilation messages similar to: **"Cache miss due to new autograd node"**.

### 2. Dynamic Tensor Shapes
Changing the shape of input tensors across runs will also cause recompilations, as Compiled Autograd must handle the tensors as dynamically shaped.

```python
torch._dynamo.config.compiled_autograd = True

for i in [10, 100, 10]:
    x = torch.randn(i, i, requires_grad=True)
    loss = x.sum()
    torch.compile(lambda: loss.backward(), backend="eager")()
    # The shape of 'x' changes each iteration.
```
This will trigger recompilation messages like: **"Cache miss due to changed shapes"**. After the first shape change, `x` is marked as a dynamic shape tensor.

## Conclusion

In this tutorial, you learned:
*   The role of Compiled Autograd in extending `torch.compile` to capture full backward graphs.
*   How to enable and use Compiled Autograd via configuration flags and context managers.
*   How it overcomes key limitations of AOTAutograd regarding graph breaks and backward hooks.
*   Common scenarios that lead to recompilation when using this feature.

Compiled Autograd is a powerful tool for optimizing training loops where the standard `torch.compile` backward capture is insufficient. For deeper technical discussions and updates, follow the [PyTorch Dev Discuss](https://dev-discuss.pytorch.org/) forum.