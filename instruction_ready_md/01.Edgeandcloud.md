# Phi Models: A Guide to Model Availability and Deployment Platforms

This guide provides a comprehensive overview of the Microsoft Phi-3 family of small language models (SLMs), detailing their specifications and the various platforms where they are available for deployment and experimentation. Understanding these options is crucial for selecting the right model and infrastructure for your AI application, whether it's for edge computing, cloud inference, or fine-tuning.

## Model Specifications & Platform Availability

The table below lists the key Phi-3 models, their capabilities, and direct links to access them across major AI platforms.

| Model | Input | Context Length | Azure AI (MaaS) | Azure ML (MaaP) | ONNX Runtime | Hugging Face | Ollama | Nvidia NIM |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Phi-3-vision-128k-instruct** | Text + Image | 128k | [Playground & Deployment](https://ai.azure.com/explore/models/Phi-3-vision-128k-instruct/version/2/registry/azureml) | [Playground, Deployment & Finetuning](https://ml.azure.com/registries/azureml/models/Phi-3-vision-128k-instruct/version/2) | [CUDA](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cuda/tree/main), [CPU](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cpu/tree/main), [DirectML](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-directml/tree/main) | [Download](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) | -NA- | [NIM APIs](https://build.nvidia.com/microsoft/phi-3-vision-128k-instruct) |
| **Phi-3-mini-4k-instruct** | Text | 4k | [Playground & Deployment](https://aka.ms/phi3-mini-4k-azure-ml) | [Playground, Deployment & Finetuning](https://aka.ms/phi3-mini-4k-azure-ml) | [CUDA](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx), [Web](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx) | [Playground & Download](https://huggingface.co/chat/models/microsoft/Phi-3-mini-4k-instruct) | [GGUF](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) | [NIM APIs](https://build.nvidia.com/microsoft/phi-3-mini-4k) |
| **Phi-3-mini-128k-instruct** | Text | 128k | [Playground & Deployment](https://ai.azure.com/explore/models/Phi-3-mini-128k-instruct/version/9/registry/azureml) | [Playground, Deployment & Finetuning](https://ai.azure.com/explore/models/Phi-3-mini-128k-instruct/version/9/registry/azureml) | [CUDA](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx) | [Download](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx) | -NA- | [NIM APIs](https://build.nvidia.com/microsoft/phi-3-mini) |
| **Phi-3-small-8k-instruct** | Text | 8k | [Playground & Deployment](https://ml.azure.com/registries/azureml/models/Phi-3-small-8k-instruct/version/2) | [Playground, Deployment & Finetuning](https://ai.azure.com/explore/models/Phi-3-small-8k-instruct/version/2/registry/azureml) | [CUDA](https://huggingface.co/microsoft/Phi-3-small-8k-instruct-onnx-cuda) | [Download](https://huggingface.co/microsoft/Phi-3-small-8k-instruct-onnx-cuda) | -NA- | [NIM APIs](https://build.nvidia.com/microsoft/phi-3-small-8k-instruct?docker=false) |
| **Phi-3-small-128k-instruct** | Text | 128k | [Playground & Deployment](https://ai.azure.com/explore/models/Phi-3-small-128k-instruct/version/2/registry/azureml) | [Playground, Deployment & Finetuning](https://ml.azure.com/registries/azureml/models/Phi-3-small-128k-instruct/version/2) | [CUDA](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cuda) | [Download](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) | -NA- | [NIM APIs](https://build.nvidia.com/microsoft/phi-3-small-128k-instruct?docker=false) |
| **Phi-3-medium-4k-instruct** | Text | 4k | [Playground & Deployment](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct) | [Playground, Deployment & Finetuning](https://ml.azure.com/registries/azureml/models/Phi-3-medium-4k-instruct/version/2) | [CUDA](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-cuda/tree/main), [CPU](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-cpu/tree/main), [DirectML](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-directml/tree/main) | [Download](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct) | -NA- | [NIM APIs](https://build.nvidia.com/microsoft/phi-3-medium-4k-instruct?docker=false) |
| **Phi-3-medium-128k-instruct** | Text | 128k | [Playground & Deployment](https://ai.azure.com/explore/models/Phi-3-medium-128k-instruct/version/2) | [Playground, Deployment & Finetuning](https://ml.azure.com/registries/azureml/models/Phi-3-medium-128k-instruct/version/2) | [CUDA](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cuda/tree/main), [CPU](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cpu/tree/main), [DirectML](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-directml/tree/main) | [Download](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct) | -NA- | -NA- |

## Choosing Your Deployment Path

Hereâ€™s a brief guide on how to interpret the platform options:

1.  **Azure AI (Model-as-a-Service / MaaS)**: Ideal for quick prototyping and serverless inference. Use the Playground for testing and deploy directly as a managed API endpoint without managing the underlying infrastructure.
2.  **Azure ML (Model-as-a-Product / MaaP)**: Best for full lifecycle management within the Azure ecosystem. This platform supports not only deployment but also fine-tuning and advanced MLOps workflows.
3.  **ONNX Runtime**: Choose this for high-performance, cross-platform inference. Select the variant (CUDA, CPU, DirectML) that matches your hardware for optimal speed and efficiency, especially for edge deployment scenarios.
4.  **Hugging Face**: The go-to source for downloading model weights and using the `transformers` library for local inference, fine-tuning, or integration into your custom pipelines.
5.  **Ollama**: Available for the `Phi-3-mini-4k-instruct` model in GGUF format, perfect for local execution on consumer hardware using the Ollama tool.
6.  **Nvidia NIM**: Provides optimized microservices with standard APIs (NIM) for high-performance inference on Nvidia platforms, simplifying deployment on GPU-accelerated infrastructure.

Use this guide as a reference map to navigate the rich ecosystem supporting the Phi-3 model family and select the best tool for your project's stage and requirements.