# Building Effective Evaluations for AI Models

## Introduction
Optimizing Claude for high accuracy on specific tasks requires empirical testing and continuous improvement. Whether you're testing prompt changes or preparing for production deployment, a robust offline evaluation system is essential. This guide walks through common evaluation patterns and practical guidelines for building effective evals.

## Prerequisites

First, install the required package and set up your environment:

```bash
pip install anthropic
```

```python
from anthropic import Anthropic
import re

client = Anthropic()
MODEL_NAME = "claude-3-opus-20240229"
```

## Understanding Evaluation Components

Every evaluation typically consists of four parts:

1. **Input Prompt**: The question or instruction fed to the model
2. **Model Output**: The completion generated by the model
3. **Golden Answer**: The reference answer for comparison
4. **Score**: A metric representing model performance

## Choosing a Grading Method

Grading is the most recurring cost in evaluation workflows. You'll run evaluations frequently, so choosing an efficient grading method is crucial. Here are the three main approaches:

1. **Code-based grading**: Fast and reliable, but requires structured outputs
2. **Human grading**: Most flexible but slow and expensive
3. **Model-based grading**: Claude grading itself - a good balance for many tasks

Let's explore each method with practical examples.

## Method 1: Code-Based Grading

Code-based grading works best when model outputs follow a predictable format. Let's create an evaluation where Claude identifies how many legs different animals have.

### Step 1: Define the Prompt Template

```python
def build_input_prompt(animal_statement):
    user_content = f"""You will be provided a statement about an animal and your job is to determine how many legs that animal has.

    Here is the animal statement:
    <animal_statement>{animal_statement}</animal_statement>

    How many legs does the animal have? Return just the number of legs as an integer and nothing else."""

    messages = [{"role": "user", "content": user_content}]
    return messages
```

### Step 2: Create Your Evaluation Dataset

```python
eval = [
    {"animal_statement": "The animal is a human.", "golden_answer": "2"},
    {"animal_statement": "The animal is a snake.", "golden_answer": "0"},
    {
        "animal_statement": "The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.",
        "golden_answer": "5",
    },
]
```

### Step 3: Generate Model Completions

```python
def get_completion(messages):
    response = client.messages.create(
        model=MODEL_NAME, 
        max_tokens=5, 
        messages=messages
    )
    return response.content[0].text

# Generate outputs for all evaluation questions
outputs = [
    get_completion(build_input_prompt(question["animal_statement"])) 
    for question in eval
]

# Inspect the results
for output, question in zip(outputs, eval, strict=False):
    print(f"Animal Statement: {question['animal_statement']}")
    print(f"Golden Answer: {question['golden_answer']}")
    print(f"Output: {output}\n")
```

### Step 4: Implement the Code-Based Grader

```python
def grade_completion(output, golden_answer):
    return output == golden_answer

# Calculate the score
grades = [
    grade_completion(output, question["golden_answer"])
    for output, question in zip(outputs, eval, strict=False)
]

score = sum(grades) / len(grades) * 100
print(f"Score: {score}%")
```

This method is extremely efficient because it uses simple string comparison. However, it only works when outputs follow an exact format.

## Method 2: Human Grading

For open-ended questions where outputs vary significantly, human grading might be necessary. Let's create an evaluation for a general-purpose chat assistant.

### Step 1: Define the Open-Ended Prompt Template

```python
def build_input_prompt(question):
    user_content = f"""Please answer the following question:
    <question>{question}</question>"""

    messages = [{"role": "user", "content": user_content}]
    return messages
```

### Step 2: Create Evaluation Questions with Grading Rubrics

Instead of exact answers, we provide rubrics that human graders can use:

```python
eval = [
    {
        "question": "Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.",
        "golden_answer": "A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.",
    },
    {
        "question": "Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.",
        "golden_answer": "A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).",
    },
    {
        "question": "Who won the super bowl in 2024 and who did they beat?",
        "golden_answer": "A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.",
    },
]
```

### Step 3: Generate Model Responses

```python
def get_completion(messages):
    response = client.messages.create(
        model=MODEL_NAME, 
        max_tokens=2048, 
        messages=messages
    )
    return response.content[0].text

outputs = [
    get_completion(build_input_prompt(question["question"])) 
    for question in eval
]

# Review outputs for human grading
for output, question in zip(outputs, eval, strict=False):
    print(f"Question: {question['question']}")
    print(f"Grading Rubric: {question['golden_answer']}")
    print(f"Output: {output}\n")
```

At this point, you would export these outputs and rubrics to a CSV file for human graders to evaluate. While flexible, this approach becomes impractical for large or frequently-run evaluations.

## Method 3: Model-Based Grading

Model-based grading offers a middle ground: using Claude to grade its own responses. This is often accurate enough for many tasks and much more scalable than human grading.

### Step 1: Create a Grader Prompt Template

```python
def build_grader_prompt(answer, rubric):
    user_content = f"""You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.

    Here is the answer that the assistant gave to the question.
    <answer>{answer}</answer>

    Here is the rubric on what makes the answer correct or incorrect.
    <rubric>{rubric}</rubric>

    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect.
    
    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags."""

    messages = [{"role": "user", "content": user_content}]
    return messages
```

### Step 2: Implement the Model-Based Grader

```python
def grade_completion(output, golden_answer):
    messages = build_grader_prompt(output, golden_answer)
    completion = get_completion(messages)
    
    # Extract the correctness label from the completion
    pattern = r"<correctness>(.*?)</correctness>"
    match = re.search(pattern, completion, re.DOTALL)
    
    if match:
        return match.group(1).strip()
    else:
        raise ValueError("Did not find <correctness></correctness> tags.")

# Grade all outputs using the model-based grader
grades = [
    grade_completion(output, question["golden_answer"])
    for output, question in zip(outputs, eval, strict=False)
]

# Calculate the final score
correct_count = grades.count('correct')
score = correct_count / len(grades) * 100
print(f"Score: {score}%")
```

The model-based grader can accurately analyze Claude's responses against complex rubrics, saving significant time compared to human grading while maintaining flexibility.

## Best Practices for Building Evaluations

As you design your own evaluations, keep these principles in mind:

1. **Task-Specific Design**: Create evaluations that closely match your actual use case. The distribution of questions and difficulties should reflect real-world scenarios.

2. **Test Model-Based Grading**: The only way to know if model-based grading works for your task is to try it. Start with a small sample and manually verify the grades to assess accuracy.

3. **Clever Question Design**: Often, you can make evaluations automatable with thoughtful design. Consider reformatting open-ended questions into multiple-choice formats or structuring outputs to enable code-based grading.

4. **Volume Over Perfection**: Generally, prefer having more questions with slightly lower individual quality over having very few perfectly crafted questions. Larger evaluation sets provide more statistical power and better represent real-world performance.

5. **Iterative Improvement**: Start with simple evaluations and gradually increase complexity as you understand your model's capabilities better. Regular evaluation helps track progress and identify areas for improvement.

## Conclusion

Building effective evaluations is a critical skill for AI development. By understanding the trade-offs between different grading methods and applying the right approach for your specific task, you can create efficient, scalable evaluation systems that drive continuous model improvement. Remember that evaluation design is iterativeâ€”start simple, measure results, and refine your approach based on what you learn.