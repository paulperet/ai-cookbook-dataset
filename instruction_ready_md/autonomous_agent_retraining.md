# Self-Evolving Agents: A Cookbook for Autonomous Agent Retraining

## Overview

Agentic systems often reach a plateau after proof-of-concept because they depend on humans to diagnose edge cases and correct failures. This cookbook introduces a repeatable retraining loop that captures those issues, learns from the feedback, and promotes improvements back into production-like workflows. We ground the approach in a regulated healthcare documentation task, but the patterns generalize to any domain that demands accuracy, auditability, and rapid iteration.

### What You Will Learn
- Diagnose why an autonomous agent falls short of production readiness and instrument it with measurable feedback signals.
- Compare three prompt-optimization strategies—from quick manual iteration to fully automated loops—and understand when to reach for each.
- Assemble a self-healing workflow that combines human review, LLM-as-judge evals, and iterative prompt refinement.

### Who This Guide Is For
- ML/AI engineers and solution architects who need to move beyond toy demos.
- Product and delivery teams looking for executable artifacts they can adapt into internal tooling or production pipelines.

### How to Work Through This Guide
1. Start with Section 1 to understand the healthcare use case, baseline agent, and system architecture.
2. Use Section 2 to practice prompt optimization within the OpenAI Evals interface and collect structured feedback.
3. Run Section 3 to automate the optimization loop with graders, evals, and retraining logic.
4. Reference the appendix for reusable prompts, configurations, and evaluation templates as you tailor the workflow to your own environment.

The guide is modular—feel free to run sections independently or sequentially as you adapt the retraining loop to your own agents.

## 1. Use Case Overview: Self-Evolving Agents in Healthcare

### Problem Definition

For this cookbook, we focus on a **real-world use case**: drafting regulatory documents for pharmaceutical companies. These organizations must prepare and submit extensive documentation to regulatory authorities (e.g., the U.S. Food and Drug Administration) to obtain approval for new drugs. The accuracy and speed of these submissions are critical, as they directly impact how quickly life-saving treatments can reach patients.

Regulatory document drafting is a highly complex, iterative, and precision-driven process that requires deep scientific, medical, and compliance expertise. Despite the availability of advanced authoring tools, it remains labor-intensive and prone to human error. **Agentic systems offer substantial leverage** by assisting with research synthesis, content generation, and document structuring, yet human experts are still needed to ensure factual accuracy and regulatory compliance.

The key challenge is to design a feedback loop that enables these agentic systems to learn iteratively and refine model behavior over time. Such a system can gradually shift human effort from detailed correction to high-level oversight, improving efficiency while maintaining the rigorous standards required for regulatory submissions.

### Self-evolving Agent

The process consists of the following steps:

1.  **Baseline Agent**
    The process begins with a baseline agent. In this notebook, we use a deliberately simple example (an agent that summarizes sections of a document) to illustrate the iterative improvement loop. In real-world or enterprise settings, the baseline agent could be much more complex. The summaries it produces serve as the initial benchmark for subsequent evaluation and refinement.

2.  **Human Feedback (or LLM-as-judge)**
    The baseline agent’s outputs are then evaluated either by human reviewers (e.g., for production environments) and/or by an automated **LLM-as-judge** system. This step gathers both quantitative and qualitative feedback that indicates how well the agent meets its goals — for instance, if we are testing the length of the summary, the feedback might be “the summary is too long” or a numerical score (generally between `0` and `1`) generated by eval when assessing if the summary is under 500 words.

3.  **Evals and Aggregated Score**
    Based on the collected feedback, new prompts are generated and tested through evaluations (**Evals**). These tests measure performance against predefined criteria, and the outcomes are combined into an aggregated score that reflects the overall performance. The loop continues until the score exceeds a target threshold (e.g., `0.8`) or the maximum number of retries is reached (e.g., `max_retry = 10`). If the retry limit is hit, engineers are alerted that manual improvements are required.

4.  **Updated Baseline Agent**
    Once an improved version achieves the target performance, it replaces the original baseline agent. This updated agent becomes the foundation for the next iteration, supporting a continuous cycle of learning, feedback, and optimization.

### Dataset Overview

The dataset used for evaluation comprises ~70 sections extracted from the _Sample CMC Section for Hyperpolarized Pyruvate (13C) Injection_, publicly available [here](https://dctd.cancer.gov/drug-discovery-development/reagents-materials/imaging-ind-resources/documentation/13c-pyruvate-cmc.pdf). This dataset provides realistic, domain-specific content suitable for testing both scientific summarization and regulatory compliance behavior.

### Baseline Agent Overview

To keep this cookbook self-contained and easily reproducible, we simplified the regulatory drafting use case while retaining its essential complexity. In production, a typical regulatory authoring agent comprises multiple specialized sub-agents responsible for tasks such as drafting, data analysis, compliance checking, citation generation, and fact verification.

For this guide, we narrow the scope of the regulatory authoring agent to focus on the self-healing aspect of the system. Our regulatory authoring agent consists of two sub-agents:
-   **A summarizer** creating scientific and concise summaries.
-   **A compliance checker**: evaluating each summary against key regulatory requirements (e.g., FDA 21 CFR Part 11).

For the remainder of this cookbook, we implemented a simplified version of the Summarizer agent (see the section **Agent Setup** below). Alternatively, you can reuse the code for the agent created with AgentBuilder. If you’d like to reproduce the agent directly from the AgentBuilder UI, here are the key prompts and parameters used:

-   **Summarizer agent:** This agent used the file search tool, where the [CMC PDF]("data/c13_pyruvate_sample_CMC_from_UCSF.pdf") was uploaded to the vector store.
    > _Prompt:_ "Summarize section {{workflow.input_as_text}} from {{state.cmc_pdf}} uploaded to the vector store."

-   **Compliance Checker agent:**
    > _Prompt:_ "Verify that the summary below is compliant with FDA 21 CFR Part 11: {{input.output_text}}. If the summary is compliant, return _Compliant_. Otherwise, return _This section needs to be manually summarized_."

Both agents were configured with the default parameters - using GPT-5, low reasoning effort, and text as the output format.

### Evaluation Approach

To evaluate the baseline agent, there are two main approaches:

1.  **Collecting Human Feedback.** This approach involves gathering feedback from human users through the OpenAI Evals platform (or a custom UI built for a specific application). It is best suited for production settings or when piloting a tool where subject matter experts (SMEs) interact with the tool in real-world scenarios. This method helps uncover edge cases that may not have been identified during development. On the Evals platform, users can provide thumbs-up or thumbs-down ratings and share qualitative feedback about the summaries.

2.  **Using an LLM-as-a-Judge.** This option is typically used during the development phase, enabling fast feedback loops without requiring SME's time. An **LLM-as-a-judge** uses an LLM to automatically evaluate and score the agent’s outputs based on predefined criteria. It can also be used for monitoring model drift (e.g., in production) or validating changes between model and model versions (e.g., switching between `gpt-5` and `gpt-5-mini`).

This cookbook demonstrates both approaches:
-   **Section 2** shows the platform UI approach for manual prompt optimization
-   **Section 3** implements the fully automated API approach using LLM-as-a-Judge

_Note: The Evals platform does not yet provide an API to retrieve user feedback programmatically._

## 2. Using the OpenAI Evals Platform

The OpenAI Evals platform provides an intuitive interface for prompt optimization and evaluation. This section demonstrates the complete workflow from dataset upload through iterative prompt improvement, showing how you can leverage the platform's visual interface to optimize your prompts before implementing automated solutions.

### Step 1: Upload Dataset

To begin using the OpenAI Evaluation platform, you'll first need to upload your dataset:

1.  Click the **+ Create** button
2.  Define the dataset name
3.  Upload a CSV file and select the columns to keep
4.  Upload

Your dataset should contain the documents or document sections that need to be summarized. Each row represents one input that will be processed by your system.

### Step 2: Explore Your Data

Once uploaded, you can explore your dataset. Click the dataset name to explore the uploaded data. This allows you to verify that your data is properly formatted and contains the expected content before proceeding with prompt configuration.

### Step 3: Configure Initial Prompt

This is where you define your initial system prompt and configure how data flows through your model.

#### Configuration Steps

1.  **System Prompt**: Add the system message that defines the model's task and behavior (this prompt will be optimized)
2.  **User Prompt Template**: Add the prompt message template for user messages, using variables such as `{{<column_name>}}` that get replaced with actual data from your dataset
3.  **Model Selection**: Choose the model for generation (e.g., gpt-4.1, gpt-5)
4.  **Temperature**: Configure creativity vs. determinism

You can start with a very simple prompt to demonstrate the power of the optimization process. For example, beginning with just "summarize" shows how the system can evolve from a minimal starting point.

### Step 4: Generate Outputs

Once your prompt is configured, you're ready to generate outputs across your dataset. The prompt will run once per row and output will be generated on a new **output** column.

1.  Click **"Generate Output"**
2.  The platform runs your prompt against all samples
3.  Results appear in a new **Output** column

The platform will process each row in your dataset, replacing template variables with actual values and calling the model with your system prompt. This creates a baseline of outputs that you can evaluate.

### Step 5: Review and Evaluate

Evaluation is where you provide structured feedback to guide prompt improvement.

#### Review Outputs

1.  **Add Evaluation Columns** if not automatically added - Click "Columns" → "Annotations" → "Add":
    -   **Rating** - Binary (good/bad) or numeric ratings
    -   **Feedback** - Text describing what needs improvement

2.  **Provide Rating and Feedback** - Add your assessment for each output.

    Depending on the quality of the output, you may select a good or bad rating and explain your score based on how you would like the answer to be improved. For example:

    > (Rating) | Feedback
    > - (Good) Good, but only the answer should be provided. The output should not include headers or any text other than the answer.
    > - (Bad) The information is good, but it should be presented as bullet points.
    > - (Good) Good summary; it is clear.
    > - (Bad) Use bullet points when answering to improve readability. Summarize each sub-section individually.

3.  **Save Annotations** - Your feedback is saved with the evaluation run

This structured feedback becomes the foundation for automatic prompt optimization.

### Step 6: Optimize Prompt

After collecting feedback, the platform can automatically generate an improved prompt.

1.  Click **"Optimize"**
2.  A new prompt version is generated in a new tab
3.  Click **"View Prompt"** to see the improved version

### Step 7: Iterate and Compare

With your improved prompt ready, start a new iteration to measure improvement.

1.  Click **"Generate Output"**
2.  Review the new results and provide feedback on any remaining issues
3.  Click **"Optimize"** again if needed
4.  Repeat until satisfied

The platform's tab structure allows you to compare performance across iterations. You can easily see how outputs evolved from your initial prompt to the optimized versions.

#### When to Stop Iterating

Continue the optimization cycle until:
-   **Quality threshold reached**: >80% of outputs receive positive feedback
-   **Diminishing returns**: New iterations show minimal improvement
-   **Specific issues resolved**: All identified failure modes are addressed

This platform-based approach provides an excellent foundation for understanding prompt optimization before moving to automated implementations. The visual interface makes it easy to see the impact of changes and understand the optimization process.

## 3. Self-evolving Loop with LLM-as-a-Judge

This section introduces a fully automated evaluation workflow using an LLM-as-a-Judge through the OpenAI API, eliminating the need for any user interface. This approach enables scalable, programmatic assessment of agent performance, supporting rapid iteration and continuous model monitoring in production.

### 3.1 Setup and Prerequisites

First, install the required packages and set up your environment.

```bash
pip install --upgrade openai openai-agents pydantic pandas gepa litellm python-dotenv -qqq
```

Create a `.env` file in your project directory and add your OpenAI API key:

```bash
# .env
OPENAI_API_KEY=sk-...
```

Now, load the environment variables and import the necessary libraries.

```python
import os
from openai import OpenAI

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Initialize the OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
```

### 3.2 Eval Creation

To evaluate the baseline summarization agent, we use four complementary graders that balance deterministic checks with semantic judgment.

| Grader | Type | Pass threshold | What it checks | Why |
|---|---|---:|---|---|
| Chemical string name | `python` | 0.8 | If any exact chemical names in the section appear in the summary. | Forces preservation of critical domain entities so summaries don’t omit chemically meaningful terms. |
| Summarization length | `python` | 0.85 | Inverse deviation from an expected 100-word length. | Keeps summaries concise and comparable, reducing verbosity that can mask poor content. |
| Cosine similarity | `text_similarity` | 0.85 | Cosine similarity between section and summary texts. | Ensures the summary stays anchored to the source content rather than drifting semantically. |
| LLM-as-judge | `score_model` | 0.85 | A rubric-driven score from a model acting as an evaluator. | Captures nuanced quality signals that rule-based metrics miss, improving overall robustness. |

**Notes**
- The two Python graders catch domain fidelity and length discipline early, which stabilizes optimization before semantic tuning.
- Text similarity guards against superficial rephrasing that strays from the source.
- The LLM judge provides a holistic failsafe when edge cases slip past deterministic checks.

Now, let's define the data source configuration for our evals.

```python
data_source_config = {
    "type": "custom",
    "item_schema": {
        "type": "object",
        "properties": {"section": {"type": "string"}, "summary": {"type": "string"}},
        "required": ["section", "summary"],
    },
    "include_sample_schema": False,
}
```

Next, we'll define the testing criteria, starting with the `chemical_name_grader`.

```python
testing_criteria = [
    {
        "type": "python",
        "name": "chemical_name_grader",
        "image_tag": "2025-05-08",
        "pass_threshold": 0.8,
        "source": r"""def grade(sample: dict, item: dict) -> float:
    section = item["section"]
    summary = item["summary"]
    CHEMICALS_MASTER = ["[1-¹³C]Pyruvic acid","[1-¹³C]Pyruvate","¹²C Pyruvic acid","Sodium [1-¹³C]pyruvate","Sodium pyruvate (¹²C)","AH111501 (Trityl radical)","Tris{8-carboxyl-2,2,6,6-tetra[2-(1-methoxyethyl)]-benzo(1,2-d:4,5-d’)bis(1,3)dithiole-4-yl}methyl acid","AH111501 sodium salt","Methyl, tris[8-carboxy-2,2,6,6-tetrakis(2-methoxyethyl)benzo[1,2-d:4,5-d’]bis[1,3]dithiol-4-yl]-, trisodium salt","AH111501 trisodium salt","AH111576","2,2′,2″,2‴-(4,8-Dibromobenzo[1,2-d:4,5-d′]bis([1,3]dithiole)-2,2,6,6-tetrayl)tetraethanol","AH111586","4,8-Dibromo-2,2,6,6-tetrakis(2-methoxyethyl)benzo[1,2-d:4,5-d′]bis([1,3]dithiole)","AH111709","AH111743","AH112615","4,4-Bis-hydroxymethyl-2-methyl-oxazolidine-2-carboxylic acid","AH112623","Parapyruvate","2-Hydroxy-2-methyl-4-oxo-pentanedioic acid","AH113127","(4-Hydroxymethyl-oxazolidin-4-yl)-methanol","AH113462/E","Enol lactone","AH113462/K","Keto lactone","Acetyl bromide","Methanol","Dimethyl sulfoxide","DMSO","Tetrahydrofuran","THF","Acetonitrile","ACN","Diethyl ether","Et₂O","N,N-Dimethylacetamide","DMA","1,3-Dimethyl-2-imidazolidinone","DMI","Hydrochloric acid","HCl","Sodium hydroxide","NaOH","Disodium ethylenediaminetetraacetate","Na₂EDTA","Ethylenediaminetetraacetic acid","EDTA","Tris(hydroxymethyl)aminomethane","TRIS","Trometamol","Trifluoroacetic acid","TFA","Toluene","Heptane","Ethyl acetate","Ethanol","Water","H₂O","Sodium chloride","NaCl","Cuprous [1-¹³C]cyanide","Cu¹³CN","Gadolinium","Gd","Tin","Sn","Phosphorus","P","Carbon dioxide","CO₂","Sodium [1-13C]pyruvate","[1-13C]Pyruvic acid","1-13C pyruvate"]

    # Identify the chemical names present in the original section
    chemicals_in_section = [chem for chem in CHEMICALS_MASTER if chem in section]

    if not chemicals_in_section:
        # If no chemicals are in the section, this