# Building with the Phi Family in Azure AI Foundry

## Overview

[Azure AI Foundry](https://ai.azure.com) is an enterprise-grade platform designed to help developers build, test, and deploy generative AI applications. It provides access to cutting-edge models, tools, and services while adhering to responsible AI practices.

This guide walks you through using the Phi family of models within Azure AI Foundry, from deployment to programmatic interaction.

## Prerequisites

Before you begin, ensure you have the following:

1.  An active Azure subscription with access to Azure AI Foundry.
2.  The necessary permissions to deploy models and create API connections.
3.  Python 3.8 or later installed on your machine.

## Step 1: Deploy a Phi Model

Azure AI Foundry's Model Catalog provides access to a variety of models, including the Microsoft Phi family.

1.  Navigate to the [Azure AI Foundry Models page](https://ai.azure.com/explore/models/?selectedCollection=phi).
2.  Browse the available Phi models (e.g., Phi-4).
3.  Select your desired model and click **Deploy**.
4.  Configure your deployment (name, compute type, etc.) and proceed. Once deployment is complete, note your endpoint URL and deployment name.

## Step 2: Test in the Playground (Optional)

After deployment, you can immediately test the model's capabilities.

1.  Go to your deployed model's details page in Azure AI Foundry.
2.  Open the **Test** tab to access the built-in playground.
3.  Enter a prompt (e.g., "Can you introduce yourself?") and review the model's response to verify it's working correctly.

## Step 3: Set Up Your Python Environment

To call your deployed model from code, you need to install the required Python packages.

```bash
pip install openai azure-identity
```

## Step 4: Configure Authentication and Client

Azure AI Foundry uses Azure Active Directory for secure authentication. You will use the `azure-identity` library to manage credentials.

Create a new Python script and start by importing the necessary modules and setting up your client.

```python
import os
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider

# 1. Set your endpoint and deployment name
# Replace these with the values from your Azure AI Foundry deployment
endpoint = os.getenv("ENDPOINT_URL", "https://your-resource.openai.azure.com/")
deployment = os.getenv("DEPLOYMENT_NAME", "Phi-4")

# 2. Set up token-based authentication
token_provider = get_bearer_token_provider(
    DefaultAzureCredential(),
    "https://cognitiveservices.azure.com/.default"
)

# 3. Initialize the Azure OpenAI client
client = AzureOpenAI(
    azure_endpoint=endpoint,
    azure_ad_token_provider=token_provider,
    api_version="2024-05-01-preview",  # Use the latest stable API version
)
```

**Explanation:**
*   The `DefaultAzureCredential()` will automatically use available credentials (e.g., from Azure CLI, Visual Studio, or Managed Identity).
*   The `get_bearer_token_provider` fetches a fresh access token for the Cognitive Services scope.
*   The `AzureOpenAI` client is configured to use your specific endpoint and this authentication method.

## Step 5: Create a Chat Completion

Now, you can construct a prompt and send it to your deployed Phi model.

```python
# 4. Define your conversation prompt
chat_prompt = [
    {
        "role": "system",
        "content": "You are an AI assistant that helps people find information."
    },
    {
        "role": "user",
        "content": "Can you introduce yourself?"
    }
]

# 5. Call the model
completion = client.chat.completions.create(
    model=deployment,
    messages=chat_prompt,
    max_tokens=800,
    temperature=0.7,
    top_p=0.95,
    frequency_penalty=0,
    presence_penalty=0,
    stream=False
)

# 6. Print the response
print(completion.choices[0].message.content)
```

**Explanation of Parameters:**
*   `model`: The name of your deployment in Azure AI Foundry.
*   `messages`: A list of message dictionaries defining the conversation roles (`system`, `user`, `assistant`).
*   `max_tokens`: The maximum length of the response.
*   `temperature`: Controls randomness (higher = more creative, lower = more deterministic).
*   `top_p`: Controls diversity via nucleus sampling.
*   `frequency_penalty`: Reduces repetition of the same lines.
*   `presence_penalty`: Encourages the model to talk about new topics.
*   `stream`: Set to `True` for real-time, streaming responses.

## Step 6: Run Your Script

Execute your Python script. If your authentication is correctly configured and the endpoint is reachable, you will see the model's introduction printed in your console.

```bash
python your_script_name.py
```

## Next Steps

You have successfully deployed a Phi model and called it via the API. To build a production application:

1.  **Manage Secrets Securely:** Use environment variables or a service like Azure Key Vault to store your `ENDPOINT_URL`.
2.  **Implement Error Handling:** Wrap your API calls in try-except blocks to manage network or quota errors gracefully.
3.  **Explore Advanced Features:** Experiment with function calling, different prompt engineering techniques, or streaming for a better user experience.

Azure AI Foundry provides the tools to scale this proof of concept into a full, monitored production application.