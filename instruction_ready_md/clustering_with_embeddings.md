# Clustering with Embeddings: A Step-by-Step Guide

## Overview

This tutorial demonstrates how to visualize and perform clustering using embeddings generated by the Gemini API. You will work with a subset of the 20 Newsgroup dataset, reduce the dimensionality of the embeddings using t-SNE for visualization, and then apply the KMeans clustering algorithm.

## Prerequisites

To follow this guide, you need:
*   Python 3.11+
*   A Gemini API key. You can obtain one from [Google AI Studio](https://aistudio.google.com/app/apikey).

## Setup

### 1. Install Required Libraries
First, install the necessary Python packages.

```bash
pip install -U -q google-genai pandas numpy matplotlib seaborn scikit-learn tqdm
```

### 2. Import Libraries and Configure the Client
Import the required modules and set up the Gemini API client using your API key.

```python
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google import genai
from google.genai import types
from google.api_core import retry

from sklearn.datasets import fetch_20newsgroups
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans

# Initialize the Gemini client
# Replace 'YOUR_API_KEY' with your actual key or use an environment variable.
GEMINI_API_KEY = 'YOUR_API_KEY'
client = genai.Client(api_key=GEMINI_API_KEY)
```

### 3. Select an Embedding Model
The Gemini API offers several embedding models. For this tutorial, we'll use `gemini-embedding-001`. It's crucial to stick with one model for consistency in a real application, as embeddings from different models are not directly comparable.

```python
MODEL_ID = "gemini-embedding-001"
```

## Step 1: Load and Prepare the Dataset

You will use the training subset of the 20 Newsgroups dataset, which contains posts on 20 different topics.

```python
# Load the dataset
newsgroups_train = fetch_20newsgroups(subset='train')
print("Available topics:", newsgroups_train.target_names)
```

### 1.1 Clean the Text Data
The raw text contains metadata like email addresses and headers. Let's clean it up.

```python
# Apply cleaning functions
newsgroups_train.data = [re.sub(r'[\w\.-]+@[\w\.-]+', '', d) for d in newsgroups_train.data] # Remove email
newsgroups_train.data = [re.sub(r"\([^()]*\)", "", d) for d in newsgroups_train.data] # Remove names
newsgroups_train.data = [d.replace("From: ", "") for d in newsgroups_train.data]
newsgroups_train.data = [d.replace("\nSubject: ", "") for d in newsgroups_train.data]
```

### 1.2 Create a DataFrame
Organize the data into a Pandas DataFrame for easier manipulation.

```python
df_train = pd.DataFrame(newsgroups_train.data, columns=['Text'])
df_train['Label'] = newsgroups_train.target
df_train['Class Name'] = df_train['Label'].map(newsgroups_train.target_names.__getitem__)

# Filter out very long texts for model compatibility
df_train = df_train[df_train['Text'].str.len() < 10000]
print(f"DataFrame shape: {df_train.shape}")
```

### 1.3 Sample and Filter the Data
To make the tutorial manageable, we'll sample 150 posts from each category and then filter to keep only the science-related topics.

```python
SAMPLE_SIZE = 150
df_train = (df_train.groupby('Label', as_index=False)
                    .apply(lambda x: x.sample(SAMPLE_SIZE))
                    .reset_index(drop=True))

# Keep only science categories
df_train = df_train[df_train['Class Name'].str.contains('sci')]
df_train = df_train.reset_index(drop=True)

print("Class distribution after sampling:")
print(df_train['Class Name'].value_counts())
```

## Step 2: Generate Embeddings

Embeddings are dense vector representations of text. We'll use the Gemini API to convert our text samples into embeddings, specifying the `task_type` as `"CLUSTERING"` to optimize for this use case.

### 2.1 Define the Embedding Function
Create a function that calls the Gemini API to generate an embedding for a single piece of text.

```python
from tqdm.auto import tqdm
tqdm.pandas()

def make_embed_text_fn(model):
    @retry.Retry(timeout=300.0)
    def embed_fn(text: str) -> list[float]:
        result = client.models.embed_content(
            model=model,
            contents=text,
            config=types.EmbedContentConfig(task_type="clustering")
        )
        return np.array(result.embeddings[0].values)
    return embed_fn
```

### 2.2 Apply the Function to the Dataset
Use the function to create an embedding for each text entry in our DataFrame.

```python
def create_embeddings(df):
    embed_fn = make_embed_text_fn(MODEL_ID)
    df['Embeddings'] = df['Text'].progress_apply(embed_fn)
    return df

df_train = create_embeddings(df_train)
```

## Step 3: Dimensionality Reduction with t-SNE

The generated embeddings are 3072-dimensional vectors. To visualize them, we need to reduce their dimensionality to 2D using t-SNE, which preserves local relationships and clusters.

### 3.1 Prepare the Embedding Matrix
First, convert the list of embeddings in the DataFrame into a NumPy array.

```python
# Check the dimensionality of a single embedding
print(f"Embedding vector length: {len(df_train['Embeddings'][0])}")

# Create a matrix of all embeddings
X = np.array(df_train['Embeddings'].to_list(), dtype=np.float32)
print(f"Embedding matrix shape: {X.shape}")
```

### 3.2 Apply t-SNE
Fit the t-SNE model to transform the 3072-dimensional data into 2 dimensions.

```python
tsne = TSNE(random_state=0, n_iter=1000)
tsne_results = tsne.fit_transform(X)

# Add the 2D coordinates to the DataFrame
df_train['tsne_x'] = tsne_results[:, 0]
df_train['tsne_y'] = tsne_results[:, 1]
```

## Step 4: Visualize the Embeddings

Now you can create a scatter plot to see how the different science topics are distributed in the 2D space created by t-SNE.

```python
plt.figure(figsize=(10, 8))
sns.scatterplot(data=df_train, x='tsne_x', y='tsne_y', hue='Class Name', palette='tab20', s=60, alpha=0.8)
plt.title('t-SNE Visualization of Science Newsgroup Embeddings')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
```

**Expected Insight:** Posts from the same topic (e.g., `sci.space`) should form visible clusters or be closer to each other in the plot, demonstrating that the embeddings capture semantic similarity.

## Step 5: Apply KMeans Clustering

Let's quantitatively group the embeddings using the KMeans algorithm. Since we know there are 4 science categories, we'll set the number of clusters (`n_clusters`) to 4.

### 5.1 Perform Clustering
Apply KMeans to the original high-dimensional embeddings (not the t-SNE reduced ones) for the actual clustering task.

```python
n_clusters = 4
kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto')
cluster_labels = kmeans.fit_predict(X)

# Add the cluster assignments to the DataFrame
df_train['Cluster'] = cluster_labels
```

### 5.2 Visualize the KMeans Results
Create another scatter plot, this time colored by the KMeans cluster assignment, to see how well the algorithm's groupings align with the true labels.

```python
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Plot 1: Colored by True Class Name
sns.scatterplot(ax=axes[0], data=df_train, x='tsne_x', y='tsne_y', hue='Class Name', palette='tab20', s=60, alpha=0.8)
axes[0].set_title('Colored by True Label (Class Name)')
axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')

# Plot 2: Colored by KMeans Cluster
sns.scatterplot(ax=axes[1], data=df_train, x='tsne_x', y='tsne_y', hue='Cluster', palette='Set2', s=60, alpha=0.8)
axes[1].set_title('Colored by KMeans Cluster Assignment')

plt.tight_layout()
plt.show()
```

### 5.3 Evaluate the Clustering
You can create a confusion matrix to compare the KMeans cluster assignments with the true newsgroup categories.

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Create confusion matrix
cm = confusion_matrix(df_train['Class Name'], df_train['Cluster'])

# Display the matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=df_train['Class Name'].unique())
fig, ax = plt.subplots(figsize=(8, 6))
disp.plot(ax=ax, cmap='Blues', xticks_rotation='vertical')
plt.title('Confusion Matrix: True Label vs. KMeans Cluster')
plt.tight_layout()
plt.show()
```

**Analysis:** A strong diagonal in the confusion matrix indicates that the KMeans clusters correspond well to the true thematic categories. Misclassifications show where the model's semantic boundaries differ from the dataset's original labels.

## Summary

In this tutorial, you successfully:
1.  Loaded and preprocessed text data from the 20 Newsgroups dataset.
2.  Generated high-quality text embeddings using the Gemini API, specifically configured for a clustering task.
3.  Used t-SNE to reduce the dimensionality of embeddings for intuitive 2D visualization.
4.  Applied the KMeans algorithm to discover natural groupings within the data.
5.  Visualized and evaluated the results, confirming that the embeddings effectively capture the semantic themes of the text.

This workflow forms a foundation for more advanced NLP tasks like document retrieval, topic modeling, and automated content organization.