# Gaussian Process Regression: From Theory to Implementation

This guide provides a hands-on tutorial for performing Gaussian Process (GP) regression. We'll start with the mathematical foundations, implement a basic GP from scratch, and then transition to using the powerful GPyTorch library for more advanced applications.

## Prerequisites

First, let's install and import the necessary libraries:

```python
import numpy as np
from scipy.spatial import distance_matrix
from scipy import optimize
import matplotlib.pyplot as plt
import math
import torch
import gpytorch
from d2l import torch as d2l

d2l.set_figsize()
```

## 1. Understanding Gaussian Process Regression

### 1.1 The Regression Model

In GP regression, we assume our observations are generated by a latent noise-free function plus Gaussian noise:

$$y(x) = f(x) + \epsilon(x)$$

where $\epsilon(x) \sim \mathcal{N}(0,\sigma^2)$ and $f(x) \sim \mathcal{GP}(m,k)$ follows a Gaussian process prior with mean function $m$ and kernel function $k$.

### 1.2 Key Equations

For making predictions at test points $X_*$, the posterior predictive distribution is Gaussian with:

- **Predictive Mean**: $m_* = K(X_*,X)[K(X,X)+\sigma^2I]^{-1}\textbf{y}$
- **Predictive Covariance**: $S = K(X_*,X_*) - K(X_*,X)[K(X,X)+\sigma^2I]^{-1}K(X,X_*)$

The log marginal likelihood for learning hyperparameters is:

$$\log p(\textbf{y} | \theta, X) = -\frac{1}{2}\textbf{y}^{\top}[K_{\theta}(X,X) + \sigma^2I]^{-1}\textbf{y} - \frac{1}{2}\log|K_{\theta}(X,X)| + c$$

## 2. Creating Synthetic Data

Let's generate some synthetic data to work with:

```python
def data_maker1(x, sig):
    """Generate synthetic data: sin(x) + 0.5*sin(4x) + noise"""
    return np.sin(x) + 0.5 * np.sin(4 * x) + np.random.randn(x.shape[0]) * sig

# Generate training and test data
sig = 0.25
train_x, test_x = np.linspace(0, 5, 50), np.linspace(0, 5, 500)
train_y, test_y = data_maker1(train_x, sig=sig), data_maker1(test_x, sig=0.)

# Visualize the data
plt.scatter(train_x, train_y, label='Noisy Observations')
plt.plot(test_x, test_y, linewidth=2, label='True Function')
plt.xlabel("x", fontsize=20)
plt.ylabel("Observations y", fontsize=20)
plt.legend()
plt.show()
```

## 3. Implementing GP Regression from Scratch

### 3.1 Defining the RBF Kernel

We'll use the Radial Basis Function (RBF) kernel:

```python
def rbfkernel(x1, x2, ls=1.0):
    """RBF kernel implementation"""
    dist_matrix = distance_matrix(np.expand_dims(x1, 1), np.expand_dims(x2, 1))
    return np.exp(-(1/(2*ls**2)) * dist_matrix**2)
```

### 3.2 Visualizing the Prior

Before fitting data, let's examine samples from our GP prior:

```python
# Define prior with RBF kernel
mean = np.zeros(test_x.shape[0])
cov = rbfkernel(test_x, test_x, ls=0.2)

# Sample from the prior
prior_samples = np.random.multivariate_normal(mean=mean, cov=cov, size=5)

# Plot prior samples and 95% credible region
plt.plot(test_x, prior_samples.T, color='black', alpha=0.5, label='Prior Samples')
plt.plot(test_x, mean, linewidth=2., label='Mean')
plt.fill_between(test_x, 
                 mean - 2 * np.diag(cov), 
                 mean + 2 * np.diag(cov), 
                 alpha=0.25, 
                 label='95% Credible Region')
plt.legend()
plt.show()
```

### 3.3 Learning Hyperparameters via Marginal Likelihood

Now we'll learn the kernel hyperparameters by maximizing the marginal likelihood:

```python
def neg_MLL(pars):
    """Negative marginal log likelihood for optimization"""
    K = rbfkernel(train_x, train_x, ls=pars[0])
    noise_matrix = pars[1] ** 2 * np.eye(train_x.shape[0])
    
    # Three components of log marginal likelihood
    kernel_term = -0.5 * train_y @ np.linalg.inv(K + noise_matrix) @ train_y
    logdet = -0.5 * np.log(np.linalg.det(K + noise_matrix))
    const = -train_x.shape[0] / 2. * np.log(2 * np.pi)
    
    return -(kernel_term + logdet + const)

# Initial guesses for lengthscale and noise standard deviation
ell_est, post_sig_est = 0.4, 0.5

# Optimize hyperparameters
learned_hypers = optimize.minimize(
    neg_MLL, 
    x0=np.array([ell_est, post_sig_est]), 
    bounds=((0.01, 10.), (0.01, 10.))
)

ell, post_sig_est = learned_hypers.x[0], learned_hypers.x[1]
print(f"Learned lengthscale: {ell:.3f}, Learned noise std: {post_sig_est:.3f}")
```

### 3.4 Making Predictions

With learned hyperparameters, we can now make predictions:

```python
# Compute kernel matrices
K_x_xstar = rbfkernel(train_x, test_x, ls=ell)
K_x_x = rbfkernel(train_x, train_x, ls=ell)
K_xstar_xstar = rbfkernel(test_x, test_x, ls=ell)

noise_matrix = post_sig_est ** 2 * np.eye(train_x.shape[0])
inv_term = np.linalg.inv(K_x_x + noise_matrix)

# Compute posterior mean and covariance
post_mean = K_x_xstar.T @ inv_term @ train_y
post_cov = K_xstar_xstar - K_x_xstar.T @ inv_term @ K_x_xstar

# Compute 95% credible bounds for the latent function
lw_bd = post_mean - 2 * np.sqrt(np.diag(post_cov))
up_bd = post_mean + 2 * np.sqrt(np.diag(post_cov))

# Visualize results
plt.scatter(train_x, train_y, label='Training Data')
plt.plot(test_x, test_y, linewidth=2, label='True Function')
plt.plot(test_x, post_mean, linewidth=2, label='Predictive Mean')
plt.fill_between(test_x, lw_bd, up_bd, alpha=0.25, label='95% Credible Set')
plt.legend()
plt.show()
```

### 3.5 Examining Posterior Samples

Let's visualize samples from the posterior to understand the distribution of plausible functions:

```python
# Sample from posterior
post_samples = np.random.multivariate_normal(post_mean, post_cov, size=20)

plt.scatter(train_x, train_y, label='Training Data')
plt.plot(test_x, test_y, linewidth=2, label='True Function')
plt.plot(test_x, post_mean, linewidth=2, label='Predictive Mean')
plt.plot(test_x, post_samples.T, color='gray', alpha=0.25, label='Posterior Samples')
plt.fill_between(test_x, lw_bd, up_bd, alpha=0.25)
plt.legend()
plt.show()
```

## 4. Using GPyTorch for Production-Ready GP Regression

While implementing GPs from scratch is educational, GPyTorch provides a robust, scalable framework. Let's reproduce our results using GPyTorch.

### 4.1 Setting Up the GPyTorch Model

```python
# Convert data to PyTorch tensors
train_x_torch = torch.tensor(train_x)
train_y_torch = torch.tensor(train_y)
test_x_torch = torch.tensor(test_x)

# Define the GP model
class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ZeroMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel())
    
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# Initialize model and likelihood
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x_torch, train_y_torch, likelihood)
```

### 4.2 Training the Model

```python
# Set model to training mode
model.train()
likelihood.train()

# Define optimizer and marginal likelihood objective
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

# Training loop
training_iter = 50
for i in range(training_iter):
    optimizer.zero_grad()
    output = model(train_x_torch)
    loss = -mll(output, train_y_torch)
    loss.backward()
    
    if i % 10 == 0:
        print(f'Iter {i+1:2d}/{training_iter} - Loss: {loss.item():.3f} '
              f'Lengthscale: {model.covar_module.base_kernel.lengthscale.item():.3f} '
              f'Noise: {model.likelihood.noise.item():.3f}')
    optimizer.step()
```

### 4.3 Making Predictions with GPyTorch

```python
# Switch to evaluation mode
model.eval()
likelihood.eval()

# Make predictions
with torch.no_grad():
    observed_pred = likelihood(model(test_x_torch))
    lower, upper = observed_pred.confidence_region()
    
    # Visualize results
    f, ax = plt.subplots(1, 1, figsize=(4, 3))
    ax.scatter(train_x_torch.numpy(), train_y_torch.numpy(), label='Training Data')
    ax.plot(test_x_torch.numpy(), test_y, linewidth=2, label='True Function')
    ax.plot(test_x_torch.numpy(), observed_pred.mean.numpy(), linewidth=2, label='Predictive Mean')
    ax.fill_between(test_x_torch.numpy(), lower.numpy(), upper.numpy(), alpha=0.25, label='95% Credible Set')
    ax.set_ylim([-1.5, 1.5])
    ax.legend()
    plt.show()
```

## 5. Key Concepts and Best Practices

### 5.1 Types of Uncertainty

- **Epistemic Uncertainty**: Reducible uncertainty about the true function. Captured by `np.diag(post_cov)` in our from-scratch implementation.
- **Aleatoric Uncertainty**: Irreducible observation noise. Captured by `post_sig_est**2`.

### 5.2 Computational Considerations

- Training time scales as $\mathcal{O}(n^3)$ for $n$ training points
- Storage requires $\mathcal{O}(n^2)$ for the kernel matrix
- For large datasets, consider scalable approximations (SKI/KISS-GP, variational methods)

### 5.3 Numerical Stability

Always add a small "jitter" term ($\sim 10^{-6}$) to the diagonal of the kernel matrix when:
- Noise variance is very small
- Doing noise-free regression
- The kernel matrix is ill-conditioned

## 6. Exercises for Further Learning

1. **Hyperparameter Sensitivity**: Try different initializations for lengthscale and noise variance. How do extreme values affect predictions?

2. **Local Optima Exploration**: Initialize with (large lengthscale, large noise) vs (small lengthscale, small noise). Do you converge to different local optima?

3. **Extrapolation Behavior**: Predict beyond the training range (e.g., `test_x = np.linspace(0, 10, 1000)`). How does the credible set behave?

4. **Scalability Test**: Measure runtime with 10,000, 20,000, and 40,000 training points. How does it scale theoretically and empirically?

5. **Kernel Exploration**: Try different kernels in GPyTorch (Matern, Spectral Mixture). How do results and training dynamics change?

6. **Uncertainty Comparison**: Modify the GPyTorch example to show only epistemic uncertainty (like our from-scratch version). Do the results match?

## Summary

This tutorial covered the complete workflow for Gaussian process regression:
- Mathematical foundations of GP regression
- Implementation from first principles
- Hyperparameter learning via marginal likelihood maximization
- Transition to the GPyTorch library for production use

The key advantage of GPs is their ability to provide principled uncertainty estimates alongside predictions, making them invaluable for applications where understanding uncertainty is crucial.