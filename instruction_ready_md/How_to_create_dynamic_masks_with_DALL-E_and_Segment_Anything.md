# Dynamic Image Editing with DALL·E and Segment Anything

This guide demonstrates how to combine Meta's Segment Anything Model (SAM) with OpenAI's DALL·E to create dynamic masks and perform targeted inpainting on images. You'll learn to isolate specific parts of an image and replace them with AI-generated content, enabling creative workflows like virtual fashion design.

## Prerequisites & Setup

Before you begin, ensure you have the following:

1.  **Python Environment:** A Python environment (3.8+ recommended).
2.  **OpenAI API Key:** An API key from [OpenAI](https://platform.openai.com/api-keys). Set it as an environment variable named `OPENAI_API_KEY`.
3.  **Model Checkpoint:** Download the default SAM model checkpoint (2.4 GB) to your working directory.

### Step 1: Install Required Libraries

Run the following commands in your terminal or notebook cell to install all necessary packages.

```bash
pip install torch torchvision torchaudio
pip install git+https://github.com/facebookresearch/segment-anything.git
pip install opencv-python pycocotools matplotlib onnxruntime onnx requests openai numpy Pillow
```

### Step 2: Download the SAM Model

Download the SAM model checkpoint file.

```bash
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
```

### Step 3: Import Libraries and Initialize Clients

Now, let's set up our Python script by importing libraries and initializing the SAM and OpenAI clients.

```python
import cv2
import matplotlib.pyplot as plt
import numpy as np
from openai import OpenAI
import os
from PIL import Image
import requests
from segment_anything import sam_model_registry, SamPredictor
import torch

# Define directories for organizing images
base_image_dir = os.path.join("images", "01_generations")
mask_dir = os.path.join("images", "02_masks")
edit_image_dir = os.path.join("images", "03_edits")
os.makedirs(base_image_dir, exist_ok=True)
os.makedirs(mask_dir, exist_ok=True)
os.makedirs(edit_image_dir, exist_ok=True)

# Path to your downloaded SAM model
sam_model_filepath = "./sam_vit_h_4b8939.pth"

# Initialize the SAM model
sam = sam_model_registry["default"](checkpoint=sam_model_filepath)

# Initialize the OpenAI client
# It will automatically use the OPENAI_API_KEY environment variable
client = OpenAI()
```

## Part 1: Generate the Original Image

First, we'll create a base image using DALL·E 3 that we will later edit.

### Step 4: Define a Helper Function

Create a function to download and save images generated by DALL·E.

```python
def process_dalle_images(response, filename, image_dir):
    """
    Downloads and saves images from a DALL·E API response.
    
    Args:
        response: The response object from client.images.generate() or .edit().
        filename: Base name for the saved files.
        image_dir: Directory to save the images to.
    
    Returns:
        A list of filepaths to the saved images.
    """
    urls = [datum.url for datum in response.data]
    images = [requests.get(url).content for url in urls]
    image_names = [f"{filename}_{i + 1}.png" for i in range(len(images))]
    filepaths = [os.path.join(image_dir, name) for name in image_names]
    
    for image, filepath in zip(images, filepaths):
        with open(filepath, "wb") as image_file:
            image_file.write(image)
    
    return filepaths
```

### Step 5: Generate the Base Image

Define your creative prompt and use DALL·E 3 to generate initial images.

```python
# Define the prompt for your base image
dalle_prompt = '''
Full length, zoomed out photo of our premium Lederhosen-inspired jumpsuit.
Showcase the intricate hand-stitched details and high-quality leather, while highlighting the perfect blend of Austrian heritage and modern fashion.
This piece appeals to a sophisticated, trendsetting audience who appreciates cultural fusion and innovative design.
'''

# Call the DALL·E API to generate images
generation_response = client.images.generate(
    model="dall-e-3",
    prompt=dalle_prompt,
    n=3,
    size="1024x1024",
    response_format="url",
)

# Save the generated images
generated_filepaths = process_dalle_images(generation_response, "generation", base_image_dir)

# Display the filepaths of the new images
for filepath in generated_filepaths:
    print(f"Generated: {filepath}")
```

## Part 2: Create a Dynamic Mask with Segment Anything

Next, we'll select one of the generated images and use SAM to create a mask for the area we want to edit.

### Step 6: Select an Image and Define Visualization Functions

Choose one image to edit and define helper functions to visualize masks and points.

```python
# Select one of your generated images for editing
chosen_image = generated_filepaths[1]  # For example, use the second image
print(f"Chosen image for editing: {chosen_image}")

def show_mask(mask, ax):
    """Overlays a semi-transparent mask on an image axis."""
    color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

def show_points(coords, labels, ax, marker_size=375):
    """Plots positive (green) and negative (red) points on an image axis."""
    pos_points = coords[labels == 1]
    neg_points = coords[labels == 0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color="green", marker="*", s=marker_size, edgecolor="white", linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color="red", marker="*", s=marker_size, edgecolor="white", linewidth=1.25)
```

### Step 7: Load the Image and Select a Point for Masking

Load your chosen image and select a pixel coordinate to "click" on. This point tells SAM which object you want to segment.

```python
# Load the image using OpenCV
image = cv2.imread(chosen_image)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Define the point you want to segment (x, y coordinates)
input_point = np.array([[525, 325]])  # Adjust these coordinates based on your image
input_label = np.array([1])  # '1' indicates this is a point we want to include in the mask

# Visualize the selected point on the image
plt.figure(figsize=(10, 10))
plt.imshow(image)
show_points(input_point, input_label, plt.gca())
plt.title("Selected Point for Masking")
plt.axis("on")
plt.show()
```

### Step 8: Generate and Select the Best Mask

Use SAM to generate multiple mask proposals for the selected point and choose the best one.

```python
# Initialize the SAM predictor and set the image
predictor = SamPredictor(sam)
predictor.set_image(image)

# Generate three possible masks for the selected point
masks, scores, logits = predictor.predict(
    point_coords=input_point,
    point_labels=input_label,
    multimask_output=True,
)

print(f"Generated {len(masks)} mask proposals.")

# Display each mask with its confidence score
for i, (mask, score) in enumerate(zip(masks, scores)):
    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    show_mask(mask, plt.gca())
    show_points(input_point, input_label, plt.gca())
    plt.title(f"Mask {i+1}, Confidence Score: {score:.3f}", fontsize=18)
    plt.axis("off")
    plt.show()
```

### Step 9: Process and Save the Chosen Mask

Select the most accurate mask and process it into the format required by DALL·E's Edit API (a transparent PNG).

```python
# Select the mask you prefer (e.g., the second mask)
chosen_mask = masks[1]

# Process the mask: set the masked area to transparent (0) and everything else to opaque (255)
chosen_mask = chosen_mask.astype("uint8")
chosen_mask = np.where(chosen_mask != 0, 0, 255)  # Invert for DALL·E: object=transparent, background=opaque

# Create a new RGBA image and apply the processed mask to the alpha channel
mask_image = Image.new("RGBA", (1024, 1024), (0, 0, 0, 1))  # Start with an opaque black image
pix = np.array(mask_image)
pix[:, :, 3] = chosen_mask  # Apply our mask to the alpha channel
final_mask_image = Image.fromarray(pix, "RGBA")

# Save the mask for use with DALL·E
mask_save_path = os.path.join(mask_dir, "edit_mask.png")
final_mask_image.save(mask_save_path)
print(f"Mask saved to: {mask_save_path}")
```

## Part 3: Create the Edited Image with DALL·E

Finally, we'll use DALL·E 2's Edit endpoint to inpaint the masked area with new content.

### Step 10: Perform the Inpainting Edit

Provide the original image, the mask, and a new prompt describing what should appear in the masked area.

```python
# Define a new prompt for the inpainting
edit_prompt = "Brilliant leather Lederhosen with a formal look, detailed, intricate, photorealistic"

# Call the DALL·E Edit API
edit_response = client.images.edit(
    model="dall-e-2",  # As of early 2024, only DALL·E 2 supports the edit endpoint
    image=open(chosen_image, "rb"),
    mask=open(mask_save_path, "rb"),
    prompt=edit_prompt,
    n=3,
    size="1024x1024",
    response_format="url",
)

# Save the edited images
edited_filepaths = process_dalle_images(edit_response, "edit", edit_image_dir)

print("Edited images saved to:")
for path in edited_filepaths:
    print(f"  - {path}")
```

### Step 11: Display the Results

View the newly created images.

```python
# Display the edited images
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
for ax, img_path in zip(axes, edited_filepaths):
    img = plt.imread(img_path)
    ax.imshow(img)
    ax.axis("off")
plt.suptitle("DALL·E Edits Based on Your Dynamic Mask", fontsize=16)
plt.tight_layout()
plt.show()
```

## Conclusion

You have successfully created a dynamic mask using the Segment Anything Model and used it to guide DALL·E 2 in generating targeted edits. This workflow unlocks powerful possibilities for image manipulation, from product design to creative art projects.

**Key Takeaways:**
1.  SAM allows for intuitive, point-and-click object segmentation.
2.  Masks must be inverted (object=transparent) for DALL·E's Edit API.
3.  The `dall-e-2` model is required for the inpainting/edit functionality.

Experiment with different points, prompts, and images to see what you can create. Share your innovative applications built with these APIs!