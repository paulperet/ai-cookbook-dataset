# Guide: Building a Code Generation Flow with Phi-3-mini and Prompt Flow

## Overview
This guide walks you through creating a Prompt Flow that uses the quantized Phi-3-mini model to generate Python code from natural language prompts. You'll learn how to set up a local development environment, convert the model for efficient inference, and deploy the flow as a local API.

## Prerequisites
- **Completed Lab 0**: Ensure your environment is set up as described in the [Installations guide](./01.Installations.md).
- **Visual Studio Code** with the **Prompt Flow extension** installed.
- **Python** environment with required packages.

## Step 1: Create a New Prompt Flow Project
1. Open Visual Studio Code and activate the Prompt Flow extension.
2. Create a new, empty flow project using the extension's interface.

## Step 2: Define the Flow Structure
A Prompt Flow is defined by a YAML file (`flow.dag.yaml`) that specifies inputs, outputs, and the execution nodes.

Create or edit `flow.dag.yaml` with the following content:

```yaml
inputs:
  prompt:
    type: string
    default: "Write python code for Fibonacci series. Please use markdown as output"
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
  - name: gen_code_by_phi3
    type: python
    source:
      type: code
      path: gen_code_by_phi3.py
    inputs:
      prompt: ${inputs.prompt}
```

**Explanation:**
- **Inputs**: Defines a `prompt` parameter of type `string` with a default value.
- **Outputs**: Specifies that the final `result` will be the output of the node named `gen_code_by_phi3`.
- **Nodes**: Contains a single Python node (`gen_code_by_phi3`) that references the `gen_code_by_phi3.py` script and passes the input prompt to it.

## Step 3: Quantize the Phi-3-mini Model
To run the small language model (SLM) efficiently on local hardware, we quantize it. This reduces the model's memory footprint and can improve inference speed.

Run the following command in your terminal:

```bash
python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct
```

**Note:** By default, the converted model is saved in a folder named `mlx_model`. This process downloads the model from Hugging Face and applies quantization.

## Step 4: Implement the Python Tool
Create a file named `gen_code_by_phi3.py`. This script contains the logic to load the quantized model and generate a response.

Add the following code:

```python
from promptflow import tool
from mlx_lm import load, generate

@tool
def my_python_tool(prompt: str) -> str:
    # Path to the quantized model directory
    model_id = './mlx_model'

    # Load the model and tokenizer
    model, tokenizer = load(model_id)

    # Format the prompt according to the Phi-3 chat template
    formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>"

    # Generate the response
    response = generate(
        model,
        tokenizer,
        prompt=formatted_prompt,
        max_tokens=2048,
        verbose=True
    )

    return response
```

**Key Points:**
- The `@tool` decorator registers this function as a usable node within Prompt Flow.
- The `load` function loads the quantized model and its tokenizer from the specified directory.
- The prompt is formatted to match the Phi-3's expected chat template structure.
- The `generate` function creates the completion, with a limit of 2048 new tokens.

## Step 5: Test the Flow
1. Use the **Debug** or **Run** functionality within the Prompt Flow extension in VS Code to execute your flow.
2. Provide a test prompt (or use the default) and verify that the flow executes correctly and returns generated code.

## Step 6: Serve the Flow as a Local API
Once testing is successful, you can deploy the flow as a local web service.

Run the following command from your project's root directory:

```bash
pf flow serve --source ./ --port 8080 --host localhost
```

This starts a development server on `http://localhost:8080`.

**Testing the API:**
You can now send HTTP requests to your flow. Use a tool like **Postman** or the **Thunder Client** VS Code extension.

Example `curl` command:
```bash
curl -X POST http://localhost:8080/score \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a function to check if a number is prime."}'
```

## Important Notes & Best Practices

1. **First Run Duration**: The initial execution will take longer as it involves downloading the Phi-3 model from Hugging Face. Subsequent runs will be faster.

2. **Model Selection**: For environments with limited compute (like Intel NPU), the `Phi-3-mini-4k-instruct` variant is recommended due to its smaller size.

3. **Cache Management**: If you encounter issues after modifying the model or service, try clearing the cache. Delete any `cache` and `nc_workshop` folders that may have been generated in your project directory.

4. **Hardware Acceleration**: This guide uses the MLX framework for conversion and inference, which is optimized for Apple Silicon. For Intel NPU acceleration, refer to the [Intel NPU Acceleration Library](https://github.com/intel/intel-npu-acceleration-library).

## Resources
- **Prompt Flow Documentation**: [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)
- **Intel NPU Acceleration Library**: [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)
- **Sample Code**: Download the complete example from the [Local NPU Agent Sample Code](../../../../../../../code/07.Lab/01/AIPC/local-npu-agent/) repository.

## Next Steps
You have successfully built a local code-generation agent. You can extend this flow by:
- Adding evaluation nodes to assess the quality of generated code.
- Incorporating other tools or APIs into the workflow.
- Deploying the flow to a cloud service for production use.