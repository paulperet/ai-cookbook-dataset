# Building a Code Generation Flow with Phi-3-mini on AI PC using Prompt Flow

## Overview

This guide walks you through creating an executable workflow using Microsoft Prompt Flow to generate Python code with the Phi-3-mini model, accelerated by an Intel NPU on an AI PC.

## Prerequisites

Before starting, ensure you have:
1. Completed the environment setup from [Lab 0 - Installations](./01.Installations.md)
2. Visual Studio Code with the Prompt Flow extension installed
3. Access to an AI PC with an Intel NPU

## Step 1: Create a New Prompt Flow Project

1. Open Visual Studio Code
2. Launch the Prompt Flow extension
3. Create a new empty flow project

## Step 2: Define Flow Structure

Create a file named `flow.dag.yaml` with the following structure:

```yaml
inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}
```

This configuration defines:
- A string input parameter called `question` with a default value
- A string output called `answer` that references our Python tool's output
- A single Python node that processes the input question

## Step 3: Implement the Code Generation Tool

Create a file named `Chat_With_Phi3.py` with the following implementation:

```python
from promptflow.core import tool
from transformers import AutoTokenizer, pipeline, TextStreamer
import intel_npu_acceleration_library as npu_lib
import warnings
import asyncio
import platform

class Phi3CodeAgent:
    model = None
    tokenizer = None
    text_streamer = None
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                Phi3CodeAgent.model_id,
                torch_dtype="auto",
                dtype=npu_lib.int4,
                trust_remote_code=True
            )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    @staticmethod
    def chat_with_phi3(prompt):
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt + "<|end|><|assistant|>"

        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
        )

        result = ''
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result = response[0]['generated_text']
            return result

@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)
```

### Code Explanation

1. **Imports**: We import necessary libraries including Prompt Flow's `@tool` decorator, Hugging Face transformers, and Intel's NPU acceleration library.

2. **Phi3CodeAgent Class**: A singleton class that manages the Phi-3 model lifecycle:
   - `init_phi3()`: Initializes the model, tokenizer, and text streamer only once
   - Uses Intel NPU acceleration with INT4 quantization for efficient inference
   - Loads the "microsoft/Phi-3-mini-4k-instruct" model

3. **Prompt Engineering**: The system prompt instructs the model to act as a Python coding assistant and generate only Python code without comments or instructions.

4. **Generation Parameters**: Configured for deterministic output with temperature 0.3 and a maximum of 1024 new tokens.

5. **Tool Function**: The `@tool` decorated function `my_python_tool` is the entry point that Prompt Flow calls, handling Windows-specific event loop configuration.

## Step 4: Test the Flow

1. In the Prompt Flow extension, use the "Debug" or "Run" functionality to test your flow
2. Enter a coding question (e.g., "how to write a bubble sort algorithm")
3. Verify that the flow generates appropriate Python code

## Step 5: Deploy as a Development API

Run the following command in your terminal to start a local development server:

```bash
pf flow serve --source ./ --port 8080 --host localhost
```

This will start a local API server on port 8080. You can test it using tools like Postman or Thunder Client by sending POST requests with your coding questions.

## Important Notes

1. **First Run Performance**: The initial execution will take longer as it downloads the Phi-3 model from Hugging Face. Consider pre-downloading the model using the Hugging Face CLI.

2. **Model Selection**: Due to NPU computational constraints, use the Phi-3-mini-4k-instruct variant rather than larger models.

3. **Cache Management**: When restarting the service, delete the cache and `nc_workshop` folders to ensure clean model loading with INT4 quantization.

## Troubleshooting

- If you encounter Windows-specific async issues, the code already includes the necessary event loop policy configuration
- Ensure all dependencies are installed: `promptflow`, `transformers`, `torch`, and `intel-npu-acceleration-library`
- Verify your AI PC has NPU drivers properly installed

## Resources

1. [Prompt Flow Documentation](https://microsoft.github.io/promptflow/)
2. [Intel NPU Acceleration Library](https://github.com/intel/intel-npu-acceleration-library)
3. [Sample Code Repository](../../../../../../../code/07.Lab/01/AIPC/)

This tutorial demonstrates how to create a production-ready code generation workflow that leverages local AI acceleration for privacy and performance benefits.