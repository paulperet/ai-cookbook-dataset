# Getting Started with Phi-Family Models in Foundry Local

## Introduction

Foundry Local is a powerful on-device AI inference solution that brings enterprise-grade AI capabilities directly to your local hardware. This guide will walk you through setting up and using Phi-Family models with Foundry Local, offering you complete control over your AI workloads while maintaining privacy and reducing costs.

## Prerequisites

Before you begin, ensure you have:
*   A Windows or macOS system.
*   Basic familiarity with your system's command line/terminal.
*   For the SDK section, a working Python environment is recommended.

## Part 1: Using the Foundry Local CLI

The Command Line Interface (CLI) is the quickest way to get started with Foundry Local models.

### Step 1: Install the Foundry Local CLI

First, you need to install the Foundry Local CLI tool on your system. For the most up-to-date and detailed installation instructions, please refer to the [official Foundry Local documentation](https://github.com/microsoft/Foundry-Local/blob/main/README.md).

### Step 2: Discover Available Models

Once installed, you can list all the models available for use with Foundry Local. Open your terminal and run:

```bash
foundry model list
```

This command will display a catalog of supported models, including the Phi family.

### Step 3: Understand the Phi Model Family

The Phi family consists of several models optimized for different tasks and hardware. Here are the key models you'll find:

*   **phi-3.5-mini**: A compact model ideal for basic tasks.
*   **phi-3-mini-128k**: A version with extended context length for longer conversations.
*   **phi-3-mini-4k**: The standard context model for general use.
*   **phi-4**: An advanced model with improved overall capabilities.
*   **phi-4-mini**: A lightweight variant of the Phi-4 model.
*   **phi-4-mini-reasoning**: Specialized for complex, step-by-step reasoning tasks.

> **Note:** Each model can be configured to run on CPU or GPU, depending on your system's capabilities.

### Step 4: Run Your First Model

Let's run a model. We'll use `phi-4-mini-reasoning` as it's designed for logical problem-solving. Execute the following command:

```bash
foundry model run Phi-4-mini-reasoning-generic-cpu
```

> **First Run Note:** The first time you run a model, Foundry Local will download it to your local device. Download time depends on your internet connection.

### Step 5: Interact with the Model

After the model loads, you can interact with it directly in your terminal. Let's test it with a classic logic puzzle. When prompted, provide the following input:

```
Please calculate the following step by step: Now there are pheasants and rabbits in the same cage, there are thirty-five heads on top and ninety-four legs on the bottom, how many pheasants and rabbits are there?
```

The model should process this, recognize it as a system of equations problem (pheasants have 2 legs, rabbits have 4 legs), and provide a step-by-step solution, ultimately giving you the counts of each animal.

To exit the interactive session, you can typically use `Ctrl+C` or type a command like `/exit`, depending on the interface.

## Part 2: Integrating with the Foundry Local SDK

For building applications, the Software Development Kit (SDK) allows you to programmatically integrate Foundry Local's capabilities.

### Step 1: Choose Your SDK

Foundry Local offers SDKs for several popular programming languages. Select the one that fits your project:

*   **Python:** [SDK Documentation & Examples](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)
*   **JavaScript:** [SDK Documentation & Examples](https://github.com/microsoft/Foundry-Local/tree/main/sdk/js)
*   **C# (.NET):** [SDK Documentation & Examples](https://github.com/microsoft/Foundry-Local/tree/main/sdk/cs)
*   **Rust:** [SDK Documentation & Examples](https://github.com/microsoft/Foundry-Local/tree/main/sdk/rust)

### Step 2: Install the SDK

Follow the installation instructions in the SDK-specific documentation linked above. For example, a Python installation might look like:

```bash
pip install foundry-local-sdk
```

### Step 3: Write a Simple Application

Here is a conceptual example of how you might use the SDK (pseudocode, structure will vary by language):

1.  **Import the SDK** and initialize a client.
2.  **Load a model** (e.g., `phi-4-mini-reasoning`).
3.  **Create a prompt** for the model.
4.  **Run inference** and handle the response.

Refer to the sample code in your chosen SDK's repository for exact, runnable examples.

## Conclusion

You've successfully learned the basics of using Foundry Local with Phi-family models. You can now:

*   Install and use the Foundry Local CLI to run models interactively.
*   Identify different Phi models and their use cases.
*   Understand how to integrate these capabilities into your own applications using the provided SDKs.

Foundry Local empowers you to leverage powerful AI models directly on your device, providing control over data privacy, cost, and performance.