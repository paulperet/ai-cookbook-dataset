# Getting Started with Phi Models on GitHub Models

This guide walks you through using the Phi family of AI models hosted on GitHub Models. You'll learn how to set up your environment and make API calls using the Azure AI Inference SDK.

## Prerequisites

Before you begin, ensure you have:

- A GitHub account
- Python 3.8 or higher installed
- Basic familiarity with Python programming

## Step 1: Create a Personal Access Token

First, create a GitHub personal access token to authenticate with the models service. No special permissions are required for this token.

Set the token as an environment variable based on your operating system:

**Bash/Linux/macOS:**
```bash
export GITHUB_TOKEN="<your-github-token-goes-here>"
```

**PowerShell:**
```powershell
$Env:GITHUB_TOKEN="<your-github-token-goes-here>"
```

**Windows Command Prompt:**
```cmd
set GITHUB_TOKEN=<your-github-token-goes-here>
```

## Step 2: Install the Azure AI Inference SDK

Install the required Python package using pip:

```bash
pip install azure-ai-inference
```

## Step 3: Basic Chat Completion

Let's start with a simple synchronous chat completion request. This example asks the model to solve a financial calculation problem.

```python
import os
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

# Configuration
endpoint = "https://models.inference.ai.azure.com"
model_name = "Phi-4"
token = os.environ["GITHUB_TOKEN"]

# Initialize the client
client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

# Make the API call
response = client.complete(
    messages=[
        UserMessage(content="I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"),
    ],
    temperature=0.4,
    top_p=1.0,
    max_tokens=2048,
    model=model_name
)

# Print the response
print(response.choices[0].message.content)
```

This code:
1. Imports the necessary modules from the Azure AI Inference SDK
2. Configures the endpoint and model (using Phi-4 in this case)
3. Creates a client with your GitHub token
4. Sends a single user message to the model
5. Prints the model's response

## Step 4: Multi-Turn Conversation

For chat applications, you'll need to manage conversation history. Here's how to conduct a multi-turn conversation:

```python
import os
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

# Configuration
token = os.environ["GITHUB_TOKEN"]
endpoint = "https://models.inference.ai.azure.com"
model_name = "Phi-4"  # Replace with your preferred model

# Initialize the client
client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

# Define the conversation history
messages = [
    SystemMessage(content="You are a helpful assistant."),
    UserMessage(content="What is the capital of France?"),
    AssistantMessage(content="The capital of France is Paris."),
    UserMessage(content="What about Spain?"),
]

# Get the model's response
response = client.complete(messages=messages, model=model_name)

print(response.choices[0].message.content)
```

The key difference here is the `messages` list, which includes:
- A `SystemMessage` to set the assistant's behavior
- Alternating `UserMessage` and `AssistantMessage` objects to maintain conversation context

## Step 5: Streaming Responses

For better user experience, especially with longer responses, you can stream the output token by token:

```python
import os
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

# Configuration
token = os.environ["GITHUB_TOKEN"]
endpoint = "https://models.inference.ai.azure.com"
model_name = "Phi-4"  # Replace with your preferred model

# Initialize the client
client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

# Make a streaming request
response = client.complete(
    stream=True,
    messages=[
        SystemMessage(content="You are a helpful assistant."),
        UserMessage(content="Give me 5 good reasons why I should exercise every day."),
    ],
    model=model_name,
)

# Process the streamed response
for update in response:
    if update.choices:
        print(update.choices[0].delta.content or "", end="")

# Clean up
client.close()
```

Setting `stream=True` in the `complete()` method returns a generator that yields response chunks as they become available, providing a more responsive interface.

## Available Phi Models

GitHub Models offers several Phi family models. Replace `model_name` in the examples above with any of these:

- `Phi-4`
- `Phi-3.5-MoE-instruct`
- `Phi-3.5-vision-instruct`
- `Phi-3.5-mini-instruct`
- `Phi-3-Medium-128k-Instruct`
- `Phi-3-medium-4k-instruct`
- `Phi-3-mini-128k-instruct`
- `Phi-3-mini-4k-instruct`
- `Phi-3-small-128k-instruct`
- `Phi-3-small-8k-instruct`

## Important Considerations

1. **Rate Limits**: Free usage is subject to rate limits for prototyping. For production-scale applications, you'll need to provision resources through an Azure account.

2. **Content Safety**: GitHub Models uses Azure AI Content Safety filters that cannot be disabled in the free tier.

3. **Experimental Nature**: Remember that you're experimenting with AI, and content mistakes are possible. This service is under GitHub's Pre-release Terms.

## Next Steps

- Experiment with different Phi models by changing the `model_name` parameter
- Adjust parameters like `temperature` and `max_tokens` to control response behavior
- For production use, explore provisioning dedicated resources through Azure AI

For complete documentation and additional samples, refer to the [Azure AI Inference SDK documentation](https://docs.microsoft.com/python/api/overview/azure/ai-inference-readme).