# Video Summarization with Phi-3-Vision: A Step-by-Step Guide

This guide walks you through extracting keyframes from a video and using Microsoft's Phi-3-Vision model to generate a textual summary of the video content.

## Prerequisites

Before starting, ensure you have the necessary Python packages installed:

```bash
pip install opencv-python pillow transformers
```

## Step 1: Extract Keyframes from Video

First, we'll create a function to extract keyframes from a video based on visual similarity between consecutive frames.

```python
import cv2
import numpy as np

def save_keyframes(video_path, output_folder):
    """
    Extract and save keyframes from a video.
    
    Args:
        video_path (str): Path to the input video file
        output_folder (str): Directory where keyframes will be saved
    """
    videoCapture = cv2.VideoCapture(video_path)
    success, frame = videoCapture.read()
    i = 0
    
    while success:
        # Convert frame to grayscale for histogram comparison
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        hist = cv2.calcHist([gray_frame], [0], None, [256], [0, 256])
        
        # Read next frame
        success, next_frame = videoCapture.read()
        if not success:
            break
        
        # Process next frame
        next_gray_frame = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)
        next_hist = cv2.calcHist([next_gray_frame], [0], None, [256], [0, 256])
        
        # Compare histograms to detect scene changes
        similarity = cv2.compareHist(hist, next_hist, cv2.HISTCMP_CORREL)
        
        # Save frame if significant change detected (similarity < 0.9)
        if similarity < 0.9:
            i += 1
            cv2.imwrite(f"{output_folder}/keyframe_{i}.jpg", frame)
            print(f"Saved keyframe {i}")
        
        frame = next_frame
    
    videoCapture.release()
```

Run the function on your video:

```python
save_keyframes('../video/copilot.mp4', '../output')
```

## Step 2: Load Extracted Keyframes

Now, let's load the extracted keyframes into memory for processing with the vision model.

```python
from PIL import Image

images = []
placeholder = ""

# Load all keyframes (assuming 21 frames were extracted)
for i in range(1, 22):
    image_path = f"../output/keyframe_{i}.jpg"
    images.append(Image.open(image_path))
    placeholder += f"<|image_{i}|>\n"

print(f"Loaded {len(images)} keyframes")
```

## Step 3: Set Up Phi-3-Vision Model

We'll use Microsoft's Phi-3-Vision model, which is a multimodal model capable of understanding both text and images.

```python
from transformers import AutoModelForCausalLM, AutoProcessor

# Specify the model ID (local path or Hugging Face model name)
model_id = "../Phi3Vision"  # Update this path as needed

# Load the model with optimized settings
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="cuda",  # Use "cpu" if no GPU available
    trust_remote_code=True,
    torch_dtype="auto",
    _attn_implementation='flash_attention_2'  # Optimized attention implementation
)

# Load the processor with appropriate settings
processor = AutoProcessor.from_pretrained(
    model_id,
    trust_remote_code=True,
    num_crops=4  # Number of image crops for processing
)
```

## Step 4: Prepare the Prompt

Create a conversation-style prompt that includes placeholders for the images and asks for a video summary.

```python
messages = [
    {"role": "user", "content": placeholder + "Summarize the video."}
]

# Convert the chat template to a formatted prompt
prompt = processor.tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
```

## Step 5: Process Inputs and Generate Summary

Prepare the inputs and configure generation parameters for the model.

```python
# Prepare inputs for the model
inputs = processor(prompt, images, return_tensors="pt").to("cuda:0")

# Configure generation parameters
generation_args = {
    "max_new_tokens": 1000,  # Maximum length of generated text
    "temperature": 0.0,      # Deterministic output (no randomness)
    "do_sample": False,      # Use greedy decoding
}

# Generate the summary
generate_ids = model.generate(
    **inputs,
    eos_token_id=processor.tokenizer.eos_token_id,
    **generation_args
)

# Extract only the newly generated tokens (excluding the input prompt)
generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]

# Decode the generated tokens to text
response = processor.batch_decode(
    generate_ids,
    skip_special_tokens=True,
    clean_up_tokenization_spaces=False
)[0]
```

## Step 6: Display the Summary

Finally, let's see the video summary generated by the model:

```python
print("Video Summary:")
print("=" * 50)
print(response)
```

**Example Output:**
```
The video appears to be a promotional or informational piece about a product or service called 'Copilot'. It showcases various individuals in different settings, such as an office and a home office, engaging with the product. The video includes text overlays that suggest the product can help with tasks like lowering production costs, managing meetings, and creating presentations. There are also screenshots of a chat interface and a summary of a product launch discussion, indicating that the product may be related to project management or collaboration. The video seems to emphasize the efficiency and effectiveness of the Copilot product in a professional context.
```

## Key Takeaways

1. **Keyframe Extraction**: By comparing histogram similarity between consecutive frames, we can efficiently identify scene changes and extract representative frames.

2. **Multimodal Processing**: Phi-3-Vision can process both images and text, making it ideal for video summarization tasks.

3. **Deterministic Output**: Using `temperature=0.0` and `do_sample=False` ensures reproducible results, which is useful for testing and evaluation.

4. **Efficient Processing**: The `flash_attention_2` implementation and proper device mapping optimize memory usage and inference speed.

## Troubleshooting

- **CUDA Errors**: If you encounter CUDA-related errors, change `device_map="cuda"` to `device_map="cpu"` for CPU-only inference.
- **Model Path**: Ensure the `model_id` path points to the correct location of your Phi-3-Vision model files.
- **Memory Issues**: Reduce the number of keyframes or use smaller image resolutions if you encounter out-of-memory errors.

This approach provides an automated pipeline for converting video content into concise textual summaries, useful for content analysis, accessibility, and information retrieval applications.