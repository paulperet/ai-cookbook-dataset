# Remote Inference with a Fine-Tuned Model

This guide walks you through deploying and accessing a fine‑tuned model for remote inference using Azure Container Apps. After training adapters in a remote environment, you can interact with the model via a simple web API.

## Prerequisites

Before you begin, ensure you have:

- A fine‑tuned model and its adapters stored in Azure Files (generated during the fine‑tuning step).
- The [AI Toolkit](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) installed in VS Code.
- Access to an Azure subscription and resource group (the same ones used for fine‑tuning, by default).

---

## Step 1: Provision Azure Resources for Inference

To set up the Azure resources required for remote inference, run the AI Toolkit provisioning command.

1. Open the VS Code command palette (`Ctrl+Shift+P` or `Cmd+Shift+P`).
2. Select **`AI Toolkit: Provision Azure Container Apps for inference`**.
3. When prompted, choose your Azure subscription and resource group.

> **Note:** By default, the inference resources will use the same subscription, resource group, Azure Container App Environment, and Azure Files storage as your fine‑tuning setup. A separate Azure Container App is created solely for the inference API.

---

## Step 2: Understand the Inference Configuration

The template includes several configuration files in the `infra` folder:

| Folder/File | Purpose |
|-------------|---------|
| `infra/` | Contains all configurations for remote operations. |
| `infra/provision/inference.parameters.json` | Parameters for the Bicep templates used to provision inference resources. |
| `infra/provision/inference.bicep` | Bicep templates for provisioning Azure inference resources. |
| `infra/inference.config.json` | Generated by the provisioning command; used as input for other remote commands. |

### Key Parameters in `inference.parameters.json`

| Parameter | Description |
|-----------|-------------|
| `defaultCommands` | Commands to start the web API. |
| `maximumInstanceCount` | Maximum number of GPU instances for scaling. |
| `location` | Azure region where resources are provisioned (defaults to the resource group’s location). |
| `storageAccountName`, `fileShareName`, `acaEnvironmentName`, `acaEnvironmentStorageName`, `acaAppName`, `acaLogAnalyticsName` | Names for the Azure resources. By default, these match the fine‑tuning resource names. You can provide new, unique names to create custom resources, or specify existing resource names if you prefer to reuse them (see [Using Existing Azure Resources](#step-4-using-existing-azure-resources-optional)). |

---

## Step 3: Deploy the Inference Code

If you modify the inference code or need to reload the model, deploy your changes to Azure Container Apps.

1. Open the command palette.
2. Select **`AI Toolkit: Deploy for inference`**.

This command synchronizes your latest code with the Azure Container App and restarts the replica. After a successful deployment, the model is ready for evaluation via the inference endpoint.

---

## Step 4: Access the Inference API

Once deployment completes, you can access the inference endpoint in two ways:

1. Click the **“Go to Inference Endpoint”** button in the VS Code notification.
2. Find the web API endpoint (`ACA_APP_ENDPOINT`) in:
   - The file `./infra/inference.config.json`
   - The output panel in VS Code

> **Note:** The inference endpoint may take a few minutes to become fully operational after deployment.

---

## Step 5: Using Existing Azure Resources (Optional)

By default, inference provisioning reuses the same Azure Container App Environment, Storage Account, Azure File Share, and Azure Log Analytics created during fine‑tuning. If you customized those resources or want to use your own existing Azure resources, you can specify them manually.

1. Open `./infra/provision/inference.parameters.json`.
2. Update the relevant parameters with your existing resource names.
3. Run the **`AI Toolkit: Provision Azure Container Apps for inference`** command again.

The command updates any specified resources and creates any that are missing.

### Example: Using an Existing Azure Container App Environment

If you have an existing Azure Container App Environment, your `inference.parameters.json` should include:

```json
{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    ...
    "acaEnvironmentName": {
      "value": "<your-aca-env-name>"
    },
    "acaEnvironmentStorageName": {
      "value": null
    },
    ...
  }
}
```

---

## Step 6: Manual Provisioning (Optional)

If you prefer to set up Azure resources manually without the AI Toolkit commands:

1. Use the Bicep files in `./infra/provision` to deploy resources via the Azure CLI or portal.
2. After all resources are configured, populate `./infra/inference.config.json` with the resource details.

### Example `inference.config.json` for Manual Setup

```json
{
  "SUBSCRIPTION_ID": "<your-subscription-id>",
  "RESOURCE_GROUP_NAME": "<your-resource-group-name>",
  "STORAGE_ACCOUNT_NAME": "<your-storage-account-name>",
  "FILE_SHARE_NAME": "<your-file-share-name>",
  "ACA_APP_NAME": "<your-aca-name>",
  "ACA_APP_ENDPOINT": "<your-aca-endpoint>"
}
```

---

## Summary

You have now provisioned Azure resources, deployed inference code, and accessed the inference endpoint for your fine‑tuned model. The process leverages the AI Toolkit for streamlined provisioning and deployment, but also supports manual configuration and reuse of existing Azure resources.

Next, you can integrate the inference API into your applications or evaluate the model’s performance using the provided endpoint.