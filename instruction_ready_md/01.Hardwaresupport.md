# Microsoft Phi Hardware Support Guide

This guide outlines the hardware requirements and configuration options for running Microsoft Phi models with ONNX Runtime.

## Prerequisites

Before you begin, ensure you have the following installed:
- ONNX Runtime (with DirectML or CUDA support, depending on your hardware)
- Python 3.8 or higher

## Supported Hardware

Microsoft Phi models are optimized for ONNX Runtime and support the following hardware configurations:

### Desktop & Server Hardware
- **GPU (DirectML)**: RTX 4090 or other DirectX 12-capable GPUs
- **GPU (CUDA)**: NVIDIA A100 80GB or other GPUs with Compute Capability ≥ 7.0
- **CPU**: Standard F64s v2 (64 vCPUs, 128 GiB memory) or equivalent

### Mobile Hardware
- **Android**: Samsung Galaxy S21 or equivalent
- **iOS**: iPhone 14 or higher with A16/A17 processors

### Minimum Requirements
- **Windows**: DirectX 12-capable GPU with at least 4GB of combined RAM
- **CUDA**: NVIDIA GPU with Compute Capability ≥ 7.0

## Running ONNX Runtime on Multiple GPUs

Currently, Phi ONNX models are optimized for single-GPU execution. While multi-GPU support is technically possible, ONNX Runtime with two GPUs doesn't guarantee better throughput compared to running two separate instances.

> **Note**: The GenAI ONNX Team announced at Build 2024 that they've enabled multi-instance support instead of multi-GPU for Phi models. Check the [ONNX Runtime documentation](https://onnxruntime.ai/) for the latest updates.

### Multi-Instance Configuration

To utilize multiple GPUs, run separate ONNX Runtime instances, each targeting a different GPU:

1. **Set up your environment** to target specific GPUs using the `CUDA_VISIBLE_DEVICES` environment variable.

2. **Run separate instances** for each GPU:

```bash
# First instance using GPU 0
CUDA_VISIBLE_DEVICES=0 python infer.py

# Second instance using GPU 1  
CUDA_VISIBLE_DEVICES=1 python infer.py
```

This approach allows you to process multiple inference requests in parallel across different GPUs.

## Next Steps

For more advanced Phi model implementations and deployment options, explore:
- [Azure AI Foundry](https://ai.azure.com) for managed AI services
- [ONNX Runtime documentation](https://onnxruntime.ai/) for performance optimization guides
- The [Build 2024 GenAI ONNX Team presentation](https://youtu.be/WLW4SE8M9i8) for the latest multi-instance capabilities

Remember to always check the official documentation for the most current hardware support information and best practices.